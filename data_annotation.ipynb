{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from transformers import pipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Annotation/Annotated_rawdata/new_data'\n",
    "loader = DirectoryLoader(file_path,\n",
    "                                     glob='*.txt',\n",
    "                                     show_progress=True,\n",
    "                                     use_multithreading=True)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 100)\n",
    "chunks = [chunk for doc in documents\n",
    "                  for chunk in text_splitter.split_documents([doc])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_content = [chunk.page_content for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to sample the chunks for context in the future\n",
    "len(chunks_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_path.split('/')[-1] in ['generalinfo_cmu', 'generalinfo_pittsburgh', 'eventspittsburgh']:\n",
    "    context_sample = random.sample(chunks_content, 100)\n",
    "else:\n",
    "    context_sample = random.sample(chunks_content,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(context_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(examples, context, num_questions) -> str:\n",
    "    prompt = \"You are an AI assistant trained for data annotation.\\n\"\n",
    "    prompt += \"Your task is to generate **question-answer pairs** based on the given factual context. You will be given a context passage, and you will select a fact from the context, then ask a question from it, and then provide the answer to the asked question based on the selected fact. \"\n",
    "    prompt += \"Ensure the questions are well-formed, unambiguous, and directly answerable using the provided context. Avoid speculative, open-ended questions, or generate any new context. \"\n",
    "    prompt += \"These are some examples with questions and answers as well as the fact that help answer that question: \\n\"\n",
    "    for i, example in enumerate(examples):\n",
    "        prompt += f\"Question: {example['question']}\\n\"\n",
    "        prompt += f\"Answer: {example['answer']}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += f\"Here is the context for the question and answer generation task: {context}\\n\\n\"\n",
    "    prompt += f\"Extracted {num_questions} question-answer pairs based on the given context using the following format: \\n\"\n",
    "    prompt += \"Question: \\n\"\n",
    "    prompt += \"Answer: \\n\\n\"\n",
    "    prompt += \"Only return the question and answer you generated. Do not include any additional information.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_formated(result) -> str:\n",
    "    answers = result.split('Do not include any additional information.\\n\\n')[-1]\n",
    "    answers = answers.replace('Question','<Generated>Question')\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [17:20<00:00, 173.45s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:50<00:00, 36.67s/it]\n",
      "Device set to use cuda\n",
      "Inference process:   0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   2%|▏         | 1/50 [00:02<01:43,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   4%|▍         | 2/50 [00:04<01:36,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   6%|▌         | 3/50 [00:05<01:19,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   8%|▊         | 4/50 [00:06<01:12,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  10%|█         | 5/50 [00:08<01:05,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  12%|█▏        | 6/50 [00:09<01:07,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  14%|█▍        | 7/50 [00:11<01:12,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  16%|█▌        | 8/50 [00:13<01:12,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  18%|█▊        | 9/50 [00:15<01:14,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  20%|██        | 10/50 [00:17<01:12,  1.82s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  22%|██▏       | 11/50 [00:19<01:10,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  24%|██▍       | 12/50 [00:20<01:08,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  26%|██▌       | 13/50 [00:23<01:20,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  28%|██▊       | 14/50 [00:30<02:09,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  30%|███       | 15/50 [00:32<01:44,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  32%|███▏      | 16/50 [00:36<01:48,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  34%|███▍      | 17/50 [00:37<01:27,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  36%|███▌      | 18/50 [00:39<01:16,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  38%|███▊      | 19/50 [00:40<01:02,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  40%|████      | 20/50 [00:43<01:11,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  42%|████▏     | 21/50 [00:45<01:00,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  44%|████▍     | 22/50 [00:46<00:52,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  46%|████▌     | 23/50 [00:49<00:58,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  48%|████▊     | 24/50 [00:51<00:54,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  50%|█████     | 25/50 [00:53<00:50,  2.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  52%|█████▏    | 26/50 [00:54<00:47,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  54%|█████▍    | 27/50 [00:55<00:38,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  56%|█████▌    | 28/50 [00:58<00:45,  2.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  58%|█████▊    | 29/50 [00:59<00:36,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  60%|██████    | 30/50 [01:02<00:38,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  62%|██████▏   | 31/50 [01:03<00:34,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  64%|██████▍   | 32/50 [01:05<00:32,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  66%|██████▌   | 33/50 [01:06<00:27,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  68%|██████▊   | 34/50 [01:08<00:24,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  70%|███████   | 35/50 [01:09<00:22,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  72%|███████▏  | 36/50 [01:11<00:23,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  74%|███████▍  | 37/50 [01:13<00:21,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  76%|███████▌  | 38/50 [01:14<00:20,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  78%|███████▊  | 39/50 [01:16<00:19,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  80%|████████  | 40/50 [01:20<00:23,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  82%|████████▏ | 41/50 [01:23<00:22,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  84%|████████▍ | 42/50 [01:25<00:18,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  86%|████████▌ | 43/50 [01:26<00:13,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  88%|████████▊ | 44/50 [01:28<00:11,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  90%|█████████ | 45/50 [01:29<00:09,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  92%|█████████▏| 46/50 [01:31<00:06,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  94%|█████████▍| 47/50 [01:32<00:04,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  96%|█████████▌| 48/50 [01:33<00:02,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  98%|█████████▊| 49/50 [01:34<00:01,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process: 100%|██████████| 50/50 [01:36<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "augmented_data = ''\n",
    "with open('Annotation/example.json', 'r') as f:\n",
    "    examples = json.load(f)\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_new_tokens=512, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "for context in tqdm(context_sample, desc=\"Inference process\"):\n",
    "    prompt = prompt_generation(examples, context, 1)\n",
    "    results = pipe(prompt)\n",
    "    answers = result_formated(results[0]['generated_text'])\n",
    "    augmented_data +=  answers\n",
    "    augmented_data += '\\n\\n'\n",
    "with open(\"Annotation/Annotated_data\"+file_path.split('/')[-1]+'.txt', 'w') as f:\n",
    "    f.write(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference process:   0%|          | 0/50 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   2%|▏         | 1/50 [00:01<01:02,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   4%|▍         | 2/50 [00:03<01:28,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   6%|▌         | 3/50 [00:05<01:39,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:   8%|▊         | 4/50 [00:08<01:44,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  10%|█         | 5/50 [00:10<01:31,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  12%|█▏        | 6/50 [00:11<01:27,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  14%|█▍        | 7/50 [00:13<01:23,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  16%|█▌        | 8/50 [00:15<01:19,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  18%|█▊        | 9/50 [00:17<01:21,  1.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  20%|██        | 10/50 [00:19<01:17,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  22%|██▏       | 11/50 [00:21<01:14,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  24%|██▍       | 12/50 [00:25<01:32,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  26%|██▌       | 13/50 [00:27<01:30,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  28%|██▊       | 14/50 [00:29<01:24,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  30%|███       | 15/50 [00:31<01:18,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  32%|███▏      | 16/50 [00:32<01:03,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  34%|███▍      | 17/50 [00:34<01:00,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  36%|███▌      | 18/50 [00:35<00:55,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  38%|███▊      | 19/50 [00:38<00:57,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  40%|████      | 20/50 [00:39<00:51,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  42%|████▏     | 21/50 [00:41<00:48,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  44%|████▍     | 22/50 [00:42<00:43,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  46%|████▌     | 23/50 [00:45<00:54,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  48%|████▊     | 24/50 [00:46<00:45,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  50%|█████     | 25/50 [00:48<00:47,  1.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  52%|█████▏    | 26/50 [00:51<00:47,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  54%|█████▍    | 27/50 [00:51<00:37,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  56%|█████▌    | 28/50 [00:53<00:37,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  58%|█████▊    | 29/50 [00:54<00:31,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  60%|██████    | 30/50 [00:58<00:45,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  62%|██████▏   | 31/50 [01:00<00:37,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  64%|██████▍   | 32/50 [01:01<00:30,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  66%|██████▌   | 33/50 [01:03<00:30,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  68%|██████▊   | 34/50 [01:04<00:28,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  70%|███████   | 35/50 [01:06<00:26,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  72%|███████▏  | 36/50 [01:08<00:24,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  74%|███████▍  | 37/50 [01:09<00:19,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  76%|███████▌  | 38/50 [01:10<00:17,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  78%|███████▊  | 39/50 [01:13<00:19,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  80%|████████  | 40/50 [01:15<00:19,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  82%|████████▏ | 41/50 [01:16<00:15,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  84%|████████▍ | 42/50 [01:18<00:14,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  86%|████████▌ | 43/50 [01:19<00:10,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  88%|████████▊ | 44/50 [01:21<00:10,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  90%|█████████ | 45/50 [01:23<00:08,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  92%|█████████▏| 46/50 [01:24<00:06,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  94%|█████████▍| 47/50 [01:26<00:04,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  96%|█████████▌| 48/50 [01:28<00:03,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process:  98%|█████████▊| 49/50 [01:29<00:01,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Inference process: 100%|██████████| 50/50 [01:31<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "augmented_data = ''\n",
    "with open('Annotation/example.json', 'r') as f:\n",
    "    examples = json.load(f)\n",
    "for context in tqdm(context_sample, desc=\"Inference process\"):\n",
    "    prompt = prompt_generation(examples, context, 1)\n",
    "    results = pipe(prompt)\n",
    "    answers = result_formated(results[0]['generated_text'])\n",
    "    augmented_data +=  answers\n",
    "    augmented_data += '\\n\\n'\n",
    "with open(\"Annotation/Annotated_data\"+file_path.split('/')[-1]+'.txt', 'w') as f:\n",
    "    f.write(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [01:07<00:00, 22.43s/it]\n",
      "100%|██████████| 3/3 [02:28<00:00, 49.62s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.09s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model= \"mistralai/Mistral-7B-Instruct-v0.2\", max_new_tokens=512, torch_dtype=torch.bfloat16, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact = [\"The first known European to enter the region was the French explorer Robert de La Salle from Quebec during his 1669 expedition down the Ohio River.\", \"In 1859, the Clinton and Soho iron furnaces introduced coke-fire smelting to the region.\", \"The NHL's Pittsburgh Penguins have played in Pittsburgh since the team's founding in 1967.\"]\n",
    "question = [\"Who was the first known person to enter the Pittsburgh?\", \"When did coke-fire smelting introduced in Pittsburgh?\", \"When did Penguins funded?\"]\n",
    "answer = [\"The first person to enter the Pittsburgh was the French explorer Robert de La Salle\", \"Coke-fire smelting was introduced in Pittsburgh in 1859 by the Clinton and Soho iron furnaces.\",\"Penguins were funded in 1967.\"]\n",
    "#formatted in json and save it\n",
    "data = []\n",
    "for i in range(len(fact)):\n",
    "    data.append({\"fact\": fact[i], \"question\": question[i], \"answer\": answer[i]})\n",
    "import json\n",
    "with open('Annotation/example.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(examples, context, num_questions):\n",
    "    prompt = \"You are an AI assistant trained for data annotation.\\n\"\n",
    "    prompt += \"Your task is to generate **fact-question-answer pairs** based on the given factual context. You will be given a context passage, and you will select a fact from the context, then ask a question from it, and then provide the answer to the asked question based on the selected fact. \"\n",
    "    prompt += \"Ensure the questions are well-formed, unambiguous, and directly answerable using the provided context. Avoid speculative, open-ended questions, or generate any new context. \"\n",
    "    prompt += \"These are some examples with questions and answers as well as the fact that help answer that question: \\n\"\n",
    "    for i, example in enumerate(examples):\n",
    "        prompt += f\"Fact: {example['fact']}\\n\"\n",
    "        prompt += f\"Question: {example['question']}\\n\"\n",
    "        prompt += f\"Answer: {example['answer']}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += f\"Here is the context for the fact, question, and answer generation task: {context}\\n\\n\"\n",
    "    prompt += f\"Extracted {num_questions} fact-question-answer pairs based on the given context using the following format: \\n\"\n",
    "    prompt += \"Fact: \\n\"\n",
    "    prompt += \"Question: \\n\"\n",
    "    prompt += \"Answer: \\n\\n\"\n",
    "    prompt += \"Only return the fact, question, and answer you generated. Do not include any additional information.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(examples, context, num_questions):\n",
    "    prompt = \"You are an AI assistant trained for data annotation.\\n\"\n",
    "    prompt += \"Your task is to generate **question-answer pairs** based on the given factual context. You will be given a context passage, and you will select a fact from the context, then ask a question from it, and then provide the answer to the asked question based on the selected fact. \"\n",
    "    prompt += \"Ensure the questions are well-formed, unambiguous, and directly answerable using the provided context. Avoid speculative, open-ended questions, or generate any new context. \"\n",
    "    prompt += \"These are some examples with questions and answers as well as the fact that help answer that question: \\n\"\n",
    "    for i, example in enumerate(examples):\n",
    "        # prompt += f\"Fact: {example['fact']}\\n\"\n",
    "        prompt += f\"Question: {example['question']}\\n\"\n",
    "        prompt += f\"Answer: {example['answer']}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += f\"Here is the context for the question and answer generation task: {context}\\n\\n\"\n",
    "    prompt += f\"Extracted {num_questions} question-answer pairs based on the given context using the following format: \\n\"\n",
    "    # prompt += \"Fact: \\n\"\n",
    "    prompt += \"Question: \\n\"\n",
    "    prompt += \"Answer: \\n\\n\"\n",
    "    prompt += \"Only return the question and answer you generated. Do not include any additional information.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant trained for data annotation.\n",
      "Your task is to generate **question-answer pairs** based on the given factual context. You will be given a context passage, and you will select a fact from the context, then ask a question from it, and then provide the answer to the asked question based on the selected fact. Ensure the questions are well-formed, unambiguous, and directly answerable using the provided context. Avoid speculative, open-ended questions, or generate any new context. These are some examples with questions and answers as well as the fact that help answer that question: \n",
      "Question: Who was the first known person to enter the Pittsburgh?\n",
      "Answer: The first person to enter the Pittsburgh was the French explorer Robert de La Salle\n",
      "\n",
      "Question: When did coke-fire smelting introduced in Pittsburgh?\n",
      "Answer: Coke-fire smelting was introduced in Pittsburgh in 1859 by the Clinton and Soho iron furnaces.\n",
      "\n",
      "Question: When did Penguins funded?\n",
      "Answer: Penguins were funded in 1967.\n",
      "\n",
      "Here is the context for the question and answer generation task: Contents\n",
      "\n",
      "History of Pittsburgh\n",
      "\n",
      "The history of Pittsburgh began with centuries of Native American civilization in the modern Pittsburgh region, known as Jaödeogë’ in the Seneca language.[1] Eventually, European explorers encountered the strategic confluence where the Allegheny and Monongahela Rivers meet to form the Ohio, which leads to the Mississippi River. The area became a battleground when France and Great Britain fought for control in the 1750s. When the British were victorious, the French ceded control of territories east of the Mississippi.\n",
      "\n",
      "Extracted 2 question-answer pairs based on the given context using the following format: \n",
      "Question: \n",
      "Answer: \n",
      "\n",
      "Only return the question and answer you generated. Do not include any additional information.\n"
     ]
    }
   ],
   "source": [
    "with open('Annotation/example.json', 'r') as f:\n",
    "    examples = json.load(f)\n",
    "prompt = prompt_generation(examples, chunks_content[0], 2)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "results = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an AI assistant trained for data annotation.\\nYour task is to generate **question-answer pairs** based on the given factual context. You will be given a context passage, and you will select a fact from the context, then ask a question from it, and then provide the answer to the asked question based on the selected fact. Ensure the questions are well-formed, unambiguous, and directly answerable using the provided context. Avoid speculative, open-ended questions, or generate any new context. These are some examples with questions and answers as well as the fact that help answer that question: \\nQuestion: Who was the first known person to enter the Pittsburgh?\\nAnswer: The first person to enter the Pittsburgh was the French explorer Robert de La Salle\\n\\nQuestion: When did coke-fire smelting introduced in Pittsburgh?\\nAnswer: Coke-fire smelting was introduced in Pittsburgh in 1859 by the Clinton and Soho iron furnaces.\\n\\nQuestion: When did Penguins funded?\\nAnswer: Penguins were funded in 1967.\\n\\nHere is the context for the question and answer generation task: Contents\\n\\nHistory of Pittsburgh\\n\\nThe history of Pittsburgh began with centuries of Native American civilization in the modern Pittsburgh region, known as Jaödeogë’ in the Seneca language.[1] Eventually, European explorers encountered the strategic confluence where the Allegheny and Monongahela Rivers meet to form the Ohio, which leads to the Mississippi River. The area became a battleground when France and Great Britain fought for control in the 1750s. When the British were victorious, the French ceded control of territories east of the Mississippi.\\n\\nExtracted 2 question-answer pairs based on the given context using the following format: \\nQuestion: \\nAnswer: \\n\\nOnly return the question and answer you generated. Do not include any additional information.\\n\\nQuestion: Who discovered the strategic confluence of the Allegheny and Monongahela Rivers?\\nAnswer: European explorers discovered the strategic confluence of the Allegheny and Monongahela Rivers.\\n\\nQuestion: When did the British gain control of the Pittsburgh region?\\nAnswer: The British gained control of the Pittsburgh region in the 1750s.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents\\n\\nHistory of Pittsburgh\\n\\nThe history of Pittsburgh began with centuries of Native American civilization in the modern Pittsburgh region, known as Jaödeogë’ in the Seneca language.[1] Eventually, European explorers encountered the strategic confluence where the Allegheny and Monongahela Rivers meet to form the Ohio, which leads to the Mississippi River. The area became a battleground when France and Great Britain fought for control in the 1750s. When the British were victorious, the French ceded control of territories east of the Mississippi.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ADA-Disability-Rights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"raw_data/citypittsburgh/citypittsburgh_conbined.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"raw_data/description_pages\", \n",
    "    required_exts=[\".txt\"],  # Ensure only .txt files are read\n",
    "    recursive=True  # Enable recursive search inside subdirectories\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name='all-mpnet-base-v2')\n",
    "llm = HuggingFaceLLM(model_name = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document(s)\n",
    "documents = SimpleDirectoryReader(\"your_folder_path\").load_data()\n",
    "\n",
    "# Use an open-source embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load an open-source LLM for Q&A generation (Mistral-7B or smaller)\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct\",\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}  # Efficient inference\n",
    ")\n",
    "\n",
    "# Create service context\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "\n",
    "# Build the index\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '11711-anlp-spring2025-hw2/raw_data/events_pittsburgh_cmu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_content = ''\n",
    "with open('Annotation/Annotated_rawdata/new_data/penguins_roster_with_injuries.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "data = json.loads(content)\n",
    "for position in data:\n",
    "    penguins_content += f\"{position}\\n\"\n",
    "    for player in data[position]:\n",
    "        penguins_content += f\"Player information: {str(player)[1:-1]}\\n\"\n",
    "    penguins_content += '\\n\\n'\n",
    "with open('penguins_roster_with_injuries.txt', 'w') as f:\n",
    "    f.write(penguins_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
