{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xx/Documents/Repositories/anlp-spring2025-hw2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()  # * Check working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = 'raw_data/events_food_related'\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scrapable_url(url: str) -> bool:\n",
    "    skip_sites = ['instagram.com', 'facebook.com', 'x.com']\n",
    "    for site in skip_sites:\n",
    "        if site in url:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [06:46<00:00,  6.78s/it]\n"
     ]
    }
   ],
   "source": [
    "def scrape_webpage(url: str):\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    contents = [driver.title + '.']\n",
    "    for element in soup.find('body').find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        text = element.text.strip()\n",
    "        if text:\n",
    "            if element.name.startswith('h'):\n",
    "                text = f'\\n{text}'\n",
    "            if re.search(r'[\\w\\]\\}\\)]$', text):\n",
    "                text += '.'\n",
    "            contents.append(text)\n",
    "\n",
    "    filename = re.sub(r'[^a-zA-Z0-9-_]', '_', driver.title.lower())\n",
    "    filepath = f'{RAW_DATA_DIR}/{filename}.txt'\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('\\n'.join(contents).strip())\n",
    "\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "\n",
    "def get_food_festivals():\n",
    "    url = 'https://www.visitpittsburgh.com/events-festivals/food-festivals'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract events in main content\n",
    "    contents = []\n",
    "    main_body = soup.find('main')\n",
    "    for event in main_body.children:\n",
    "        if isinstance(event, str):\n",
    "            if not event.isspace():\n",
    "                contents.append(event.text)\n",
    "        else:  # One event\n",
    "            for child in event.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "                text = child.get_text(' ', strip=True)\n",
    "                if re.search(r'[\\w\\]\\}\\)]$', text):\n",
    "                    text += '.'\n",
    "                contents.append(text)\n",
    "            contents.append('')  # New line separator between events\n",
    "\n",
    "    # Save to file\n",
    "    filename = [s.lower() for s in urlparse(url).path.split('/') if len(s) > 0][-1]\n",
    "    filepath = f'{RAW_DATA_DIR}/{filename}.txt'\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('\\n'.join(contents).strip())\n",
    "\n",
    "    # Iteratively scrape hyperlinks\n",
    "    for a in tqdm(main_body.find_all('a', href=True)):\n",
    "        hyperlink = a['href']\n",
    "        if is_scrapable_url(hyperlink):\n",
    "            scrape_webpage(hyperlink)\n",
    "\n",
    "\n",
    "get_food_festivals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
