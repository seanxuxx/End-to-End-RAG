Pittsburgh, city, seat (1788) of Allegheny county, southwestern Pennsylvania, U.S. The city is located at the confluence of the Allegheny and Monongahela rivers, which unite at the point of the “Golden Triangle” (the business district) to form the Ohio River. A city of hills, parks, and valleys, it is the centre of an urban industrial complex that includes the surrounding cities of Aliquippa (northwest), New Kensington (northeast), McKeesport (southeast), and Washington (southwest) and the borough of Wilkinsburg (east). Inc. borough, 1794; city, 1816. Area city, 58 square miles (150 square km). Pop. (2010) 305,704; Pittsburgh Metro Area, 2,356,285; (2020) 302,971; Pittsburgh Metro Area, 2,370,930.
Algonquian- and Iroquoian-speaking peoples were early inhabitants of the region. The conflict between the British and French over territorial claims in the area was settled in 1758 when General John Forbes and his British and colonial army expelled the French from Fort Duquesne (built 1754). Forbes named the site for the British statesman William Pitt the Elder. The British built Fort Pitt (1761) to ensure their dominance at the source of the Ohio. Settlers began arriving after Native American forces led by Ottawa chief Pontiac were defeated in 1763; an agreement subsequently was made between Native American groups and the Penn family, and a boundary dispute was ended between Pennsylvania and Virginia. Pittsburgh was laid out (1764) by John Campbell in the area around the fort (now the Golden Triangle). Following the American Revolution, the town became an outfitting point for settlers traveling westward down the Ohio River.
Pittsburgh’s strategic location and wealth of natural resources spurred its commercial and industrial growth in the 19th century. A blast furnace, erected by George Anschutz about 1792, was the forerunner of the iron and steel industry that for more than a century was the city’s economic mainstay; by 1850 Pittsburgh was known as the “Iron City.” The Pennsylvania Canal and the Portage Railroad, both completed in 1834, opened vital markets for trade and shipping. The city suffered a great loss in 1845 when some 24 blocks of businesses, homes, churches, and other buildings were destroyed by fire.
After the American Civil War, great numbers of European immigrants swelled Pittsburgh’s population, and industrial magnates such as Andrew Carnegie, Henry Clay Frick, and Thomas Mellon built their steel empires there. The city became the focus of historic friction between labour and management, and the American Federation of Labor was born there in 1881.
By 1900 the city’s population had reached 321,616. Growth continued nearly unabated through World War II, the war years bringing a particularly great boon for the economy. The population crested at more than 675,000 in 1950, after which it steadily declined; by the end of the century, it had returned almost to the 1900 level. Most citizens were still of European ancestry, but the growing African American proportion of the population exceeded one-fourth. During the period of economic and population growth, Pittsburgh had come to epitomize the grimy, polluted industrial city. After the war, however, the city undertook an extensive redevelopment program that emphasized smoke-pollution control, flood prevention, and sewage disposal. In 1957 it became the first American city to generate electricity by nuclear power.
By the late 1970s and early ’80s, the steel industry had virtually disappeared—a result of foreign competition and decreased demand. Many of the surrounding mill towns were laid to waste by unemployment, becoming a symbol of the notorious Rust Belt, the U.S. region where steelmaking and manufacturing once thrived before succumbing to widespread unemployment and poverty. Pittsburgh, however, successfully diversified its economy through more emphasis on light industries—though metalworking, chemicals, and plastics remained important—and on such high-technology industries as computer software, industrial automation (robotics), and biomedical and environmental technologies. Numerous industrial research laboratories were established in the area, and the service sector became increasingly important. Pittsburgh long has been one of the nation’s largest inland ports, and it remains a leading transportation centre.
By Chick-fil-A
Much of the Golden Triangle has been rebuilt and includes Point State Park (containing Fort Pitt Blockhouse and Fort Pitt Museum), the Gateway Center (site of several skyscrapers and a garden), and the David L. Lawrence Convention Center. The University of Pittsburgh was chartered in 1787. Other educational institutions include Carnegie Mellon (1900), Duquesne (1878), Point Park (1960), Chatham (1869), and Carlow (1929) universities and two campuses of the Community College of Allegheny County (1966).
Central to the city’s cultural life is the Carnegie Museums of Pittsburgh (formerly Carnegie Institute), an umbrella organization consisting of a number of institutions. Its museums include those for the fine arts and natural history (both founded in 1895), the Carnegie Science Center (1991), which now also houses the Henry Buhl, Jr., Planetarium and Observatory (1939), and the Andy Warhol Museum (1994), which exhibits the works of the Pittsburgh-born artist and filmmaker. Other institutions affiliated with the organization are the Carnegie Library of Pittsburgh, which contains more than 3.3 million volumes, and the Carnegie Music Hall. The Pittsburgh Symphony Orchestra performs at Heinz Hall, a restored movie theatre.
Phipps Conservatory and Botanical Gardens (1893), which covers 15 acres (6 hectares), is noted for its extensive greenhouses. The city’s zoo, in the northeastern Highland Park neighbourhood, includes an aquarium. Two new sports venues opened in 2001 on the north bank of the Allegheny opposite the Golden Triangle: PNC Park is home of the Pirates, the city’s professional baseball team, and Acrisure Stadium houses the Steelers, its professional football team. The Penguins, Pittsburgh’s professional ice hockey team, plays at PPG Paints Arena. Popular summertime attractions include riverboat excursions on Pittsburgh’s waterways and Kennywood, an amusement park southeast of the city in West Mifflin.
Carnegie Mellon University, private, coeducational institution of higher learning in Pittsburgh, Pennsylvania, U.S. The university includes the Carnegie Institute of Technology, the College of Humanities and Social Sciences, the College of Fine Arts, the Mellon College of Science, the School of Computer Science, the H. John Heinz III School of Public Policy and Management, and the Graduate School of Industrial Administration. Undergraduate and graduate degree programs are offered in a range of fields. Total enrollment is about 7,700.
In 1900 the industrialist Andrew Carnegie gave a gift of $1 million to the city of Pittsburgh for the creation of a technical school. Originally called Carnegie Technical Schools, it was renamed Carnegie Institute of Technology in 1912. The institute merged with the Mellon Institute (established in 1913 in Pittsburgh by financier Andrew W. Mellon) in 1967. The university has built a reputation as a vital arts centre, operating three art galleries, two concert halls, and two theatres. The faculty has included Nobel Prize-winning economists Herbert Alexander Simon and Merton Miller.
University of Pittsburgh, coeducational state system of higher learning in Pennsylvania, U.S., comprising a main campus in Pittsburgh and branches in Bradford, Greensburg, Johnstown, and Titusville. The Pittsburgh campus is a comprehensive research institution of higher learning and includes 16 schools that offer more than 360 degree programs. Among these schools are those of Medicine, Dental Medicine, Law, Engineering, and Social Work. The university offers a broad range of undergraduate, graduate, and professional degree programs. Research facilities affiliated with the main campus include the Learning Research and Development Center and the Pymatuning Laboratory of Ecology. The Johnstown campus is a four-year college with a liberal arts and sciences and engineering curriculum. The branches in Bradford and Greensburg are both four-year liberal arts colleges. The Titusville branch is a junior college. Total enrollment for the system is approximately 31,400.
The university began in 1787, in a three-room log schoolhouse, as Pittsburgh Academy. In 1819 it became the Western University of Pittsburgh. The School of Medicine, originally chartered in 1886 as the Western Pennsylvania Medical College, joined the university in 1892. The school’s name was changed to the University of Pittsburgh in 1908. The central feature of the main campus is its Cathedral of Learning, a 42-story Gothic skyscraper. Notable alumni include dancer Gene Kelly, filmmaker Werner Herzog, and philanthropist Andrew Mellon. Jonas Salk conducted his polio-vaccine research while on the medical school faculty.
Appalachian Mountains, great highland system of North America, the eastern counterpart of the Rocky Mountains. Extending for almost 2,000 miles (3,200 km) from the Canadian province of Newfoundland and Labrador to central Alabama in the United States, the Appalachian Mountains form a natural barrier between the eastern Coastal Plain and the vast Interior Lowlands of North America. As a result, they have played a vital role in the settlement and development of the entire continent. They combine a heritage of natural beauty and a distinctive regional culture with contemporary problems of economic deprivation and environmental deterioration.
The system may be divided into three large physiographic regions: northern, central, and southern Appalachia. These include such mountains as, in the northern area, the Shickshocks (French: Chic-Chocs) and the Notre Dame ranges in Quebec; the Long Range on the island of Newfoundland; the great monadnock (isolated hill of bedrock) of Mount Katahdin in Maine; the White Mountains of New Hampshire; and Vermont’s Green Mountains, which become the Berkshire Hills in Massachusetts, Connecticut, and eastern New York. New York’s Catskill Mountains are in central Appalachia, as are the beginnings of the Blue Ridge range in southern Pennsylvania and the Allegheny Mountains, which rise in southwestern New York and cover parts of western Pennsylvania, western Maryland, and eastern Ohio before merging into the third, or southern, region. This area includes the Alleghenies of West Virginia and Virginia; the Blue Ridge range, extending across Virginia and western North Carolina, the northwestern tip of South Carolina, and the northeastern corner of Georgia; the Unaka Mountains in southwestern Virginia, eastern Tennessee, and western North Carolina (of which the Great Smoky Mountains are a part); and the Cumberland Mountains of eastern Kentucky, southwestern West Virginia, southwestern Virginia, eastern Tennessee, and northern Alabama.
The highest elevations in the Appalachians are in the northern division, with Maine’s Mount Katahdin (5,268 feet [1,606 metres]), New Hampshire’s Mount Washington (6,288 feet), and other pinnacles in the White Mountains rising above 5,000 feet (1,525 metres), and in the southern region, where peaks of the North Carolina Black Mountains and the Tennessee–North Carolina Great Smoky Mountains rise above 6,000 feet (1,825 metres) and the entire system reaches its highest summit, on Mount Mitchell (6,684 feet [2,037 metres]).
A distinctive feature of the system is the Great Appalachian Valley. It includes the St. Lawrence River valley in Canada and the Kittatinny, Cumberland, Shenandoah, and Tennessee valleys in the United States; the latter is the site of the world-famous Tennessee Valley Authority (TVA), a government agency for natural resource conservation, power production, and regional development.
In the area known geologically as “New” Appalachia, especially where there are softer limestone rocks that yield to the constant solution by water and weak acids, numerous caves are a distinctive feature of the physiography. The chief caverns lie within or border the Great Valley region of Pennsylvania, Maryland, West Virginia, Virginia, and Tennessee. Caverns of the Shenandoah Valley in Virginia provide well-known and dramatic examples of underground passages, rooms, watercourses, formations, and other cave features that honeycomb much of the land below central and southern Appalachia.
The Appalachians are among the oldest mountains on Earth, born of powerful upheavals within the terrestrial crust and sculpted by the ceaseless action of water upon the surface. The two types of rock that characterize the present Appalachian ranges tell much of the story of the mountains’ long existence.
First there are the most ancient crystalline rocks. Between about 1.1 billion and 541 million years ago, during the Precambrian era, long periods of sedimentation and violent eruptions alternated to create rocks and then subject them to such extreme heat and pressure that they were changed into sequences of metamorphic rocks. Among the oldest of these are the gneisses. Limestone changed into marble, shales became slate and schist, sandstones were transformed into quartzite, and intrusions of magma formed bodies of granite. These ancient rocks antedated most plant or animal life; in addition, the intense pressure and heat destroyed any traces of primitive life—so that the Precambrian crystallines contain no trace of fossils. They make up what is known as “Old” Appalachia in Canada, New England, and a belt east of the Great Valley with the Blue Ridge at its heart.
To the west the Great Valley, the Valley Ridges, and the Appalachian Plateau (including the Alleghenies) are characterized by the second type of rocks, sediments of Paleozoic age (i.e., about 252 to 541 million years old). These make up “New” Appalachia—the shales, sandstones, limestones, and coals that were formed as sediments were deposited, stratified, and solidified over geologic time. During the Carboniferous Period (358.9 to 298.9 million years ago), this long process included the formation of some of the richest coal beds in the world. During the Permian Period (298.9 to 252.2 million years ago), a great mountain folding occurred. This was the Appalachian Revolution, a vast interior crumpling resulting from the stress placed on huge masses of subterranean rock. As parts of the Earth buckled into folds, cracked, and faulted, other parts were uplifted—sometimes in the parallel ridges distinctive of the Appalachians—and thrust faults served to move one rock mass atop another. Thus, the ancient crystallines were lifted in places above the more recent sedimentary rock deposits.
In addition to the massive folding of the Alleghenian orogeny (Appalachian Revolution), however, two other agents—ice and water—have carved the steep ridges and pinnacles and gouged out the deep ravines and valleys of the Appalachians. This building, eroding, uplifting, and shaping of the Appalachians has been a continuous process throughout the ages. Many of the major rivers are older than the mountains. This accounts for the fact that northeast of the New River in Virginia the major Appalachian rivers flow into the Atlantic Ocean, often through dramatic passages called water gaps, while southwest of the New the rivers, with few exceptions, flow to the Ohio River. When the mountains were thrust up, blocking their westward course to the ancient sea that once covered the American Midwest, these old rivers cut out their own routes, creating those spectacular canyons, gorges, and “narrows” that are part of Appalachian scenery.
The northern Appalachians were also affected by glacial forces. During the Pleistocene Epoch (about 2,588,000 to 11,700 years ago), continental ice sheets flowed down over North America, covering New England but reaching no nearer the southern Appalachians than the Ohio River valley. These moving tongues of ice stripped topsoil, ground and polished certain peaks, and elsewhere scattered rock debris and random boulders, all the while driving plants and animals farther south where they could survive. Thus, the southern Appalachians became the refuge for northern life forms, a giant bed for reseeding when the glaciers retreated and the plants moved slowly north again, leaving behind a rich botanical variety thriving in northern and southern latitudes. Even today, many “northern” plant species are found in the higher elevations of the Great Smoky Mountains, where the cooler temperatures and relative isolation provide them refuge.
The New River, rising on the Blue Ridge in North Carolina, runs northward and then turns westward across the Appalachian Valley and the Alleghenies (where it becomes the Kanawha River) and empties into the Mississippi River basin. The larger mountain streams to the south, dominated by the Tennessee River, follow this example. Exceptions are the rivers rising southeastward on the Blue Ridge, which flow into the Atlantic, and the Chattahoochee, rising in the northeastern corner of Georgia, which runs southwestward into the Gulf of Mexico.
The entire Appalachian system is laced with an intricate network of springs, streams, waterfalls, and rivers. Water is most abundant in the southern Appalachians. Certain areas of the Blue Ridge receive an annual rainfall of about 70 inches (1,775 mm) during an average year. Elsewhere precipitation is even higher—the western slopes of the Great Smoky Mountains, for example, often receive as much as 90 inches per year—being exceeded in the United States only along the northwest Pacific coast. Much of this rainfall comes in extremely heavy downpours during short periods. Since this region does not have the natural storehouses of numerous lakes and glacial deposits of sand and gravel spread over hills and valleys, such as are found in the northern Appalachian region, sudden rainfalls bring rapid rises in the southern Appalachian stream flows. Under certain conditions (such as when the forest cover, which serves as a biotic buffer, has been destroyed) destructive floods and debris flows characterize much of the hydrologic history of this part of the Appalachians. Geologic evidence of past floods, landslides, and mudflows abounds, especially in the middle and southern Appalachians. There, lobes of rock, soil, and debris choke the lower reaches of many small stream valleys. Recent studies suggest that many of these ancient debris flows were initiated by hurricanes and their heavy rainfall. To contain these floods and harness the might of an entire river system for purposes of navigation, power production, land reclamation, and watershed development, the Tennessee Valley Authority was established in 1933, and it quickly became one of the chief factors influencing the ecology of the Southern Appalachian region. Its system of dams turned a river that rampaged and often destroyed into a river that flows gently and productively. The TVA created a series of spacious reservoirs (the majority of which are in or adjoining the Appalachian region) called “the Great Lakes of the South.” These lakes, in turn, have altered the natural and human resources of the region, using Appalachian water power to produce electrical power that has expanded industrial and agricultural and recreational opportunities.
Waterfalls are common throughout much of the Appalachian system. Most of those in the northern Appalachians, especially from New York to Maine, were created when glacial moraine or debris, scraped from surrounding peaks by the melting ice cap, solidified into shelves along creeks or river valleys over which the water must plunge as over a terrace. Southern Appalachian waterfalls generally were formed by the action of water on alternating layers of soft and hard rock.
Generally temperate and humid, the climate of the Appalachians presents sharp contrasts. In the Canadian ranges and the Presidential Range of the White Mountains, Arctic and subarctic conditions prevail. Elevations below 2,000 feet usually have milder weather in the hills of northwestern Georgia and northeastern and north-central Alabama. Snowfall is heaviest in the Shickshocks, Newfoundland’s Long Range, and the White Mountains, but Mount Mitchell in North Carolina has recorded more than 100 inches in a single year. Unique in climatic severity is barren, boulder-strewn Mount Washington, which is lashed by some of the world’s strongest winds (a gust of 231 miles per hour was recorded there in 1934); temperatures recorded on its summit have never risen above 71 °F (22 °C). Heavy clouds and haze are common throughout the Appalachians, often frustrating recreational activities and sightseeing but nourishing the abundant plant life and the river system.
From Maine to Georgia, the Appalachian Mountain system was once almost totally covered with forest. Today some of the best and most-extensive broad-leaved deciduous forests in the world still flourish in the Appalachians and bordering areas, notably in southern Appalachia. To the north are the conifers (red spruce and balsam fir, which grow at the highest elevations and distinguish the Canadian and Maine woods) and the northern hardwoods (sugar maple, buckeye, beech, ash, birch, and red and white oak). Farther south are hickory, poplar, walnut, sycamore, and at one time the important and—before they were destroyed by blight—plentiful chestnuts. All of these, plus other of the 140 species of trees of Appalachia, are found in the southern mountain region. Lofty elevations nurture representatives of the Canadian forest, while the western slopes of the Great Smokies, with their abundant rainfall, produce trees that have reached record maximum height and diameter. Among these are the tulip tree (yellow poplar), buckeye, eastern (Canadian) hemlock, and chestnut oak.
The interdependent system of southern plant growth known as the “Appalachian forest” is highly complex. It forms one of the great floral provinces of the Earth. There are the trees that bear luxuriant bloom, such as serviceberry, redbud, hawthorn, tulip tree, dogwood, locust, sourwood, and many others. Among the numerous shrubs with particularly showy flowers are the rhododendron, azalea, and mountain laurel. Certain summits of the southern Appalachians are called heath balds—open meadows or grasslands interspersed with thick growths of heath. Roan Mountain in the North Carolina–Tennessee Unakas is one of the most extensive of these, with some 1,200 acres of natural gardens sprawling vivid rose and pink and purple rhododendron across its high pinnacle and down its slopes. It is estimated that, of some 2,000 species of Appalachian flora, perhaps 200 are native and wholly confined to the southern Appalachians. Ferns, mosses, and mushrooms of many species also are part of the complex Appalachian plant life.
Bison, elk, and wolves, once common to the Appalachians, disappeared long ago, although elk subsequently have returned to the northern mountains; caribou and moose are still found in the northernmost corners of the region. Scattered through other areas are the black bear, white-tailed deer, wild boar, fox, raccoon, beaver, and numerous other small animals. All areas of Appalachia, from the Gaspé Peninsula to Georgia, support an abundant birdlife. In the Great Smoky Mountains alone some 200 varieties of game birds and songbirds have been recorded.
Various Eastern Woodlands Indian groups, including the Pennacook, Mohican (Mahican), and Susquehanna, inhabited the northern half of the Appalachians for centuries before European settlement. In the southern mountains, the Cherokee were predominant. Warfare and eviction had driven most of the Indian populations from the mountains by the mid-19th century. The so-called Trail of Tears, the forced march of the Cherokee to Oklahoma in the fall and winter of 1838–39, is perhaps the best-known episode of Indian removal.
The long blue wall of the Appalachians thrust up a mighty barrier to colonial expansion. Exploration and settlement was further discouraged by the size and complexity of the lateral mountain ranges, the rugged courses of many of the streams and rivers, and the ubiquitous dense forest. The central Appalachians, with their more spacious water gaps affording easy passage, attracted the largest number of early settlers. Many of these were Germans and Scotch-Irish who went into the interior of Pennsylvania and subsequently migrated down the Great Appalachian Valley in Virginia and Tennessee. In the New England Appalachians the narrow notches, often blocked by glacial debris, and the steepness of the mountains discouraged early settlement—as did the massive ruggedness of the successive ranges in the southern Appalachians—so that each of these regions remained a wilderness long after the American frontiersman Daniel Boone had forged his pioneer route through the Cumberland Gap. During the French and Indian War (1754–63), trails to the Ohio country and Great Lakes led through central Appalachia, and these, with their scattered lonely forts of the interior, became brutal battlegrounds of border warfare. Indian allies of the British waged bloody skirmishes against Appalachian settlements during the American Revolution (1775–83), but the Appalachian Mountains confined most of that struggle for independence to the Eastern Seaboard.
The diversity that characterizes Appalachian plant and animal life also exists in the system’s mineral resources. As the immense stands of timber throughout the Appalachians brought into existence important lumber and wood-pulp industries, so rich coal beds, veins of iron ore, salt springs and licks, and deposits of granite and marble created major American industries in the region. Each of these has been attended by its own peculiar problems, especially in the field of conservation. There is, for instance, pollution of Appalachian waterways by the pulp and chemical industries and the devastation of land and human resources brought on by certain coal-mining operations. Air pollution, especially in the form of acid rain and fog, has taken a toll on Appalachian forests from North Carolina to Canada. The pinnacle of the Appalachians, Mount Mitchell, once called Black Dome because of its blanket of virgin evergreens, is now a ghost forest of dead trees.
Despite the early arrival of the lumber industry and the opening of the coal mines, some areas of Appalachia remained isolated until early in the 20th century, notably those mountain areas of the southern region where rough terrain hindered road building. As a consequence, Southern Appalachian highlanders developed a distinctive culture characterized by handicrafts, ballads, folklore, and mores that reflect both the massive problems and the rich potential of the region.
By Chick-fil-A
The Appalachian region has developed into one of the premier recreational areas of North America. One unique feature of a large portion of the system is the 2,100-mile (3,400-km) Appalachian Trail. This footpath, stretching from Mount Katahdin in Maine to Springer Mountain in Georgia, provides a hiker’s grandstand on the varied ranges of the Appalachians. Overnight shelters are scattered along the way. A noncommercial motor route, the Blue Ridge Parkway, stretches 469 miles (755 km) from the Shenandoah National Park in northern Virginia to the Great Smoky Mountains National Park and is the most popular area administered by the U.S. National Park Service.
The springtime profusion of flowering wild azalea, rhododendron, and laurel is a major tourist attraction in the Appalachians, beginning in the south in April and spreading northward. In autumn the pattern is reversed, as the brilliant coloration of the foliage moves from north to south. Motoring, hiking, camping, fishing, skiing, whitewater rafting, and spelunking are encouraged throughout the Appalachians, as are visits to numerous craft centres and historic sites. Famous spas are reminders of more leisurely days in both the northern and southern mountains, while conference facilities and theme parks reflect a growing emphasis on tourism, with its attendant benefits and its problems of environmental stress.
The ruggedness of the Appalachians, the transverse ranges by which they are crossed, their maze of streams and rivers, and their lack of natural passes created a formidable barrier to early explorers and settlers. The Spanish conquistador Hernando de Soto was probably the first European to enter southern Appalachia, in 1539–40. In 1716 Governor Alexander Spotswood of Virginia led the first organized body of English colonists across the Blue Ridge Mountains. During the 1760s and ’70s Daniel Boone became America’s frontier folk hero through his exploits in exploring and settling the Blue Ridge and Cumberland Mountain country. Historical figures associated with the opening of northern Appalachia include the French explorer Samuel de Champlain, who sighted the mountains in 1605 as he sailed along the Maine coast; the American Darby Field, who made the first climb up Mount Washington (1642); Timothy Nash, discoverer of the Crawford Notch (1771), which made possible communication between the coast and the Connecticut River valley; and Sir William Logan, first director of Canada’s geologic survey, who made a cross section of the geologic formation of the Gaspé Peninsula in 1844 and became the first European to cross the Shickshock Mountains. During the mid-19th century the first extensive scientific studies of Appalachia began when in 1849 the Swiss geographer Arnold Guyot commenced mapping the eastern mountains. Starting with the White Mountains, he spent five years in northern Appalachia, then moved south to the Great Smoky Mountains area. He mapped, measured elevation, and made the first methodical effort to name mountains. The highest peak in the eastern United States was named for another pioneer explorer-scientist, Professor Elisha Mitchell, who fell to his death in 1857 while establishing the fact of this mountain’s preeminence. It remained for Horace Kephart, a St. Louis, Missouri, librarian turned naturalist who came to the southern mountains in 1904, to bring the isolated and scenic region to national attention. From his writings grew the interest and impetus that led to the establishment of the Great Smoky Mountains National Park.
Ohio River, major river artery of the east-central United States. Formed by the confluence of the Allegheny and Monongahela rivers at Pittsburgh, it flows northwest out of Pennsylvania, then in a general southwesterly direction to join the Mississippi River at Cairo, Illinois (see photograph), after a course of 981 miles (1,579 km). It marks several state boundaries: the Ohio–West Virginia, Ohio–Kentucky, Indiana–Kentucky, and Illinois–Kentucky. The Ohio River contributes more water to the Mississippi than does any other tributary and drains an area of 203,900 square miles (528,100 square km). The river’s valley is narrow, with an average width of less than 0.5 mile (0.8 km) between Pittsburgh and Wheeling (West Virginia), a little more than 1 mile (1.6 km) from Cincinnati (Ohio) to Louisville (Kentucky) and somewhat greater below Louisville.
The Ohio is navigable, and, despite seasonal fluctuations that occasionally reach flood proportions, its fairly uniform flow has supported important commerce since settlement first began. Following destructive floods at Johnstown, Pennsylvania, in 1889 and Portsmouth, Ohio, in 1937, the federal government built a series of flood-control dams. While not developed for hydropower in Ohio, the river, kept at a navigable depth of 9 feet (3 metres), carries cargoes of coal, oil, steel, and manufactured articles. It has a total fall of only 429 feet (130 metres), the one major hazard to navigation being the Falls of the Ohio at Louisville, where locks control a descent of about 24 feet (7 metres) within a distance of 2.5 miles (4 km).
The Ohio’s tributaries include the Tennessee, Cumberland, Kanawha, Big Sandy, Licking, Kentucky, and Green rivers from the south and the Muskingum, Miami, Wabash, and Scioto rivers from the north. Chief cities along the river, in addition to Pittsburgh, Cairo, Wheeling, and Louisville, are Steubenville, Marietta, Gallipolis, Portsmouth, and Cincinnati in Ohio; Madison, New Albany, Evansville, and Mount Vernon in Indiana; Parkersburg and Huntington in West Virginia; and Ashland, Covington, Owensboro, and Paducah in Kentucky.
René-Robert Cavelier, Sieur de La Salle, is said to have been the first European to see the Ohio, in 1669, and he descended it until obstructed by a waterfall (presumably the Falls at Louisville). In the 1750s the river’s strategic importance (especially the fork at Pittsburgh) in the struggle between the French and the English for possession of the interior of the continent became fully recognized. By the treaty of 1763 ending the French and Indian Wars, the English finally gained undisputed control of the territory along its banks. When (by an ordinance of 1787) the area was opened to settlement, most of the settlers entered the region down the headwaters of the Ohio.
sports, physical contests pursued for the goals and challenges they entail. Sports are part of every culture past and present, but each culture has its own definition of sports. The most useful definitions are those that clarify the relationship of sports to play, games, and contests. “Play,” wrote the German theorist Carl Diem, “is purposeless activity, for its own sake, the opposite of work.” Humans work because they have to; they play because they want to. Play is autotelic—that is, it has its own goals. It is voluntary and uncoerced. Recalcitrant children compelled by their parents or teachers to compete in a game of football (soccer) are not really engaged in a sport. Neither are professional athletes if their only motivation is their paycheck. In the real world, as a practical matter, motives are frequently mixed and often quite impossible to determine. Unambiguous definition is nonetheless a prerequisite to practical determinations about what is and is not an example of play.
Explore the ProCon debate
There are at least two types of play. The first is spontaneous and unconstrained. Examples abound. A child sees a flat stone, picks it up, and sends it skipping across the waters of a pond. An adult realizes with a laugh that he has uttered an unintended pun. Neither action is premeditated, and both are at least relatively free of constraint. The second type of play is regulated. There are rules to determine which actions are legitimate and which are not. These rules transform spontaneous play into games, which can thus be defined as rule-bound or regulated play. Leapfrog, chess, “playing house,” and basketball are all games, some with rather simple rules, others governed by a somewhat more complex set of regulations. In fact, the rule books for games such as basketball are hundreds of pages long.
As games, chess and basketball are obviously different from leapfrog and playing house. The first two games are competitive, the second two are not. One can win a game of basketball, but it makes no sense to ask who has won a game of leapfrog. In other words, chess and basketball are contests.
A final distinction separates contests into two types: those that require at least a minimum of physical skill and those that do not. Shuffleboard is a good example of the first; the board games Scrabble and Monopoly will do to exemplify the second. It must of course be understood that even the simplest sports, such as weightlifting, require a modicum of intellectual effort, while others, such as baseball, involve a considerable amount of mental alertness. It must also be understood that the sports that have most excited the passions of humankind, as participants and as spectators, have required a great deal more physical prowess than a game of shuffleboard. Through the ages, sports heroes have demonstrated awesome strength, speed, stamina, endurance, and dexterity.
Click Here to see full-size table
Sports, then, can be defined as autotelic (played for their own sake) physical contests. On the basis of this definition, one can devise a simple inverted-tree diagram. Despite the clarity of the definition, difficult questions arise. Is mountain climbing a sport? It is if one understands the activity as a contest between the climber and the mountain or as a competition between climbers to be the first to accomplish an ascent. Are the drivers at the Indianapolis 500 automobile race really athletes? They are if one believes that at least a modicum of physical skill is required for winning the competition. The point of a clear definition is that it enables one to give more or less satisfactory answers to questions such as these. One can hardly understand sport if one does not begin with some conception of what sports are.
No one can say when sports began. Since it is impossible to imagine a time when children did not spontaneously run races or wrestle, it is clear that children have always included sports in their play, but one can only speculate about the emergence of sports as autotelic physical contests for adults. Hunters are depicted in prehistoric art, but it cannot be known whether the hunters pursued their prey in a mood of grim necessity or with the joyful abandon of sportsmen. It is certain, however, from the rich literary and iconographic evidence of all ancient civilizations that hunting soon became an end in itself—at least for royalty and nobility. Archaeological evidence also indicates that ball games were common among ancient peoples as different as the Chinese and the Aztecs. If ball games were contests rather than noncompetitive ritual performances, such as the Japanese football game kemari, then they were sports in the most rigorously defined sense. That it cannot simply be assumed that they were contests is clear from the evidence presented by Greek and Roman antiquity, which indicates that ball games had been for the most part playful pastimes like those recommended for health by the Greek physician Galen in the 2nd century CE.
It is unlikely that the 7th-century Islamic conquest of North Africa radically altered the traditional sports of the region. As long as wars were fought with bow and arrow, archery contests continued to serve as demonstrations of ready prowess. The prophet Muhammad specifically authorized horse races, and geography dictated that men race camels as well as horses. Hunters, too, took their pleasures on horseback.
Among the many games of North Africa was ta kurt om el mahag (“the ball of the pilgrim’s mother”), a Berber bat-and-ball contest whose configuration bore an uncanny resemblance to baseball. Koura, more widely played, was similar to football (soccer).
Cultural variation among black Africans was far greater than among the Arab peoples of the northern littoral. Ball games were rare, but wrestling of one kind or another was ubiquitous. Wrestling’s forms and functions varied from tribe to tribe. For the Nuba of southern Sudan, ritual bouts, for which men’s bodies were elaborately decorated as well as carefully trained, were the primary source of male status and prestige. The Tutsi and Hutu of Rwanda were among the peoples who staged contests between females. Among the various peoples of sub-Saharan Africa, wrestling matches were a way to celebrate or symbolically encourage human fertility and the earth’s fecundity. In southern Nigeria, for instance, Igbo tribesmen participated in wrestling matches held every eighth day throughout the three months of the rainy season; hard-fought contests, it was thought, persuaded the gods to grant abundant harvests of corn (maize) and yams. Among the Diola of the Gambia, adolescent boys and girls wrestled (though not against one another) in what was clearly a prenuptial ceremony. Male champions were married to their female counterparts. In other tribes, such as the Yala of Nigeria, the Fon of Benin, and the Njabi of the Congo, boys and girls grappled with each other. Among the Kole, it was the kin of the bride and the bridegroom who wrestled. Stick fights, which seem to have been less closely associated with religious practices, were common among many tribes, including the Zulu and Mpondo of southern Africa.
Contests for runners and jumpers were to be found across the length and breadth of the continent. During the age of imperialism, explorers and colonizers were often astonished by the prowess of these “primitive” peoples. Nandi runners of Kenya’s Rift Valley seemed to run distances effortlessly at a pace that brought European runners to pitiable physical collapse. Tutsi high jumpers of Rwanda and Burundi soared to heights that might have seemed incredible had not the jumpers been photographed in flight by members of Adolf Friedrich zu Mecklenburg’s anthropological expedition at the turn of the 20th century.
Long before European conquest introduced modern sports and marginalized native customs, conversion to Islam tended to undercut—if not totally eliminate—the religious function of African sports, but elements of pre-Christian and pre-Islamic magical cults have survived into postcolonial times. Zulu football players rely not only on their coaches and trainers but also on the services of their inyanga (“witch doctor”).
Like the highly evolved civilizations of which they are a part, traditional Asian sports are ancient and various. Competitions were never as simple as they seemed to be. From the Islamic Middle East across the Indian subcontinent to China and Japan, wrestlers—mostly but not exclusively male—embodied and enacted the values of their cultures. The wrestler’s strength was always more than a merely personal statement. More often than not, the men who strained and struggled understood themselves to be involved in a religious endeavour. Prayers, incantations, and rituals of purification were for centuries an important aspect of the hand-to-hand combat of Islamic wrestlers. It was not unusual to combine the skills of the wrestler with those of a mystic poet. Indeed, the celebrated 14th-century Persian pahlavan (ritual wrestler) Maḥmūd Khwārezmī was both.
Typical of the place of sport within a religious context was the spectacle of 50 sturdy Turks who wrestled in Istanbul in 1582 to celebrate the circumcision of the son of Murad III. When Indian wrestlers join an akhara (gymnasium), they commit themselves to the quest for a holy life. As devout Hindus, they recite mantras as they do their knee bends and push-ups. In their struggle against “pollution,” they strictly control their diet, sexual habits, breathing, and even their urination and defecation.
While the religious aspects of Turkish and Iranian “houses of strength” (where weightlifting and gymnastics were practiced) became much less salient in the course of the 20th century, the elders in charge of Japanese sumo added a number of Shintō elements to the rituals of their sport to underscore their claim that it is a unique expression of Japanese tradition. A somewhat arbitrary distinction can be made between wrestling and the many forms of unarmed hand-to-hand combat categorized as martial arts. The emphasis of the latter is military rather than religious, instrumental rather than expressive. Chinese wushu (“military skill”), which included armed as well as unarmed combat, was highly developed by the 3rd century BCE. Its unarmed techniques were especially prized within Chinese culture and were an important influence on the martial arts of Korea, Japan, and Southeast Asia. Much less well known in the West are varma adi (“hitting the vital spots”) and other martial arts traditions of South Asia. In the early modern era, as unarmed combat became obsolete, the emphasis of Asian martial arts tended to shift back toward religion. This shift can often be seen in the language of sports. Japanese kenjutsu (“techniques of the sword”) became kendō (“the way of the sword”).
Of the armed (as opposed to unarmed) martial arts, archery was among the most important in the lives of Asian warriors from the Arabian to the Korean peninsulas. Notably, the Japanese samurai practiced many forms of archery, the most colourful of which was probably yabusame, whose mounted contestants drew their bows and loosed their arrows while galloping down a straight track some 720 to 885 feet (220 to 270 metres) long. They were required to shoot in quick succession at three small targets—each about 9 square inches (55 square cm) placed on 3-foot- (0.9-metre-) high poles 23 to 36 feet (7 to 11 metres) from the track and spaced at intervals of 235 to 295 feet (71.5 to 90 metres). In yabusame, accuracy was paramount.
In Turkey, where the composite (wood plus horn) bow was an instrument of great power, archers competed for distance. At Istanbul’s Okmeydanı (“Arrow Field”), the record was set in 1798 when Selim III’s arrow flew more than 2,900 feet (884 metres).
As can be seen in Mughal art of the 16th and 17th centuries, aristocratic Indians—like their counterparts throughout Asia—used their bows and arrows for hunting as well as for archery contests. Mounted hunters demonstrated equestrian as well as toxophilite skills. The Asian aristocrat’s passion for horses, which can be traced as far back as Hittite times, if not earlier, led not only to horse races (universal throughout Asia) but also to the development of polo and a host of similar equestrian contests. These equestrian games may in fact be the most distinctive Asian contribution to the repertory of modern sports.
In all probability, polo evolved from a far rougher game played by the nomads of Afghanistan and Central Asia. In the form that survived into the 21st century, Afghan buzkashi is characterized by a dusty melee in which hundreds of mounted tribesmen fought over the headless carcass of a goat. The winner was the hardy rider who managed to grab the animal by the leg and drag it clear of the pack. Since buzkashi was clearly an inappropriate passion for a civilized monarch, polo filled the bill. Persian manuscripts from the 6th century refer to polo played during the reign of Hormuz I (271–273). The game was painted by miniaturists and celebrated by Persian poets such as Ferdowsī (c. 935–c. 1020) and Ḥāfeẓ (1325/26–1389/90). By 627 polo had spread throughout the Indian subcontinent and had reached China, where it became a passion among those wealthy enough to own horses. (All 16 emperors of the Tang dynasty [618–907] were polo players.) As with most sports, the vast majority of polo players were male, but the 12th-century Persian poet Neẓāmī commemorated the skills of Princess Shīrīn. Moreover, if numerous terra-cotta figures can be trusted as evidence, polo was also played by aristocratic Chinese women.
There were also ball games for ordinary men and women. Played with carefully sewn stuffed skins, with animal bladders, or with found objects as simple as gourds, chunks of wood, or rounded stones, ball games are universal. Ball games of all sorts were quite popular among the Chinese. Descriptions of the game cuju, which resembled modern football (soccer), appeared as early as the Eastern Han dynasty (25–220). Games similar to modern badminton were also played in the 1st century. Finally, the Ming dynasty (1368–1644) scroll painting Grove of Violets depicts elegantly attired ladies playing chuiwan, a game similar to modern golf.
Sports were unquestionably common in ancient Egypt, where pharaohs used their hunting prowess and exhibitions of strength and skill in archery to demonstrate their fitness to rule. In such exhibitions, pharaohs such as Amenhotep II (ruled 1426–1400 BCE) never competed against anyone else, however, and there is reason to suspect that their extraordinary achievements were scribal fictions. Nonetheless, Egyptians with less claim to divinity wrestled, jumped, and engaged in ball games and stick fights. In paintings found at Beni Hassan, in a tomb dating from the Middle Kingdom (1938–c. 1630 BCE), there are studies of 406 pairs of wrestlers demonstrating their skill.
Since Minoan script still baffles scholars, it is uncertain whether images of Cretan boys and girls testing their acrobatic skills against bulls depict sport, religious ritual, or both. That the feats of the Cretans may have been both sport and ritual is suggested by evidence from Greece, where sports had a cultural significance unequaled anywhere else before the rise of modern sports. Secular and religious motives mingle in history’s first extensive “sports report,” found in Book XXIII of Homer’s Iliad in the form of funeral games for the dead Patroclus. These games were part of Greek religion and were not, therefore, autotelic; the contests in the Odyssey, on the other hand, were essentially secular. Odysseus was challenged by the Phaeacians to demonstrate his prowess as an athlete. In general, Greek culture included both cultic sports, such as the Olympic Games honouring Zeus, and secular contests.
The most famous association of sports and religion was certainly the Olympic Games, which Greek tradition dates from 776 BCE. In the course of time, the earth goddess Gaea, originally worshiped at Olympia, was supplanted in importance by the sky god Zeus, in whose honour priestly officials conducted quadrennial athletic contests. Sacred games also were held at Delphi (in honour of Apollo), Corinth, and Nemea. These four events were known as the periodos, and great athletes, such as Theagenes of Thasos, prided themselves on victories at all four sites. Although most of the events contested at Greek sacred games remain familiar, the most important competition was the chariot race. The extraordinary prestige accorded athletic triumphs brought with it not only literary accolades (as in the odes of Pindar) and visual commemoration (in the form of statues of the victors) but also material benefits, contrary to the amateur myth propagated by 19th-century philhellenists. Since the Greeks were devoted to secular sports as well as to sacred games, no polis, or city-state, was considered a proper community if it lacked a gymnasium where, as the word gymnos indicates, naked male athletes trained and competed. Except in militaristic Sparta, Greek women rarely participated in sports of any kind. They were excluded from the Olympic Games even as spectators (except for the priestess of Demeter). The 2nd-century-CE traveler Pausanias wrote of races for girls at Olympia, but these events in honour of Hera were of minor importance.
Although chariot races were among the most popular sports spectacles of the Roman and Byzantine eras, as they had been in Greek times, the Romans of the republic and the early empire were quite selectively enthusiastic about Greek athletic contests. Emphasizing physical exercises for military preparedness, an important motive in all ancient civilizations, the Romans preferred boxing, wrestling, and hurling the javelin to running footraces and throwing the discus. The historian Livy wrote of Greek athletes’ appearing in Rome as early as 186 BCE; however, the contestants’ nudity shocked Roman moralists. The emperor Augustus instituted the Actian Games in 27 BCE to celebrate his victory over Antony and Cleopatra, and several of his successors began similar games, but it was not until the later empire, especially during the reign of Hadrian (117–138 CE), that many of the Roman elite developed an enthusiasm for Greek athletics.
Greater numbers flocked to the chariot races held in Rome’s Circus Maximus. They were watched by as many as 250,000 spectators, five times the number that crowded into the Colosseum to enjoy gladiatorial combat. Nevertheless, there is some evidence that the latter contests were actually more popular than the former. Indeed, the munera, which pitted man against man, and the venationes, which set men against animals, became popular even in the Greek-speaking Eastern Empire, which historians once thought immune from the lust for blood. The greater frequency of chariot races can be explained in part by the fact that they were relatively inexpensive compared with the enormous costs of gladiatorial combat. The editor who staged the games usually rented the gladiators from a lanista (the manager of a troupe of gladiators) and was required to reimburse him for losers executed in response to a “thumbs down” sign. Brutal as these combats were, many of the gladiators were free men who volunteered to fight, an obvious sign of intrinsic motivation. Indeed, imperial edicts were needed to discourage the aristocracy’s participation. During the reign of Nero (54–68), female gladiators were introduced into the arena.
The Roman circus and the Byzantine hippodrome continued to provide chariot racing long after Christian protests (and heavy economic costs) ended the gladiatorial games, probably early in the 5th century. In many ways the chariot races were quite modern. The charioteers were divided into bureaucratically organized factions (e.g., the “Blues” and the “Greens”), which excited the loyalties of fans from Britain to Mesopotamia. Charioteers boasted of the number of their victories as modern athletes brag about their “stats,” indicating, perhaps, some incipient awareness of what in modern times are called sports records. The gladiatorial games, however, like the Greek games before them, had a powerful religious dimension. The first Roman combats, in 264 BCE, were probably derived from Etruscan funeral games in which mortal combat provided companions for the deceased. It was the idolatry of the games, even more than their brutality, that horrified Christian protesters. The less-obtrusive pagan religious associations of the chariot races helped them survive for centuries after Constantine’s conversion to Christianity in 337 CE.
The sports of medieval Europe were less well-organized than those of classical antiquity. Fairs and seasonal festivals were occasions for men to lift stones or sacks of grain and for women to run smock races (for a smock, not in one). The favourite sport of the peasantry was folk football, a wild no-holds-barred unbounded game that pitted married men against bachelors or one village against another. The violence of the game, which survived in Britain and in France until the late 19th century, prompted Renaissance humanists, such as Sir Thomas Elyot, to condemn it as more likely to maim than to benefit the participants.
The nascent bourgeoisie of the Middle Ages and the Renaissance amused itself with archery matches, some of which were arranged months in advance and staged with considerable fanfare. When town met town in a challenge of skill, the companies of crossbowmen and longbowmen marched behind the symbols of St. George, St. Sebastian, and other patrons of the sport. It was not unusual for contests in running, jumping, cudgeling, and wrestling to be offered for the lower classes who attended the match as spectators. Grand feasts were part of the program, and drunkenness commonly added to the revelry. In Germanic areas a Pritschenkoenig was supposed to simultaneously keep order and entertain the crowd with clever verses.
The burghers of medieval towns were welcome to watch the aristocracy at play, but they were not allowed to participate in tournaments or even, in most parts of Europe, to compete in imitative tournaments of their own. Tournaments were the jealously guarded prerogative of the medieval knight and were, along with hunting and hawking, his favourite pastime. At the tilt, in which mounted knights with lances tried to unhorse one another, the knight was practicing the art of war, his raison d’être. He displayed his prowess before lords, ladies, and commoners and profited not only from valuable prizes but also from ransoms exacted from the losers. Between the 12th and the 16th century, the dangerously wild free-for-all of the early tournament evolved into dramatic presentations of courtly life in which elaborate pageantry and allegorical display quite overshadowed the frequently inept jousting. Some danger remained even amid the display. At one of the last great tournaments, in 1559, Henry II of France was mortally wounded by a splintered lance.
Peasant women participated freely in the ball games and footraces of medieval times, and aristocratic ladies hunted and kept falcons, but middle-class women contented themselves with spectatorship. Even so, they were more active than their contemporaries in Heian Japan during the 8th to 12th centuries. Encumbered by many-layered robes and sequestered in their homes, the Japanese ladies were unable to do more than peep from behind their screens at the courtiers’ mounted archery contests.
By the time of the Renaissance, sports had become entirely secular, but in the minds of the 17th-century Czech educator John Amos Comenius and other humanists, a concern for physical education on what were thought to be classic models overshadowed the competitive aspects of sports. Indeed, 15th- and 16th-century elites preferred dances to sports and delighted in geometric patterns of movement. Influenced by the ballet, which developed in France during this period, choreographers trained horses to perform graceful movements rather than to win races. French and Italian fencers such as the famed Girard Thibault, whose L’Académie de l’espée (“Fencing Academy”) appeared in 1628, thought of their activity more as an art form than as a combat. Northern Europeans emulated them. Humanistically inclined Englishmen and Germans admired the cultivated Florentine game of calcio, a form of football that stressed the good looks and elegant attire of the players. Within the world of sports, the emphasis on aesthetics, rather than achievement, was never stronger.
While the aesthetic element survives in sports such as figure skating, diving, and gymnastics, the modern emphasis is generally on quantified achievement. In fact, the transition from Renaissance to modern sports can be seen in a semantic shift; the word measure, which once connoted a sense of balance and proportion, began to refer almost exclusively to numerical measurements.
Behind this epochal transition from Renaissance to modern sports lay the scientific developments that sustained the Industrial Revolution. Technicians sought to perfect equipment. Athletes trained systematically to achieve their physical maximum. New games, such as basketball, volleyball, and team handball, were consciously invented to specifications as if they were new products for the market. As early as the late 17th century, quantification became an important aspect of sports, and the cultural basis was created for the concept of the sports record. The word record, in the sense of an unsurpassed quantified achievement, appeared, first in English and then in other languages, late in the 19th century, but the concept went back nearly 200 years.
The development of modern sports having begun in late 17th-century England, it was appropriate that the concept of the sports record also first appeared there. During the Restoration and throughout the 18th century, traditional pastimes such as stick fighting and bullbaiting, which the Puritans had condemned and driven underground, gave way to organized games such as cricket, which developed under the leadership of the Marylebone Cricket Club (founded 1787). Behind these changes lay a new conception of rationalized competition. Contests that seem odd to the modern mind, such as those in which the physically impaired were matched against children, were replaced by horse races in which fleeter steeds were handicapped, a notion of equality that led eventually to age and weight classes (though not to height classes) in many modern sports. Although the traditional sport of boxing flourished throughout the 18th century, it was not until 1743 that boxer-entrepreneur Jack Broughton formulated rules to rationalize and regulate the sport. The minimal controls on mayhem imposed by Broughton were strengthened in 1867 by the marquess of Queensberry.
In the course of the 19th century, modern forms of British sports spread from the privileged classes to the common people. National organizations developed to standardize rules and regulations, to transform sporadic challenge matches into systematic league competition, to certify eligibility, and to register results.
Rowing (crew), one of the first sports to assume its modern form, began to attract a following after the first boat race between the Universities of Oxford and Cambridge (1829) and the inauguration of the Henley Regatta (1839). “Athletics” became popular after Oxford and Cambridge held their first track-and-field meet in 1864. The Amateur Athletic Association, which emphasized track-and-field sports, was founded in 1880, the Amateur Rowing Association in 1882.
Neither sport enjoyed the popularity of association football. The various versions of football played at elite schools such as Eton, Winchester, and Charterhouse were codified in the 1840s, and England’s Football Association was formed in 1863 to propagate what came to be known as “association football” (or simply “soccer”). The Rugby Football Union followed in 1871. Although the Football Association and most of its affiliated clubs were initially dominated by the middle and upper classes, soccer had definitely become “the people’s game” by the end of the century. For instance, Manchester United, one of Britain’s most storied teams, can trace its history to a club established by the city’s railroad workers in 1880.
The entry of working-class athletes into soccer and other sports, as participants if not as administrators, inspired Britain’s middle and upper classes to formulate the amateur rule, which originally excluded not only anyone paid for athletic performances but also anyone who earned his living by manual labour of any sort.
From the British Isles, modern sports (and the amateur rule) were diffused throughout the world. Sports that originally began elsewhere, such as tennis (which comes from Renaissance France), were modernized and exported as if they too were raw materials imported for British industry to transform and then export as finished goods.
In the 18th and 19th centuries, the British expelled the French from Canada and from India and extended British rule over much of Africa. To the ends of the earth, cricket followed the Union Jack, which explains the game’s current popularity in Australia, South Asia, and the West Indies. Rugby football flourishes in other postcolonial cultures, such as New Zealand and South Africa, where the British once ruled. It was, however, association football’s destiny to become the world’s most widely played modern sport.
Cricket and rugby seemed to require British rule in order to take root. Football needed only the presence of British economic and cultural influence. In Buenos Aires, for instance, British residents founded clubs for cricket and a dozen other sports, but it was the Buenos Aires Football Club, founded June 20, 1867, that kindled Argentine passions. In almost every instance, the first to adopt football were the cosmopolitan sons of local elites, many of whom had been sent to British schools by their Anglophile parents. Seeking status as well as diversion, middle-class employees of British firms followed the upper-class lead. From the gamut of games played by the upper and middle classes, the industrial workers of Europe and Latin America, like the indigenous population of Africa, appropriated football as their own.
By the late 19th century, the United States had begun to rival Great Britain as an industrial power and as an inventor of modern sports. Enthusiasts of baseball denied its origins in British children’s games such as cat and rounders and concocted the myth of Abner Doubleday, who allegedly invented the game in 1839 in Cooperstown, New York. A more plausible date for the transformation of cat and rounders into baseball is 1845, when a New York bank clerk named Alexander Cartwright formulated the rules of the Knickerbocker Base Ball Club. Even before the Civil War, the game had been taken over by urban workers such as the volunteer firemen who organized the New York Mutuals in 1857. By the time the National League was created in 1876, the game had spread from coast to coast. (It was not until the 1950s, however, that Major League Baseball planted its first franchises on the West Coast.)
Basketball, invented in 1891 by James Naismith, and volleyball, invented four years later by William Morgan, are both quintessentially modern sports. Both were scientifically designed to fulfill a perceived need for indoor games during harsh New England winters.
Football (soccer) is the world’s most popular ball game, but, wherever American economic and culture influence has been dominant, the attraction to baseball, basketball, and volleyball has tended to exceed that to football. Baseball, for example, boomed in Cuba, where Nemesio Guilló introduced the game to his countrymen in 1863, and in Japan, where Horace Wilson, an American educator, taught it to his Japanese students in 1873. Since basketball and volleyball were both invented under the auspices of the YMCA (Young Men’s Christian Association), it seemed reasonable for YMCA workers to take the games to China, Japan, and the Philippines, where the games took root early in the 20th century. It was, however, only in the post-World War II world that U.S. influence generally overwhelmed British; only then did basketball and volleyball become globally popular.
American football, which now enjoys enclaves of enthusiasm in Great Britain and on the European continent, traces its origins to 1874, when a rugby team from Montreal’s McGill University traveled to Cambridge, Massachusetts, to challenge a team of Harvard University students. Adopted by American students, rugby evolved into gridiron football, and in that form it became the leading intercollegiate game. Although the National Football League was established in 1920 (at $100 a franchise), the professional game was a relatively minor affair until after World War II, when football joined baseball and basketball to form the “trinity” of American sports. (Ice hockey, imported from Canada, runs a poor fourth in the race for fans of team sports.)
In the dramatic global diffusion of modern sports, the French have also played a significant role. They left it to an Englishman, Walter Wingfield, to modernize the game of tennis, which originated in Renaissance France, but the French took the lead, early in the 19th century, in the development of the bicycle and in the popularization of cycling races. The first Paris–Rouen race took place in 1869; the Tour de France was inaugurated in 1903. The huge success of the latter inspired the Giro d’Italia (1909) and a number of other long-distance races.
The French also left their mark on sports in another way. In 1894, at a conference held at the Sorbonne in Paris, Pierre de Coubertin selected the first members of a Comité International Olympique (International Olympic Committee; IOC) and arranged for the first Olympic Games of the modern era to be held in Athens in 1896. In 1904 Robert Guérin led a group of football (soccer) enthusiasts in forming the Fédération Internationale de Football Association (FIFA), which England’s insular Football Association was at first too arrogant to join. The English name of the International Amateur Athletic Federation (1912; since 2001 known as the International Association of Athletics Federations; IAAF) suggests that the British were more cooperative in track-and-field sports than in football, but the IAAF’s founder was a Swedish industrialist, Sigfrid Edström.
Japan, one of the few non-Western nations where traditional sports still rival modern ones in popularity, is also one of the few non-Western nations to contribute significantly to the repertory of modern sports. Judo, invented in 1882 by Kanō Jigorō in an effort to combine Western and Asian traditions, attracted European adherents early in the 20th century. In 1964 judo became an Olympic sport.
From 1952, when the Soviet Union emerged from its self-imposed sports isolation, to 1991, when the Union of Soviet Socialist Republics ceased to exist, the communist societies of eastern Europe dominated the Olympic Games. In 1988, for instance, the German Democratic Republic (East Germany), with a population of some 16 million, outscored the United States, 15 times its size. While anabolic steroids and other banned substances contributed to the East Germans’ triumph, credit must also be given to their relentless application of scientific methods in the search for the ultimate sports performance. The collapse of communism undermined state-sponsored elite sports in eastern Europe, but not before the nations of western Europe had begun to emulate their athletic adversaries by sponsoring scientific research, subsidizing elite athletes, and constructing vast training centres.
In the 20th century, sports underwent social as well as spatial diffusion. After a long and frequently bitter struggle, African Americans, Australian Aboriginal people, “Cape Coloureds” (in South Africa), and other excluded racial and ethnic groups won the right to participate in sports. After a long and somewhat less-bitter struggle, women also won the right to compete in sports—such as rugby—that had been considered quintessentially masculine.
While the British Isles may be considered the homeland of modern sports, modern physical education can be traced back to German and Scandinavian developments of the late 18th and early 19th centuries. Men such as Johann Christoph Friedrich Guts Muths in Germany and Per Henrik Ling in Sweden elaborated systems of gymnastic exercise that were eventually adopted by school systems in Britain, the United States, and Japan. These noncompetitive alternatives to modern sports also flourished in eastern Europe during the late 19th and early 20th centuries. Among repressed ethnic peoples such as the Poles and Czechs, gymnastics became almost a way of life. For them, gymnastic festivals were grand occasions at which tens of thousands of disciplined men and women demonstrated nationalistic fervour.
Gymnastic fervour was not, however, much in evidence among the world’s schoolchildren and college students as they encountered gymnastics in required physical-education classes. Calisthenic exercises designed to improve health and fitness were dull and dreary compared with the excitement of modern sports. Long before the end of the 20th century, even German educators had abandoned Leibeserziehung (“physical education”) in favour of Sportunterricht (“instruction in sports”). For young and for old, for better and for worse, sports are the world’s passion.
Although the German scholar Heinz Risse published Soziologie des Sports (“Sociology of Sports”) in 1921, it was not until 1966 that an international group of sociologists formed a committee and founded a journal to study the place of sports in society. Since then, many universities have established centres for research into the sociology of sports. Organizations such as the North American Society for the Sociology of Sport have proliferated. Prominent among the topics investigated by sports sociologists are socialization into and through sports; sports and national identity; globalization and sports processes; elite sports systems; labour migration and elite sports; mass media and the rise of professional sports; commercialization of sports; violence and sports; gender and sports; race, ethnicity, and sports; and human performance and the use of drugs.
Several questions are central to understanding the socialization into sports. How exactly are young people socialized to become involved in sports and to stay involved in them? Why do some continue to participate actively in sports throughout their lives while others are content to watch? Different questions arise when one asks how people are changed as a result of their socialization into sports. Why do some people find their primary identity as athletes, and what happens when injury, age, or loss of motivation brings their athletic careers to an end? More generally, what impact do sports have on an individual’s character, relationships, thoughts, and feelings?
Socialization is the process by which people become familiar with and adapt themselves to the interpersonal relationships of their social world. Through socialization, people develop ideas about themselves and about those with whom they interact. Inevitably, socialization is a two-way process that affects everyone to a greater or lesser degree. It takes place throughout one’s life, but it is during the early years that the most crucial phases occur. In these phases a person’s sense of self, social identity, and relationships with others are shaped.
Play, games, contests, and sports have crucial and quite specific roles in the general socialization process. The sense of self is not natural; it develops through childhood socialization as a result of role-playing. Influenced by George Herbert Mead and Jean Piaget among others, sociologists have identified two stages in childhood socialization: a “play stage” and a “game stage.” In the play stage (more accurately, the stage of noncompetitive games), children play the role of a father, mother, teacher, firefighter, or athlete. Children learn the difference between their real selves and the parts they are playing. As they grow older, children shift from noncompetitive games (such as peekaboo and playing house) to contests (such as footraces and ball games). In the game stage (more accurately, the stage of competitive games), children encounter stricter rules and regulations. They develop a reflexive conception of the self and its position in relation to others, and they learn to see themselves as others see them. Through socialization with “significant others” and with the “generalized other,” children develop their sense of identity and self. They become self-conscious social actors.
In most premodern societies, boys were encouraged by their families to compete in sports, which were presumed to prepare them for their adult roles as warriors and workers, while girls were encouraged to continue to play noncompetitive games that prepared them for motherhood. In modern societies, boys and young men continue to outnumber girls and young women involved in sports competition, but the gender gap has narrowed considerably. This has been true for the private clubs that organize European sports as well as for the interscholastic and intercollegiate teams that are a prominent feature of the North American sports landscape.
The role of socializer into sports has been played by many actors, among them parents, older siblings, peers, teachers, coaches, and elite athletes appearing in the mass media. In the course of the 20th century, parents and older siblings became relatively less influential while coaches and elite athletes became more influential.
In modern as in premodern societies, there is a tendency for sports participation to decline with age because of both the added responsibilities and time demands of paid employment and of parenthood and the physical decline of the body. Early socialization into sports is the best predictor of lifelong involvement in sports. Those who disliked sports as children are unlikely to become involved as adults, while those who loved sports are likely to participate throughout their lives. Elite athletes may be an exception to this rule. If pushed as children to compete nationally and internationally, they are liable to experience burnout and to abandon their sports careers before reaching adulthood.
The value of socialization through sports has long been recognized, which is one reason for state support of physical education in the schools and adult-organized children’s sports programs. The effects of sports socialization, however, are not always what the socializers expect. They are in fact quite controversial. From the mid-19th to the early 21st century, sports were alleged to train young athletes in self-discipline, teamwork, leadership, and other highly prized traits and behaviours. Empirical research has shown that involvement in sports can also inculcate a socially destructive desire to win at all costs. Depending on the values of the socializing agents, sports can encourage young people to play fairly or to cheat. The evidence suggests that the propensity to cheat increases with age and the level of competition.
Another important aspect of the experience of sports is emotion, the feelings that reflect athletes’ self-evaluation or expectation of their performance and their perception of others’ evaluations and expectations. Some of the feelings expressed are anticipatory, prior to performing. Pregame “butterflies in the stomach” are as familiar to an athlete as stage fright is to an actor. Other feelings occur during and after the performance. All these feelings are “scripted” by the subculture of the sport in question. These scripts, or “feeling rules,” guide athletes as they manage their emotions, prompting, for instance, appropriate behaviour during pregame renditions of national anthems or during postgame victory celebrations. Norms for the display of emotions vary widely among sports. Rugby players and boxers are permitted to express their feelings with ostentatious displays that are impermissible for golfers and sumo wrestlers. The importance of the contest is another variable influencing the emotions involved. Exhibition matches evoke less-intense feelings than football’s World Cup championship game.
The orchestration of emotions in sports begins with the arousal of expectations, provoking a diffuse emotional state that is then directed into a series of discrete and identifiable emotional displays. In other words, competitors become “psyched up.” In elite sports, players have already internalized the scripts that coaches call upon them to rehearse immediately before the contest and to adhere to during the contest. It is not, however, just the players who experience this scripting. Drawing upon fans’ previous experiences, media pundits and other “stage setters” also contribute to the management of the fans’ emotions. Cues provided by the stage setters prompt fans to express a variety of emotions throughout a game. These emotions range from passionate identification with one’s representative team and with one’s fellow fans to hatred for the opposing team and its misguided supporters. Fans feel despair when an idolized player is injured; they feel ecstasy when a last-minute goal transforms humiliating defeat into triumphant victory.
While there may be a scripting or an orchestration of the emotions, individuals vary in the degree to which they internalize and follow scripts. Despite such individual variations, rules do structure the emotional experience of sports subcultures. These emotional processes, which help define roles of players, coaches, and fans, also help forge the link between sports and national identity.
In addition to the social practices that contribute actively to a nation’s image, national cultures are characterized by competing discourses through which people construct meanings that influence their self-conception and behaviour. These discourses often take the form of stories that are told about the nation in history books, novels, plays, poems, the mass media, and popular culture. Memories of shared experiences—not only triumphs but also sorrows and disasters—are recounted in compelling ways that connect a nation’s present with its past. The construction of a national identity in large part involves reference to an imagined community based on a range of characteristics thought to be shared by and specific to a set of people. Stories and memories held in common contribute to the description of those characteristics and give meaning to the notion of nation and national identity. Presented in this way, nationalism can be used to legitimize, or justify, the existence and activities of modern territorial states.
Sports, which offer influential representations of individuals and communities, are especially well placed to contribute to this process of identity formation and to the invention of traditions. Sports are inherently dramatic (from Greek dran, “to act, do, perform”). They are physical contests whose meanings can be “read” and understood by everyone. Ordinary citizens who are indifferent to national literary classics can become emotionally engaged in the discourses promoted in and through sports. Sometimes the nationhood of countries is viewed as indivisible from the fortunes of the national teams of specific sports. Uruguay, which hosted and won the first World Cup football championship in 1930, and Wales, where rugby union is closely woven with religion and community to reflect Welsh values, are prime examples. In both cases national identity has been closely tied to the fortunes of male athletes engaged in the “national sport.” England’s eclipse as a cricket power is often thought, illogically, to be symptomatic of a wider social malaise. These examples highlight the fact that a sport can be used to support, or undermine, a sense of national identity. Clifford Geertz’s classic study of Balinese cockfighting, Deep Play: Notes on the Balinese Cockfight (1972), illustrates another case in point. Although Balinese culture is based on the avoidance of conflict, men’s identification with their birds allows for the vicarious expression of hostility.
By the beginning of the final decades of the 19th century, sports had become a form of “patriot games” in which particular views of national identity were constructed. Both established and outsider groups used and continue to use sports to represent, maintain, and challenge identities. In this way sports can either support or undermine hegemonic social relations. The interweaving of sports and national identity politics can be illustrated with several telling examples.
In 1896 a team of Japanese schoolboys soundly defeated a team of Americans from the Yokohama Athletic Club in a series of highly publicized baseball games. Their victories, “beating them at their own game,” were seen as a national triumph and as a repudiation of the American stereotype of the Japanese as myopic weaklings.
Similarly, the “bodyline” controversy of the 1932–33 cricket Test series between Australia and England exemplifies the convergence of sports and politics. At issue were the violent tactics employed by the English bowlers, who deliberately threw at the bodies of the Australian batsmen in order to injure or intimidate them. The bowlers’ “unsporting” behaviour raised questions about fair play, good sportsmanship, and national honour. It also jeopardized Australia’s political relationship with Great Britain. So great was the resulting controversy that the Australian and British governments became involved. Arguably, one consequence was the forging of a more independent attitude in Australians’ dealings with the British in the political, economic, and cultural realms.
The Soviet Union’s military suppression of reformist efforts to create “socialism with a human face” in Hungary (1956) and in Czechoslovakia (1968) were followed by famous symbolic reenactments of the conflicts in the form of an Olympic water-polo match (U.S.S.R. versus Hungary) and an ice hockey encounter (U.S.S.R. versus Czechoslovakia). In both cases, sports were invested with tremendous political significance, and the Soviet team’s defeat was seen as a vindication of national identity.
In each of these examples, a historical legacy was invoked, past glories or travesties were emphasized, and the players were faced with maintaining or challenging a set of invented traditions. This link between sports, national culture, and identity can be extended further. Some sports are seen to encompass all the qualities of national character. In the value system of upper-class Englishmen, for example, cricket embodies the qualities of fair play, valour, graceful conduct, and steadfastness in the face of adversity. Seen to represent the essence of England, the game is a focus of national identification in the emotions of upper-class males. Moreover, just as Englishness is represented as an indefinable essence too subtle for foreigners to comprehend, so too are the mysteries of cricket deemed to be inscrutable to the outsider.
In a similar manner, bullfighting has been portrayed in the visual and the verbal arts as a material embodiment of the Spanish soul, Gaelic football is thought to be an expression of an authentic Irishness, and sumo wrestling is said to represent the indefinable uniqueness of Japanese culture (which is why foreign-born sumo wrestlers are almost never elevated to the sport’s highest rank of yokozuna).
National culture and identity are also represented by an emphasis on origins, continuity, tradition, and timelessness. For most English people, for example, the origins of their culture and national identity seem to be lost in antiquity. Englishness is taken for granted as the result of centuries of uninterrupted tradition. This emphasis on continuity is strikingly evident in sports contests between nations. Accordingly, when teams from England and Scotland compete, they are characterized as “auld enemies.” That political institutions are also imbued with a sense of venerable tradition is easily exemplified in the pageantry that surrounds the English monarchy. Yet the traditions associated with both the monarchy and sports are not as old as claimed. Indeed, both appear to be based on foundational myths—that is, on myths that seek to locate the origins of a nation, a people, or a national character much earlier in time and place than the evidence supports.
Baseball, which for a century was considered to be the "national game" of the United States, is a case in point. Instead of tracing the origins of the game to its English roots in children’s games such as cat and rounders, Americans accepted the addled recollections of a lone octogenarian and credited Abner Doubleday with having invented a game that he may never have played. Similarly, Italians use the word calcio to describe the sport known to the rest of the world as “association football,” as “soccer,” or simply as “football” (or “fútbol” or “voetbal” or another cognate). The use of calcio implies that the origins of modern football can be traced to Renaissance Italy. Sumo provides another striking example of invented tradition. The colourful traditional costume worn by sumo officials suggests that the sport has evolved almost unchanged since the 11th century, but the costume was actually devised in 1909 during a period of intense nationalism.
The role sports play in the interaction of culture and national identity is sometimes viewed as inherently conservative. Some believe that the association of sports with nationalism goes beyond mere patriotism and becomes chauvinistic and xenophobic. The behaviour of football hooligans at international matches lends support to the argument. On the other hand, sports also have contributed to liberal nationalist political struggles. One frequently cited example is the 19th-century Slavic gymnastics movement known as Sokol (“Falcon”). Gymnastic clubs in what is now the Czech Republic, Slovakia, and Poland were in the forefront of the struggle for national liberation from Austrian and Russian rule. A similar role was played by Algerian football clubs when they became centres of resistance to French colonialism. Sports—through the use of nostalgia, mythology, invented traditions, flags, anthems, and ceremonies—contribute greatly to the quest for national identity. Sports serve to nurture, refine, and develop the sense that nations have of themselves. Yet, in the context of global sports, this role has become increasingly contradictory. In introducing people to other societies, global sports strengthen cosmopolitanism even as they feed ethnic defensiveness and exclusiveness. For example, the development of cricket in South Asia reflects that region’s imperial past and postcolonial present, but the game has taken on uniquely Indian, Pakistani, and Sri Lankan attributes far removed from the pastoral values associated with the English village green.
The globalization of sports is part of a much larger—and much more controversial—globalization process. Examined historically and analytically, this larger globalization process can be understood as the development of a worldwide network of interdependencies. The 20th century witnessed the advent of a global economy, a transnational cosmopolitan culture, and a variety of international social movements. As a result of modern technology, people, money, images, and ideas are able to traverse the globe with tremendous speed. The development of modern sports was influenced by the interwoven economic, political, social, and cultural patterns of globalization. These patterns both enable and constrain people’s actions, which means that there are winners and losers in the diffusion of modern sports from Europe and North America to the rest of the world.
The emergence and diffusion of modern sports in the 19th and 20th centuries are clearly part of the larger process of globalization. The globalization of sports has been characterized by the creation of national and international sports organizations, the standardization and worldwide acceptance of the rules and regulations for individual and team sports, the development of regularly scheduled international competitions, and the establishment of special competitions, such as the Olympic Games and the various world championships, that aspire to involve athletes from nations in all corners of the globe.
The emergence and diffusion of modern sports is bound up in complex networks and interdependency chains that are marked by unequal power relations. The world can be understood as an interdependent whole, where groups constantly compete for dominant (or less-subordinate) positions. In sports as in other social realms, Europe and North America have been hegemonic. Modern sports are to an overwhelming degree Western sports. As modern sports spread throughout the world, the myriad traditional sports of Asia, Africa, and South America were marginalized. Sports such as Japanese kemari and Afghan buzkashi survive as folkloric curiosities.
No master plan has governed the process of sports globalization. Throughout the period of Western imperialism that reached its apogee in the late 19th and early 20th centuries, colonized peoples were often forced to adopt Western sports. (This was especially true at missionary schools.) More often than not, however, politically and economically colonized peoples were motivated by emulation. Anglophile Argentines formed football teams not because they were coerced to play but rather because football was the game played by the English whom they admired. More recently, however, as transnational corporations have sought to sell every kind of product to every reachable consumer, modern sports have been systematically marketed to the entire world, not only as sources of pleasure but also as signs of distinction, prestige, and power.
Western values and capitalist marketing, advertising, and consumption have influenced the ways people throughout the world construct, use, represent, imagine, and feel about their bodies. Unquestionably, there is a political economy at work in the production and consumption of global sports and leisure products that has resulted in the relative ascendancy of a narrow selection of Western sports, but non-Western sports and attitudes toward the physical self have not completely disappeared. Not only have they survived, but some of them, such as the martial arts and yoga, have also found a prominent place in the sports and body cultures of Europe and North America.
It is possible, therefore, to overstate the extent to which the West has dominated in terms of global sports structures, organizations, and ideologies. As noted, non-Western cultures resist and reinterpret Western sports and maintain, foster, and promote on a global scale their own indigenous recreational pursuits. The popularity of Asian martial arts in Europe and the Americas is one sign of this. In other words, global sports processes involve multidirectional movements of people, practices, customs, and ideas that reflect a series of shifting power balances. These processes have unintended as well as intended consequences. While the intentional actions of transnational agencies or corporations such as the International Olympic Committee (IOC) or Nike, Inc., are probably more significant in the short term, over the longer term the unintentional, relatively autonomous transnational practices predominate. The 19th-century diffusion of football (soccer) is one example of this sort of globalization. The 20th-century diffusion of surfboarding from Hawaii is another.
In sum, the speed, scale, and volume of sports development can be imagined as eddies within the broader global flows of people, technology, finance, images, and ideologies that are dominated by Europe and North America (whose elites are predominantly white males). There are, however, signs that global processes may be leading to the diminution of Western power in a variety of contexts, including sports. Sports may become increasingly contested, with Asian and African cultures challenging 19th- and 20th-century hegemonic masculine notions regarding the content, meaning, control, organization, and ideology of sports. Moreover, global flows are simultaneously increasing the varieties of body cultures and identities available to people in local cultures. Global sports, then, seem to be leading not only to the reduction in contrasts between societies but also to the simultaneous emergence of new varieties of body cultures and identities.
That international sports success in the late 20th century involved a contest between systems located within a global context was vividly displayed in the sporting struggles of the Cold War era. From the 1950s to the dissolution of the Soviet Union in the 1990s, there was intense athletic rivalry between the Soviet bloc on the one hand and the United States and its allies on the other. On both sides of the Iron Curtain, sports victories were touted as proof of ideological superiority. A partial list of the most memorable Soviet-Western showdowns might include the Soviet Union’s disputed victory over the U.S. basketball team in the final seconds of the gold medal game of the 1972 Summer Olympics; Canada’s last-minute goal against the Soviet Union in the concluding game of their 1972 eight-game ice hockey series; the defeat of the veteran Soviet ice hockey team by a much younger American squad at the 1980 Winter Olympics; and a number of track-and-field showdowns between East and West Germany.
Success in these encounters depended on several factors, among them the identification and recruitment of human resources (including coaches and trainers as well as athletes), innovations in coaching and training, advances in sports medicine and sports psychology, and—not surprisingly—the expenditure of a significant portion of the gross domestic product to support these systems. While neglecting the infrastructure for recreational sports for ordinary citizens, the Soviet Union and the German Democratic Republic (East Germany) sought to enhance their international prestige by investing huge sums in elite sports. At universities and sports centres in Moscow, Leipzig, Bucharest, and elsewhere, Soviet-bloc countries developed an elaborate sports-medicine and sports-science program (allied in the case of East Germany with a state-sponsored drug regime). For a time, the Soviet-bloc countries were outcompeting their Western counterparts, but the major Western sporting nations began to create similar state-sponsored programs. Poorer nations, with the notable exception of Fidel Castro’s Cuba, were for the most part unable or unwilling to dedicate scarce economic resources to the athletic “arms race.” As a result, they had difficulty competing on the world stage.
Even after the dissolution of the Soviet bloc, an international order persists in which nations can be grouped into core, semiperipheral, and peripheral blocs, not by geography but rather by politics, economics, and culture. The core of the sports world comprises the United States, Russia, western Europe, Australia, New Zealand, and Canada. Japan, South Korea, Cuba, China, Brazil, and several of the former Soviet-bloc states can be classified as semiperipheral sports powers. On the periphery are most Asian, African, and Latin American nations. The core may be challenged on the field of play in one sport or another (East African runners dominate middle-distance races), but control over the ideological and economic resources associated with sports still tends to lie in the West, where the IOC and the headquarters of nearly all the international sports federations are located. Despite their relative weakness in international competition, noncore countries have used regularly recurring sports festivals, such as the Asian Games, to solidify regional and national identities and to enhance international recognition and prestige.
Despite programs such as Olympic Solidarity, which provides aid and technical assistance to poorer nations, material resources still tend to be concentrated in the core nations, while those on the periphery lack the means to develop and retain their athletic talent. They lose many of their best athletes to more powerful nations that can offer better training facilities, stiffer competition, and greater financial rewards. The more commercialized the sport, the greater the “brawn drain.” At the turn of the 21st century, Western nations recruited not only sports scientists and coaches from the former Soviet bloc but also athletic talent from Africa and South America. This was especially true in sports such as football, where players were lured by the lucrative contracts offered by European and Japanese clubs. Noncore leagues remain in a dependent relationship with the dominant European core. In other sports, such as track and field and baseball, this drain of talent flows to the United States. Despite some competition from Japan, the West also remains overwhelmingly dominant in terms of the design, production, and marketing of sportswear and equipment.
Labour migration is an important and established feature of the sporting “global village.” While this movement of workers primarily involves athletes, it also includes coaches, officials, administrators, and sports scientists. Although migrant labour has been a feature of the sports process since ancient times, the phenomenon increased in complexity and intensity during the last decades of the 20th century. This acceleration is closely tied to globalization processes.
The migration of athletes and others involved in sports occurs at three levels: within nations, between nations located on the same continent, and between nations located in different continents and hemispheres. Extensive migration within nations has been common since the beginnings of modern sports in the 18th century, but intercontinental migration was infrequent before the 20th century. Recent examples of intracontinental migration include the flow of baseball players from the Dominican Republic to the United States and of eastern European football, ice hockey, and basketball players to western Europe. Coaches in these and other sports have joined the exodus. Availing themselves of their new freedom of movement, Poles, Hungarians, Czechs, Slovaks, and Romanians have moved west. Eastward expansion of the European Union, whose rules have further liberalized the labour market, has accelerated this migration.
Movement of sports labour also occurs between North America, Europe, South America, Africa, and Asia in many sports, including football (soccer), baseball, and basketball. Canadians play ice hockey in Britain, Germany, France, and Switzerland; conversely, there is a flow of sports labour in the opposite direction when North American ice hockey teams acquire Russian, Czech, and Scandinavian players. American universities actively recruit Europeans to participate in track and field, football (soccer), rugby, basketball, and swimming, while large numbers of Africans have competed at the college level in the United States in basketball and track-and-field sports. What had begun as the unilateral movement of American basketball players to European professional leagues in the 1960s became a two-way flow by the end of the century, and the number of international players in the National Basketball Association increased dramatically. Similarly, while American baseball players had for decades competed on Japanese teams, beginning in the 1990s a few elite Japanese players made an impact on Major League Baseball. Australian, Afro-Caribbean, South Asian, and South African players have for decades figured prominently in English cricket.
The migration of athletes between nations is sometimes complicated by the imposition by professional leagues and associations of quotas that limit the number of foreign players a team can field. In some cases these restrictions are circumvented when a player is able to claim ancestral links to another country, as Diego Maradona did when he moved from Argentina to Italy.
In specific sports, such as cricket and rugby, labour migration has a seasonal pattern, with the Northern and Southern hemispheres scheduling two different seasons of play. One consequence is that the natural rhythm of the traditional sporting calendar (most often governed by climate) has diminished in importance. In other sports, participants experience an even more transitory form of migration because their “workplace” constantly changes as the venue for competition shifts. Examples include the experience of European, American, and African track-and-field athletes on Europe’s Grand Prix circuit and that of European and North American skiers competing in World Cup Alpine skiing.
Occasionally seasonal and transitory migration patterns interweave, as they do for golf and tennis players. Tennis stars crisscross the globe in pursuit of Grand Slam titles and points that determine their world ranking. These migratory forays tend to last no more then eight days per tournament venue. In this respect, tennis players and golfers are probably the ultimate nomads of the sports migration process, with constantly shifting workplaces and places of residence. Migrant athletes have generally improved their lives, experiencing social as well as spatial mobility, but they have also experienced economic exploitation, dislocation, and culture shock.
Gender relations play a significant role in contouring a migrant athlete’s life. The disadvantages of sports migration have been greater for female athletes. Although women now travel more frequently and in greater numbers than in the past, men continue to move more freely (and to be paid more generously). This pattern results from social structures that continue to assume that women are solely responsible for domestic matters and child care.
As with broader global processes, an economic analysis is a necessary but insufficient explanation of sports migration. The migrant trails of world sports are constructed by shifting sets of multilayered interdependencies that include not only economic but also political, historical, geographic, social, and cultural factors. As with global sports in general, a broad approach must be taken to make sense of these migration processes.
The experience of migrant athletes once they arrive in a host country (along with the impact of their presence on the hosts) is determined by a wide range of factors, including the residual impact of colonial heritages and cultural traditions; cultural and legal encouragement or discouragement of migration; economic, social, and cultural dependency; and political changes within and between societies and power blocs. A number of processes that are more immediately related to sports are also involved. Special status is ascribed to particular sporting traditions and particular leagues. Young cricket players are often eager to bowl and bat in England; aspiring football players dream of a career in Germany’s Bundesliga. Ethnic and racial stereotyping, which categorizes athletes as desirable or undesirable candidates for recruitment, also plays a role. Other factors influencing migration include the political, economic, and playing ambitions of individual clubs, leagues, and national associations; the role of agents and coaching networks; and the resources available for the identification, development, and exploitation of new talent sources. All of these factors will influence the speed, scale, and volume of future sports migration.
The relationship between mass media and sports has profoundly influenced both institutions. From the late 18th century onward, this relationship has passed through a series of stages, the first of which was parallel development, with the mass media reaching a broader audience through new technologies and market growth while sports were attracting a growing base of paying spectators. Next, their trajectories began to intersect—the commercial mass media (especially after their emergence in electronic form) increasingly viewed sports coverage as an inexpensive way of supplying much-needed content. Sports were correctly perceived as ideal for capturing audiences for advertisers. Public or state media also recognized sporting events as opportunities to reaffirm national culture and to bolster patriotism. As the economic infrastructure of sports developed to the level of a bona fide industry, sports entrepreneurs began to see the mass media as important for generating interest among spectators and sponsors.
Finally, by the late 20th century, mass media and elite sports formed a marriage of convenience, becoming in this last stage so economically interdependent as to be virtually inseparable. It is now, for example, impossible to imagine the continued existence of professional sports—football, basketball, gridiron football, or baseball—without billion-dollar broadcast rights and saturation coverage in the sports pages. It is also difficult to suggest another cultural form capable of attracting billions of viewers to watch live events (such as the Olympic Games opening ceremony or football’s World Cup final). Media magnates such as Ted Turner, Rupert Murdoch, and Silvio Berlusconi, along with the Walt Disney Company, have developed this logic of convergence to the highest level, becoming the owners of sports teams—the Atlanta Braves and Los Angeles Dodgers baseball teams, football’s AC Milan, and the National Hockey League’s Mighty Ducks, respectively. This coming together of media and sports, however, can reinstate older practices, with the costs to media corporations of acquiring broadcast rights and sports clubs offset by reintroducing the charge for watching that home viewers previously evaded. The introduction of cable, satellite, and microwave delivery systems has enabled broadcasters to exact payment for access to 24-hour sports channels or, in an even more direct revival of turnstile arrangements, for access to pay-per-view live broadcasts of especially popular sports events such as championship boxing matches. Sports bars and other entertainment venues with multiple television screens also offer a more public way of watching sports, just as large screens are now a feature at most major sports stadiums. For those who prefer to stay at home, however, the spreading availability of the Internet has created many new ways of connecting sports fans, media companies, sponsors, and advertisers. For example, all the major American media companies now have a substantial online presence. Cyberspace is the latest site for the intimate relationship between the mass media and professional sports to be consummated.
Tracing the rise of the mass media and professional sports demonstrates constant change and innovation in the presentation of sports in the media. The pace of this change has accelerated with the intensification of competition between media organizations, between different sports, and between sports and other forms of leisure entertainment. The print sports media have evolved far beyond their original 18th-century role of announcing imminent sports events and recording their outcomes. Beginning in the early 19th century with the boxing reports of England’s Pierce Egan, newspapers transformed their sports coverage from factual statements of results to expansive, dramatic, and linguistically innovative accounts of sporting events. By the end of the century, the popularity of these sports stories among (mostly male) readers had prompted the growth of sports desks staffed by specialized journalists. They produced sports pages, often conveniently located at the back of the newspaper, that provided readers with abundant, although largely sanitized, information about athletes and their performances. Sportswriters tended to concentrate on the anticipation, atmospheric description, and postmortem dissection of major sporting occasions. Newspaper proprietors quickly discovered that the back page was often consulted before the weightier matters of state at the front of the newspaper. The importance of sports for newspaper circulation can be illustrated by the placement, as a lure for its readers, of a detailed horse-racing form in The Morning Star, the long-running (but now defunct) British Communist Party newspaper.
The space devoted to sports coverage in the daily press increased to the point where, by the middle of the 20th century, even the august New York Times was producing bulky sports sections. By that time the public’s appetite for sports news was so great that daily newspapers exclusively dedicated to sports had sprung up in many countries. The most famous of them, L’Équipe (Paris), traces its origins to the beginning of the 20th century.
A host of sportswriting styles and genres are available to readers. Some of these are of long standing—for example, the “morning after” sports report detailing the outcomes and the main features of a sports contest. Others are of more recent invention, such as “soft news” and celebrity sports gossip. Journalists have become increasingly enthusiastic about probing sports scandals. Sports fans have been enlightened about official corruption (such as that surrounding the successful bid by Salt Lake City, Utah, to host the 2002 Winter Olympics), performance-enhancing drugs, and off-field violence committed by athletes and fans. There is also considerable space in the print media devoted to in-depth profiles of athletes and the examination of sports issues, some of which are collected in books such as the Best American Sports Writing series. In book publishing there are fictions (e.g., Henry de Montherlant’s Les Olympiques [1938], Alan Sillitoe’s The Loneliness of the Long-Distance Runner [1959]); biographies and autobiographies (usually ghostwritten) of prominent athletes (e.g., Muhammad Ali, Pelé, Martina Navratilova, Michael Jordan); reflections on the experience of sports fandom (e.g., Nick Hornby’s Fever Pitch [1992]); various coaching manuals and guides; and an increasing body of academic literature on sports. These and other forms of writing contribute to (and are a result of) the prominence of sports in the contemporary economy and society.
However evocative sportswriting might be, it lacks the immediate impact of a striking visual sports image. As newspapers have developed their design appeal, sports photography has enhanced the attractiveness of the sports pages and of general current-affairs magazines such as Time, Newsweek, Paris-Match, and Der Spiegel. In the thousands of specialized magazines devoted entirely to sports, verbal texts and visual images are appealingly combined with an eye to the adult male sports fans who are the magazines’ principal readers. One consequence of this focus on male readers is that magazines such as Sports Illustrated and The Sporting News provide minimal coverage of women’s sports (and tend to emphasize the erotic appeal of female athletes when they do allot space to women’s sports).
Despite the convenience of sports journalism in the print media, the reader’s experience is—by definition—mediated. It still lacks a vibrant sense of immediacy. The diffusion of radio technology throughout Europe and North America in the 1920s allowed fans, absent from the game for whatever reason (distance, scheduling, venue capacity, cost), to listen in to play-by-play descriptions of events. A new market developed around those who tuned in to sports and hearkened to the sponsors’ and advertisers’ messages. Once radio broadcasting had been established, the next technological innovation—television—added the crucial visual to the existing audio dimension of live sports spectatorship.
Television provides an unprecedented opportunity for vicarious experience. Initially, in the 1950s, those who staged, organized, and performed at sports events feared that the availability of games on television might keep fans from attending, especially if they could receive these live television sports broadcasts “free-to-air”—that is, for only the cost of the reception equipment and electrical power. The doubts quickly disappeared when it was discovered that television also had the capacity to generate legions of new sports fans. The enthusiastic response to sports programming provided sports organizations with a powerful new revenue stream: the sale of broadcast rights. By the late 20th century, as the cultural economy became increasingly important and the need to attract consumers to converging broadcast, computer, and telecommunications technologies became ever more urgent, entrepreneurs sold audiovisual access to their performances at vastly inflated prices. It has been estimated that the global value of broadcast income to the IOC for the Summer and Winter Games of 1996 through 2008 will exceed $10 billion.
For televised sports, technical and presentational complexity has increased alongside the cost, scope, and density of coverage. From a single, static camera attempting to capture sports events as if from the perspective of a well-positioned spectator at the venue, the number and capabilities of cameras and microphones have vastly increased. At contemporary major sports events, multiple cameras are positioned to capture the action from a variety of angles (including overhead), distances (from extreme close-ups to panoramas), and speeds (from super slow motion to time-lapse speed). Highly sensitive directional microphones and “lipstick” cameras and microphones placed on sports participants or their equipment take the spectator ever closer to the play. Electronic sports will move far beyond today’s relatively passive viewing when home stadium and virtual-reality technology are introduced. The first will allow viewers to make their own production choices of camera angle and displayed sports data; the second will so immerse viewers in the sports action that they will feel like participants.
This heightened “spectacularization” of the electronic sports media is designed to maintain the interest of sports fans and to attract detached viewers seeking sensation and stimulation. In this way sports will remain central to the economics of the media. When combined with the treatment of sports in other media—from Hollywood films such as Field of Dreams (1989) and Any Given Sunday (1999) to the compact-disc recording of Plácido Domingo, Luciano Pavarotti, and José Carreras singing at the opening ceremonies of football’s 1990 World Cup—the vibrancy and inventiveness of the sports media are readily apparent. This popularity and adaptability have ensured that media companies will continue to invest a major share of their resources in one of their most valuable commercial assets—sports.
Modern sports and modern mass media are both multibillion-dollar businesses. Elite sports cannot function as they do without the mass media to publicize and underwrite them. The huge market for sports equipment and team-related merchandise is to a large extent sustained by the media’s 24-hour-a-day sports coverage, and the economic infrastructure of the mass media depends to a considerable extent on the capacity of sports to create large, loyal cohorts of readers, listeners, viewers, and interactive consumers. This dynamic synergy between sports and the mass media is not without its problems. The mass media have enormous influence not only on the way that sports events are staged but also on when they take place. When Olympic sprinters run their races at 5 AM so that New Yorkers can watch them in prime time, as happened at the 1988 Summer Games in Seoul, South Korea, the media have clearly exercised a degree of influence that was unthinkable in the days of Olympic founder Pierre de Coubertin. That the media’s economic interests are uppermost is evidenced by the advertisements that continually interrupt the action of sports events covered by commercial television networks. Not surprisingly, there is an occasional backlash against the symbiosis of sports and the media. Some athletes and spectators resentfully accuse the media (especially television) of “taking over sports” and altering their ethos, rules, and structure. Evidence of concern about the economic power of the mass media was provided in 1999 when the British government decided to prevent Rupert Murdoch’s BSkyB, which owns the broadcast rights to English Premier League association football, from acquiring control of Manchester United, one of the world’s richest and best-known sports “brands.” There is also some evidence that the commercial interests of individual media companies, especially when monopolistic, may damage the crucial requirement for uncertain outcomes in sports leagues and tournaments and create a popular perception that sports contests have been “fixed” to further the interests of media corporations. With various abuses in mind, some critics have argued that sports need to be monitored by governments, elite sports bodies, and fan organizations in order, ironically, to secure their long-term commercial value.
Corporate sponsorship is one key area where the “brand value” of sports is central to the relationship between mass media and sports. Corporate sponsorship, which has long since replaced the aristocratic patrons who once staged sports events, has enabled sports organizations and competitions to be funded while expanding brand recognition, identification, and loyalty for the sponsors. Naming-rights sponsorship for events and facilities and the prominent placement of the sponsors’ logos where spectators cannot help but see them are extremely valuable marketing tools for which sponsors are prepared to pay enormous sums. In 1997, for example, when the newly elected British Labour government attempted to introduce a comprehensive ban on tobacco advertising and sponsorship in sports, it was delayed by the fact that the revenue from advertisements and sponsorships was in excess of £300 million a year. Linked to sponsorship is merchandising, which enables sports, sponsors, and companies to derive additional income and exposure by selling to sports fans goods and services that identify the fans as supporters both of teams (such as football shirts) and of sponsors (through displaying, for example, the Nike sportswear company’s “swoosh” or the distinctive stripes of Adidas). Additional impetus to this marketing effort is bestowed by paying star athletes, such as basketball player Michael Jordan or tennis player Anna Kournikova, to actively endorse branded sports products or merely to display or use them.
The key to the commercialization of sports through sponsorship, celebrity endorsement, and merchandising is, of course, the mass media, whose astonishing capacity to showcase sports events and individual athletes has propelled sports contests from local to global phenomena. The story of the development and evolution of modern sports is therefore one in which the mass media are among the essential agents of change across the whole field of sports culture. Economically, sports are intimately and enduringly married to the mass media—with no prospect of a divorce.
Violence can be defined as any interpersonal behaviour intended to cause physical harm or mental distress. Most discussions of sports-related violence concentrate on physical harm—i.e., bodily injury. Setting aside the question of motivation, most psychologists approach the study of sports-related physical violence from a behaviouristic perspective. They infer the intention of assailants from their observable actions. In a sports context, aggression, which is often discussed as if it were synonymous with violence, can best be defined as an unprovoked physical or verbal assault. Aggressiveness, therefore, is the propensity to commit such an assault.
In attempting to map patterns of violence, sociologists such as Michael Smith have developed a sports-violence typology in which “brutal body contact” is seen as integral to some sports. This contact conforms to the rules of the sport and is completely legitimate even when the same sort of behaviour outside the sports context is defined as criminal. Examples of legitimate violence can be found in rugby and gridiron football and in boxing, wrestling, and Asian martial arts. Participants in these sports, by the very act of taking part, have implicitly accepted the inevitability of rough contact. They have implicitly consented to the probability of minor injury and the possibility of serious injury. They cannot, however, reasonably be said to have agreed to injuries sustained from physical assaults that violate the written and unwritten rules of the sport. Although violence of this latter sort is definitely illegitimate and sometimes illegal, it has proved very difficult for injured athletes to find redress in the courts. Judges and juries are reluctant to convict athletes of criminal behaviour committed in the course of a sports contest, and they are equally reluctant to convict coaches, schools, and sports leagues of negligence.
“Borderline violence” consists of behaviours that violate the official rules of the sport but that are accepted by players and fans alike as a legitimate part of the game. Such behaviour—a fistfight in ice hockey or an intentional foul in association football’s penalty zone—is rarely subject to legal proceedings and tends to be dealt with by penalties imposed by referees, umpires, or league administrators. A memorable example of this occurred in 1997 when the Nevada Boxing Commission censured and banned heavyweight boxer Mike Tyson for biting his opponent. More-extreme rule infractions—i.e., those that violate not only the formal rules of the sport but also the law of the land—elicit a harsher formal response, especially when the violence results in serious injury. High or late tackles in gridiron football usually create serious outrage and have on occasion led to the strict imposition of a lifetime ban, but recourse to the law in cases of quasi-criminal violence is infrequent. Finally, Smith’s typology includes what he termed “criminal violence”—that is, behaviour so egregious that it is handled legally from the outset because it is not considered part of the game.
While legal scholars have sought to distinguish legitimate from illegitimate sports violence, social psychologists and sociologists have investigated the causes of sports-related violence. Here the discussion revolves around broader nature-nurture debates and the role that sports are believed to play in society. Those who believe that aggression and violence are “natural” tend to view them as instinctive and inevitable aspects of human behaviour. From the perspective of Konrad Lorenz and others in this camp, sports are seen as a form of catharsis; they allow for the safe and channeled release of the aggression that is part of every person’s instinctive makeup. Most sports sociologists, however, challenge this hypothesis and believe instead that research confirms that violence and aggression are socially learned. This latter view is supported by the fact that the levels and types of sports-related violence vary greatly from culture to culture, which strongly suggests that they are not the result of some universal human nature. Canadian ice hockey, for example, is more violent in some respects than its Scandinavian counterpart. The reason for this is that Canadian ice hockey provides a subcultural context in which boys and young men are introduced to highly aggressive behaviour. In this and in many other sports subcultures, brutal body contact and physical assault are part and parcel of what it means to be a man. Conformity to the code of toughness certifies a player’s masculinity and confers upon him honour and prestige. Those who fail to meet such expectations drop out of the subculture or are subject to peer sanctions.
Sports-related spectator violence is often more strongly associated with a social group than with the specific nature of the sport itself. Roman gladiatorial combats were, for example, history’s most violent sport, but the closely supervised spectators, carefully segregated by social class and gender, rarely rioted. In modern times, football (soccer) is certainly less violent than rugby, but soccer hooliganism is a worldwide phenomenon, while spectator violence associated with the more upper-class but rougher sport of rugby has been minimal. Similarly, crowds at baseball games have been more unruly than the generally more affluent and better-educated fans of gridiron football, although the latter is unquestionably the rougher sport. Efforts by the police to curb sports-related violence are often counterproductive, because the young working-class males responsible for most of the trouble are frequently hostile to the authorities. Media coverage of disturbances can also act to exaggerate their importance and incite the crowd behaviour that the media then simultaneously condemn and sensationalize. The most effective means to reduce the level of spectator violence is also the simplest: abolish "terraces," where spectators stand, and provide seats for all ticket holders.
With few exceptions, modern sports were devised by and for men, with the content, meaning, and significance of the contests reflecting male values, strengths, and interests. The 19th-century institutionalization of modern sports involved changes in personality, body deportment, and social interaction; the result was a body culture that valued youthful masculinity.
A great deal of research has focused on the role sports play in the making of modern masculinity. For young men and adolescent boys, the path to manhood appears to be reinforced and confirmed by participation in sports. In some respects this can be a positive relationship. As the consideration of sports-related violence indicates, however, sports do not simply “build character,” as Victorian educators and 20th-century coaches were prone to assert; sports also create characters. Some of these characters are socially responsible role models; others can develop a tough masculine style that aggravates broader social problems such as domestic violence. Male sports heroes have at times enjoyed certain social privileges, including a tolerance of antisocial behaviour based on the rationalization that “boys will be boys.” Some sports cultures generate forms of behaviour that are openly antagonistic toward people of different sexual orientation. Gender discrimination can also take less-extreme forms. For most of the 19th and 20th centuries, for example, it was assumed that cheerleading was the most appropriate way for girls to contribute to sports.
Although in some respects modern sports remain the male preserve they were in the Victorian era, male privilege has never gone unchallenged. Many upper-middle-class women played golf, tennis, and field hockey; a few lower-class women boxed and wrestled. Women have had to campaign strenuously for access to “inappropriate” sports such as rugby and weightlifting, but they have been relatively successful in their efforts and now participate in a great range of sports, many of which were thought to be prototypically masculine. Still, even at the turn of the 21st century, at the 2000 Summer Olympics men participated in 48 more events than women did. While the number of female competitors varies considerably from one Olympic team to another, it is rare for a National Olympic Committee to send equal numbers of men and women, and some Islamic countries are represented by all-male teams. Access and opportunity remain key issues, but attention has also been paid to gender-based differences in status, prestige, and the distribution of resources and rewards. Research in these areas emphasizes that, while there are individual cases of gender bias, the more fundamental problem is the persistence of social structures that systematically privilege men.
Statistical studies documenting the greatly increased participation of women in recreational and elite sports, which are cause for optimism, must be supplemented by analyses of the way in which female athletes are positioned within the media-sports complex. Much recent evidence indicates that the mass media still tend, despite some laudable attempts to overcome gender bias, to reinforce conventional notions of masculinity. Although female athletes rarely suffer from role conflict ("an athlete or a woman?") as they once did, the mass media still contribute to the trivialization of female athletes, whose physical attractiveness is often stressed at the expense of their sporting prowess. At work is a set of enabling and constraining features that determine the recognition and financial rewards women receive for their participation in sports. Female athletes who conform to mainstream canons of sex appeal (which now call for an athletic rather than a voluptuous body) are eagerly sought after to appear on magazine covers and in product endorsements, while equally successful female competitors whose bodies are less conventionally attractive are passed over.
At the end of the 20th century, there was greater tolerance of homosexuality in many nations; however, homosexuality remained taboo in the sporting world. While a handful of elite athletes such as diver Greg Louganis and tennis star Martina Navratilova have "come out of the closet,” homosexuality among professional athletes remains largely unknown and hidden. Women’s sports in particular have struggled with issues of sexuality. Basketball and softball, for example, have been portrayed in popular culture as a haven for lesbians, which to some degree they have been. To combat this stereotype, which has damaged efforts to increase wider participation and greater spectator interest, conventional feminine ideals have been stressed in the marketing of women’s sports. The Gay Games, established in 1980, were created to provide an opportunity for male and female gay athletes to compete openly and to counteract negative perceptions about homosexuals.
Frequently overlooked in analyses of sports and gender relations is the controversial practice, common in the sporting-goods industry, of using women and children to produce equipment and clothing. Nike and a number of other manufacturers have been accused of economically exploiting women and children in developing nations (so-called sweat-shop labour) while at the same time running advertising campaigns asserting that their products empower young women.
In sports, as elsewhere in society, there is a tendency to explain differences in performance in terms of some alleged physical differences between races. When Austrians do well at skiing and Swedes excel at tennis, cultural explanations have been sought through the analysis of social structures and environmental conditions. On the other hand, when Kenyans prove exceptionally good at middle-distance running, there has been a tendency to look for a physiological explanation.
The tendency is misguided. As a result of the mapping of human DNA, the concept of "race" has become highly problematic. Scientists have discovered that the genetic diversity within populations sharing certain physical traits, such as skin colour, is as great as the diversity between different groups. If there are physical differences that account for Kenyan success and for the success of African American sprinters, physiologists have not yet discovered them and are not likely to. Ironically, while racism remains a useful concept for sociological analysis of some sports phenomena, such as the exclusion of African Americans from early 20th-century Major League Baseball, references to race are more likely to confuse than to clarify research into athletic performance.
Despite the consensus among geneticists, some sociologists continue to conduct research on the assumption that race is a meaningful concept. Most sociologists, however, prefer to use the concept of ethnicity in their attempts to account for observed differences in performance. Ethnicity refers to the shared cultural heritage of a group. This cultural heritage, which may be claimed or imposed, includes language, customs, practices, traditions, and institutions. Since ethnic cultures are normally learned in childhood, they are so familiar that they become second nature or what Pierre Bourdieu refers to as “habitus.” Ethnic differences in sports are observable in pose and style as well as in quantifiable sports performance. Sports fans are adept at reading the distinctive nonverbal body language of different groups playing the same game. In the 1950s the exuberant play of the Brazilian national football (soccer) team, which emphasized individual skill, was strikingly different from the disciplined team-oriented style of the German side.
Different ethnic groups have different rates of involvement in sports. Palestinians who are citizens of Israel are less likely than Jewish citizens to participate in sports. Turks residing in Germany are less likely than ethnic Germans to be members of sports clubs. Within both these Islamic ethnic minorities, girls and women are even less likely than boys and young men to be athletically active. Journalists have noted and sociologists have investigated the overrepresentation of African Americans in some sports (basketball, boxing, track) and their underrepresentation in others (polo, swimming, yachting). Such patterns of participation can be the result of early socialization, role modeling, peer group subcultures, economic and community structures, stereotyping, and scapegoating. Sociologists have employed these and other concepts to demonstrate why ethnic minorities tend to be less involved in sports and why, when they are involved in sports, they still tend to be excluded from or underrepresented in management, administration, and ownership. Sociological surveyors have demonstrated that sports are far from the level playing field they purport to be.
The empirical evidence demonstrates that the nature and extent of athletic involvement, the chance for success, the opportunities to hold positions of power and prestige, and the gaining of positive experiences through sports are all structured along the ethnic fault lines that exist within and between societies. These processes are part of the social structures that enable and constrain different ethnic groups. The role, meaning, and significance of sports involvement is related to but not solely determined by these processes. The concept of ethnicity not only helps make sense of the differential performance attributed to race but also aids in explaining how sports are used by groups for political ends. The roles of football (soccer) and rugby in Ireland are a case in point. While separate football teams represent Northern Ireland and the Republic of Ireland (the former a symbol of Protestant ethnic identity), international rugby games are played by a unified team that seeks to represent the whole of Ireland. These differences are tied to the complex cultural traditions of the two sports and the class profile of those involved. Similarly, games between formerly colonized nations and their former colonizers, such as cricket matches between India and England, tend to become rites of passage and are imbued with a heightened sense of symbolism. The games count as part of broader cultural struggles. Perhaps the best example of the usefulness of the concept of ethnicity rather than race as an explanation for differences in performance levels is Beyond a Boundary (1963), C.L.R. James’s classic study of the making of Caribbean cricket. James combines careful historical analysis with detailed observations of the cricket culture of his day, finding in the sport a symbolic reenactment of the struggles and inequalities that existed and still exist in the Caribbean.
Although performance-enhancing drugs were known as early as the 19th century, when professional cyclists used strychnine as a stimulant, the widespread use of drugs began in the 1960s. It is a practice that cuts across national and ideological boundaries. Sociologists investigating the phenomenon of drug use in sports normally put aside the moral outrage that characterizes media coverage of and political commentary on this issue. Media personnel tend to focus on the actions of high-profile stars such as Canadian sprinter Ben Johnson and Irish swimmer Michelle Smith, whose Olympic gold medals were stripped away (Johnson) or sadly tarnished by the suspicion of drug use (Smith). Whenever a prominent athlete tests positive for a banned substance, journalists, politicians, and sports administrations are likely to respond with calls for zero-tolerance policies. In contrast, sociologists ask: What is a drug? What are the social and sporting roots of drug usage? Why is the focus almost exclusively on drugs that enhance performance? What would constitute a viable policy for drug usage?
Three broad categories of drugs have been identified: recreational, restorative, and additive, or performance-enhancing, drugs. While attention is focused on recreational drugs such as marijuana and cocaine or on anabolic steroids (synthetic compounds of the male hormone testosterone) and other performance-enhancing drugs, little or no attention is given to drugs that restore athletes to fitness. This is unfortunate because the overuse of vitamins and food supplements can also be detrimental to an athlete’s health. Greater consideration should be given to all categories of drug consumption, not just to the abuse of cocaine and anabolic steroids.
One hindrance to the formulation of a rational policy about drugs is the often tenuous distinction between the natural and the artificial. This is especially true for vitamins, special diets, human growth hormones, and blood doping (the extraction and later infusion of an athlete’s own blood). In addition, there is no hard-and-fast distinction between different categories of drugs; some drugs, such as beta-blockers, fall into both the restorative and performance-enhancing categories.
In examining the case for and against the implementation of bans on athletes who test positive for drug use, several key arguments can be identified. The most widely used argument for a ban is that performance-enhancing drugs confer an unfair advantage on those who use them. This argument brings the ethics of sports into play, along with the notion that athletes have a moral duty not only to adhere to the rules but also to serve as role models. Also widely used is the argument that drugs harm the athletes’ health. The “harm principle” asserts or implies that athletes must be protected from themselves. Closely associated with both arguments is the notion that bans act as a deterrent, preventing athletes from cheating and from inflicting harm on themselves.
The counterargument is twofold. The argument based on fairness is said to be unpersuasive because drugs would confer no special advantage if they were legalized and made available to all athletes. Proponents of this viewpoint also note that the rules now in force allow athletes from wealthy nations to train more efficiently, with better coaching and equipment, than athletes from poorer countries, a situation that is manifestly unfair. The argument based on the “harm principle” is said to treat athletes as children. Adult athletes should be allowed to decide for themselves whether they want to harm their health by drug use.
Sociologists have contributed to the debate on drugs by pointing out that focusing on the actions of the athlete individualizes the issue of drug usage rather than examining the social roots of drug consumption. Among the causes of drug usage that have been identified are the medicalization of social life and the vastly increased importance of sports as a source of self-esteem and material benefits. Victory has always brought greater rewards than defeat, but the differences are now on an unprecedented scale. Sociologists have also raised questions about privacy rights being violated by mandatory drug testing and about the meagre resources being provided for the rehabilitation of drug offenders.
Discussions of performance-enhancing drugs are also complicated by the fact that most spectators say they disapprove of drugs even as they turn out to support athletes who have tested positive for banned substances. After the French police uncovered massive doping during the 1998 Tour de France, roadside crowds increased.
The debate over drugs is further complicated when "unnatural" factors influencing performance are considered—for example, the use of psychological techniques and biotechnological intervention. The role of sports psychology began to increase significantly in the 1990s. Goal setting, focus, and visualization exercises were designed to ensure that athletes would concentrate on reaching their peak performance. Distractions were to be eliminated.
The growth of biotechnological intervention in human affairs, including the potential impact of genetic engineering, also raises many issues for sports. While many people uncritically accept this type of intervention in the context of restorative medicine, the boundary line between rehabilitation and enhancement, as in the case of drugs, is not clear. Reconstructive surgery, implants, and technological adjustments contribute, along with drug use and masochistically intense training regimes, to the creation of what John M. Hoberman calls “mortal engines.” These interventions into the “natural” body have to be considered within the broader debate concerning sports and what it is to be human.
Although a book titled Psychologie des sports (“Psychology of Sports”) was published in 1927 by the German psychologist Alfred Peters, the field developed slowly. The International Society of Sport Psychology was not established until 1965. At that time, research tended to focus on personality, motivation, and aggression.
For decades, psychologists attempted to identify personality traits that distinguished athletes in one sport from those in another (and from nonathletes). Using American psychologist Raymond Cattell’s Personality Factor Questionnaire and a battery of other paper-and-pencil inventories, researchers came to contradictory results. Beyond the fact that athletes are more physically active than nonathletes and the equally obvious fact that athletes drawn to individual sports score higher on "autonomy" and "independence" than athletes devoted to team sports, there was little consensus on "the athletic personality." If one controls for social class, athletes tend to be very much like nonathletes and to be like one another.
Studies of the "athletic personality" have become rare, but studies of motivation and of aggression have increased in number and have become increasingly multifactored and sophisticated. Early studies of motivation, often inspired by the work of American psychologists David McClelland and John Atkinson, examined the relationship between the need for achievement and the fear of failure. Female athletes proved to be a special problem. For a number of years, their lower levels of motivation were explained as a fear that athletic success came at the cost of diminished femininity. This fear was, in turn, explained as the result of role conflict. A woman’s fervent interest in sports might be perceived as an expression of a masculine nature or of lesbianism; psychological tests such as American psychologist Sandra Bem’s Sex Role Inventory routinely classified female athletes as "masculine" because they scored high on scales for competition and aggressiveness. By the end of the century, however, in Europe and North America greater social acceptance of intensely competitive female athletes (and of lesbianism) more or less eliminated role conflict and the "fear of success." At the recreational level as well as at the elite level, recent studies have shown conclusively that sports participation generally leads to increased, rather than diminished, self-esteem for girls and women as well as for boys and men.
In Problem Athletes and How to Handle Them (1966), Americans Bruce Ogilvie and Thomas Tutko attempted to apply motivational principles to improve sports performance. Their widely used Athletic Motivation Inventory was designed to measure personality traits, such as leadership and mental toughness, conducive to athletic achievement. Other psychologists have explored a variety of techniques, including meditation, mental imaging, and even hypnosis, to lessen anxiety or control arousal or improve concentration. Still other psychologists have sought to enhance performance by studying the dynamics of small-group interaction and the relative efficacy of different coaching and leadership styles. Gender accounts for some of the observed differences. Although female athletes are increasingly similar psychologically to male athletes, they continue to respond more readily than men do to encouragement and to react more negatively than men do to admonition. Cultural differences, which sports psychologists sometimes neglect, are also important. Japanese athletes respond better than their North American counterparts to harsh criticism and punitive discipline. Cultural differences also play an important role when the stage is set for pharmacological intervention. The more authoritarian the culture is, the more likely it has been that coaches will demand that elite athletes use performance-enhancing drugs, such as anabolic steroids, and abjure recreational drugs, such as cocaine.
The motivation for recreational sports is unquestionably different from the motivation at the elite level. Recreational and elite athletes share a common desire to improve their skills and to win, rather than lose, a contest. Both are likely to value the social pleasures of team membership and to experience the moments of ecstatic fulfillment that some psychologists refer to as "flow." There are, however, important differences in the kind and in the intensity of their motivation. Material rewards figure, of course, among the motives of openly professional athletes, but, even when economic motives are not in play, elite athletes are a breed apart. They are likely to feel themselves to be representatives of their nation (or of some other collectivity). Standing on the victor’s podium and watching one’s national flag rise to the strains of one’s national anthem can motivate as strongly as the prospect of signing a million-dollar contract (and the first frequently leads to the second). When inspired by a combination of economic and representational motives, elite athletes can reach almost unimaginable levels of athletic performance, but they are also liable to develop a win-at-all-costs attitude that motivates them to use performance-enhancing drugs, to commit intentional fouls, and to risk lifelong physical disability by "playing hurt" (continuing to compete despite a serious injury).
This disregard for one’s health is perhaps the most important motivational difference between the elite and the recreational athlete. For the latter, a principal motive for sports participation (and for visits to an aerobics class or a fitness centre) is a desire to improve one’s health and to shape one’s body into closer conformity to contemporary ideals of physical attractiveness. For the former, the physical self is frequently jeopardized and sometimes sacrificed on the altar of sports success.
Sports spectators have also been the focus of a great deal of psychological research. Despite the 19th-century code of impartial good sportsmanship, spectators do strongly identify with athletes whom they see as representatives of their race, religion, national state, ethnic group, city, or school. American psychologist Daniel L. Wann has shown, among other things, that knowledge about the sport correlates strongly with the intensity of this identification. The fans’ behaviour varies in response to winning and losing. When their team wins, fans refer to "our victory" and wear the sweatshirts that identify them as loyal supporters; when their team fares badly, fans tend to doff the sweatshirts and to complain about "the team’s loss." (Similarly, studies have demonstrated that winning athletes tend to attribute their success to their own superior skills, while losing athletes tend to attribute their failure to bad luck or to their opponents’ unfairness.)
Sometimes fans do more than complain. In the 1960s and 1970s, there was a dramatic increase in violence committed by sports spectators. Most of the research on this phenomenon has been done by Eric Dunning of Great Britain and other sports sociologists, but a number of social psychologists have also studied sports-related aggression. Behind their research lay a question: Is aggressiveness innate, as Sigmund Freud insists, or is it learned, as American psychologist Albert Bandura (among others) argues? If the former, sports spectators may experience a "safety-valve" catharsis, thanks to which the propensity to commit acts of aggression is diminished; if the latter, sports spectatorship may actually increase aggressiveness. Experiments conducted with an apparatus originally designed by American Arnold Buss measured the level of electric shock subjects were ready to administer to another person. Subjects who had watched a sports event on film were willing to administer higher levels of shock than subjects who had seen a travelogue or some other nonviolent film. These experiments, in conjunction with paper-and-pencil tests and the obvious fact that sports-related riots commonly occur after (rather than before) a contest, proved conclusively that sports spectators do not experience a "safety-valve" catharsis. After leaving the venue or turning off their television sets, they are more, rather than less, prone to violence than they were before the contest began. Sports psychology leads to the odd conclusion that sports may be good for athletes and bad for spectators.
For a full treatment of this subject, see sports betting.
Wagering on sports is one of the most popular forms of gambling, not least because of the near ubiquity of sports around the world. Although horse racing has traditionally been one of the sports most commonly associated with gambling, many professional sports around the world are today the focus of betting, both legal and illegal.
Predicting the winner of a sporting event is the most straightforward, and oldest, type of sports betting. Odds betting drives much of this form of gambling: a sportsbook, for example, assesses the probability of victory through odds, such as 2 to 1 or 25 to 1 or 2 to 5. Someone who puts $50 on a team with odds of 2 to 1, for instance, will make $100 if that team wins (and will receive the initial $50 bet back for a total payout of $150). A pari-mutuel wagering system, which is based on the total amount wagered on each competitor, is used for many racing sports. A point spread adds a layer of complexity to a winner-take-all bet. Wagering on the total number of points (or runs or goals) scored in a game is the basis for over/under bets, and multiple bets can be combined into a parlay bet. The variety and complexity of the types of sports betting are extensive.
Gambling on sports outcomes has often been controversial across the history of professional sports, and it has given rise to multiple scandals. Many have revolved around athletes who have been bribed to lose purposely or, in some sports, to keep a game within the point spread. The Black Sox Scandal of 1919 remains one of the most notorious examples in the United States of players (on the Chicago White Sox) receiving money to throw games.
Around the world, sports governing bodies as well as many levels of government have taken many different approaches to controlling sports betting and the threats to the integrity of sporting events that it can represent. These efforts have ranged from simply making wagering illegal to allowing it under strict regulation. In places where sports betting is legal, the money generated is typically taxed, benefiting local governments, and it may also be used to support amateur sports. The United States is an example of a country where laws governing sports betting have changed significantly over time, ranging from making the practice illegal in most places during the 20th century to allowing it nationwide after 2018.
steel, alloy of iron and carbon in which the carbon content ranges up to 2 percent (with a higher carbon content, the material is defined as cast iron). By far the most widely used material for building the world’s infrastructure and industries, it is used to fabricate everything from sewing needles to oil tankers. In addition, the tools required to build and manufacture such articles are also made of steel. As an indication of the relative importance of this material, in 2013 the world’s raw steel production was about 1.6 billion tons, while production of the next most important engineering metal, aluminum, was about 47 million tons. (For a list of steel production by country, see below World steel production.) The main reasons for the popularity of steel are the relatively low cost of making, forming, and processing it, the abundance of its two raw materials (iron ore and scrap), and its unparalleled range of mechanical properties.
The major component of steel is iron, a metal that in its pure state is not much harder than copper. Omitting very extreme cases, iron in its solid state is, like all other metals, polycrystalline—that is, it consists of many crystals that join one another on their boundaries. A crystal is a well-ordered arrangement of atoms that can best be pictured as spheres touching one another. They are ordered in planes, called lattices, which penetrate one another in specific ways. For iron, the lattice arrangement can best be visualized by a unit cube with eight iron atoms at its corners. Important for the uniqueness of steel is the allotropy of iron—that is, its existence in two crystalline forms. In the body-centred cubic (bcc) arrangement, there is an additional iron atom in the centre of each cube. In the face-centred cubic (fcc) arrangement, there is one additional iron atom at the centre of each of the six faces of the unit cube. It is significant that the sides of the face-centred cube, or the distances between neighbouring lattices in the fcc arrangement, are about 25 percent larger than in the bcc arrangement; this means that there is more space in the fcc than in the bcc structure to keep foreign (i.e., alloying) atoms in solid solution.
Iron has its bcc allotropy below 912° C (1,674° F) and from 1,394° C (2,541° F) up to its melting point of 1,538° C (2,800° F). Referred to as ferrite, iron in its bcc formation is also called alpha iron in the lower temperature range and delta iron in the higher temperature zone. Between 912° and 1,394° C iron is in its fcc order, which is called austenite or gamma iron. The allotropic behaviour of iron is retained with few exceptions in steel, even when the alloy contains considerable amounts of other elements.
There is also the term beta iron, which refers not to mechanical properties but rather to the strong magnetic characteristics of iron. Below 770° C (1,420° F), iron is ferromagnetic; the temperature above which it loses this property is often called the Curie point.
In its pure form, iron is soft and generally not useful as an engineering material; the principal method of strengthening it and converting it into steel is by adding small amounts of carbon. In solid steel, carbon is generally found in two forms. Either it is in solid solution in austenite and ferrite or it is found as a carbide. The carbide form can be iron carbide (Fe3C, known as cementite), or it can be a carbide of an alloying element such as titanium. (On the other hand, in gray iron, carbon appears as flakes or clusters of graphite, owing to the presence of silicon, which suppresses carbide formation.)
The effects of carbon are best illustrated by an iron-carbon equilibrium diagram. The A-B-C line represents the liquidus points (i.e., the temperatures at which molten iron begins to solidify), and the H-J-E-C line represents the solidus points (at which solidification is completed). The A-B-C line indicates that solidification temperatures decrease as the carbon content of an iron melt is increased. (This explains why gray iron, which contains more than 2 percent carbon, is processed at much lower temperatures than steel.) Molten steel containing, for example, a carbon content of 0.77 percent (shown by the vertical dashed line in the figure) begins to solidify at about 1,475° C (2,660° F) and is completely solid at about 1,400° C (2,550° F). From this point down, the iron crystals are all in an austenitic—i.e., fcc—arrangement and contain all of the carbon in solid solution. Cooling further, a dramatic change takes place at about 727° C (1,341° F) when the austenite crystals transform into a fine lamellar structure consisting of alternating platelets of ferrite and iron carbide. This microstructure is called pearlite, and the change is called the eutectoidic transformation. Pearlite has a diamond pyramid hardness (DPH) of approximately 200 kilograms-force per square millimetre (285,000 pounds per square inch), compared with a DPH of 70 kilograms-force per square millimetre for pure iron. Cooling steel with a lower carbon content (e.g., 0.25 percent) results in a microstructure containing about 50 percent pearlite and 50 percent ferrite; this is softer than pearlite, with a DPH of about 130. Steel with more than 0.77 percent carbon—for instance, 1.05 percent—contains in its microstructure pearlite and cementite; it is harder than pearlite and may have a DPH of 250.
Adjusting the carbon content is the simplest way to change the mechanical properties of steel. Additional changes are made possible by heat-treating—for instance, by accelerating the rate of cooling through the austenite-to-ferrite transformation point, shown by the P-S-K line in the figure. (This transformation is also called the Ar1 transformation, r standing for refroidissement, or “cooling.”) Increasing the cooling rate of pearlitic steel (0.77 percent carbon) to about 200° C per minute generates a DPH of about 300, and cooling at 400° C per minute raises the DPH to about 400. The reason for this increasing hardness is the formation of a finer pearlite and ferrite microstructure than can be obtained during slow cooling in ambient air. In principle, when steel cools quickly, there is less time for carbon atoms to move through the lattices and form larger carbides. Cooling even faster—for instance, by quenching the steel at about 1,000° C per minute—results in a complete depression of carbide formation and forces the undercooled ferrite to hold a large amount of carbon atoms in solution for which it actually has no room. This generates a new microstructure, martensite. The DPH of martensite is about 1,000; it is the hardest and most brittle form of steel. Tempering martensitic steel—i.e., raising its temperature to a point such as 400° C and holding it for a time—decreases the hardness and brittleness and produces a strong and tough steel. Quench-and-temper heat treatments are applied at many different cooling rates, holding times, and temperatures; they constitute a very important means of controlling steel’s properties. (See also below Treating of steel: Heat-treating.)
A third way to change the properties of steel is by adding alloying elements other than carbon that produce characteristics not achievable in plain carbon steel. Each of the approximately 20 elements used for alloying steel has a distinct influence on microstructure and on the temperature, holding time, and cooling rates at which microstructures change. They alter the transformation points between ferrite and austenite, modify solution and diffusion rates, and compete with other elements in forming intermetallic compounds such as carbides and nitrides. There is a huge amount of empirical information on how alloying affects heat-treatment conditions, microstructures, and properties. In addition, there is a good theoretical understanding of principles, which, with the help of computers, enables engineers to predict the microstructures and properties of steel when alloying, hot-rolling, heat-treating, and cold-forming in any way.
A good example of the effects of alloying is the making of a high-strength steel with good weldability. This cannot be done by using only carbon as a strengthener, because carbon creates brittle zones around the weld, but it can be done by keeping carbon low and adding small amounts of other strengthening elements, such as nickel or manganese. In principle, the strengthening of metals is accomplished by increasing the resistance of lattice structures to the motion of dislocations. Dislocations are failures in the lattices of crystals that make it possible for metals to be formed. When elements such as nickel are kept in solid solution in ferrite, their atoms become embedded in the iron lattices and block the movements of dislocations. This phenomenon is called solution hardening. An even greater increase in strength is achieved by precipitation hardening, in which certain elements (e.g., titanium, niobium, and vanadium) do not stay in solid solution in ferrite during the cooling of steel but instead form finely dispersed, extremely small carbide or nitride crystals, which also effectively restrict the flow of dislocations. In addition, most of these strong carbide or nitride formers generate a small grain size, because their precipitates have a nucleation effect and slow down crystal growth during recrystallization of the cooling metal. Producing a small grain size is another method of strengthening steel, since grain boundaries also restrain the flow of dislocations.
Alloying elements have a strong influence on heat-treating, because they tend to slow the diffusion of atoms through the iron lattices and thereby delay the allotropic transformations. This means, for example, that the extremely hard martensite, which is normally produced by fast quenching, can be produced at lower cooling rates. This results in less internal stress and, most important, a deeper hardened zone in the workpiece. Improved hardenability is achieved by adding such elements as manganese, molybdenum, chromium, nickel, and boron. These alloying agents also permit tempering at higher temperatures, which generates better ductility at the same hardness and strength.
The testing of steel’s properties often begins with checking hardness. This is measured by pressing a diamond pyramid or a hard steel ball into the steel at a specific load. The Vickers Diamond Pyramid Hardness tester, which measures the DPH mentioned above, uses an indenter with an included angle of 136° between opposite faces of a pyramid and usually a load of 10, 30, or 50 kilograms-force. The diagonal of the impression is measured optically, and the hardness expressed as the load in kilograms-force divided by the impressed area of the pyramid in square millimetres. Tensile and yield strength are determined by pulling a standardized machined sample in a special hydraulic press and recording the pulling force at increasing elongations until the sample breaks. The elongation at this point, and the way the fracture looks, are good indications of the steel’s ductility. Measuring the pulling force at 0.20 percent elongation and dividing it by the test bar’s cross section are a means of calculating the yield strength, a good indicator of cold formability. Impact toughness is determined by hitting a standardized, prismatic, notched sample with a test swing hammer and recording the work required to break it. This is performed at different temperatures, because brittleness increases as temperature falls.
There are many other tests used in the industry to check a steel’s mechanical properties, such as wear tests for rails, drawability tests for sheets, and bending tests for wire. Metallographic laboratories examine the microstructure of polished and etched steel samples, often on computerized and very powerful (up to 80,000× magnification) microscopes. Laboratories also check physical data such as thermal elongation and electromagnetic properties. Chemical composition is often checked by completely automated spectrometers. There are also several nondestructive tests as, for example, the ultrasonic test and magnaflux test used to check for internal and external flaws such as laminations or cracks.
There are several thousand steel grades either published, registered, or standardized worldwide, all of which have different chemical compositions, and special numbering systems have been developed in several countries to classify the huge number of alloys. In addition, all the different possible heat treatments, microstructures, cold-forming conditions, shapes, and surface finishes mean that there is an enormous number of options available to the steel user. Fortunately, steels can be classified reasonably well into a few major groups according to their chemical compositions, applications, shapes, and surface conditions.
On the basis of chemical composition, steels can be grouped into three major classes: carbon steels, low-alloy steels, and high-alloy steels. All steels contain a small amount of incidental elements left over from steelmaking. These include manganese, silicon, or aluminum from the deoxidation process conducted in the ladle, as well as phosphorus and sulfur picked up from ore and fuel in the blast furnace. Copper and other metals, called residuals, are introduced by scrap used in the steelmaking furnace. Because all these elements together normally constitute less than 1 percent of the steel, they are not considered alloys.
Carbon steels are by far the most produced and used, accounting for about 90 percent of the world’s steel production. They are usually grouped into high-carbon steels, with carbon above 0.5 percent; medium-carbon steels, with 0.2 to 0.49 percent carbon; low-carbon steels, with 0.05 to 0.19 percent carbon; extra-low-carbon steels, with 0.015 to 0.05 percent carbon; and ultralow-carbon steels, with less than 0.015 percent carbon. Carbon steels are also defined as having less than 1.65 percent manganese, 0.6 percent silicon, and 0.6 percent copper, with the total of these other elements not exceeding 2 percent.
Low-alloy steels have up to 8 percent alloying elements; any higher concentration is considered to constitute a high-alloy steel. There are about 20 alloying elements besides carbon. These are manganese, silicon, aluminum, nickel, chromium, cobalt, molybdenum, vanadium, tungsten, titanium, niobium, zirconium, nitrogen, sulfur, copper, boron, lead, tellurium, and selenium. Several of these are often added simultaneously to achieve specific properties.
The many applications of steel demonstrate best the great versatility of this material. Most often, steel consumers’ needs are met by carbon steels. Good examples are sheets for deep-drawn automobile bodies and appliances made of low-carbon steels, medium-carbon structural steels and plates employed in all kinds of construction, high-carbon railroad rails, and wires at all carbon levels used for hundreds of items. The addition of costly alloys begins when combinations of properties are requested that cannot be met by carbon steels.
The demand for high strength, good weldability, and higher resistance to atmospheric corrosion is met by a group called the high-strength low-alloy (HSLA) steels. These grades have low carbon levels (e.g., 0.05 percent) and contain small amounts of one or a combination of elements such as chromium, nickel, molybdenum, vanadium, titanium, and niobium. HSLA steels are used for oil or gas pipelines, ships, offshore structures, and storage tanks.
This group, developed for good machinability and fabricated into bolts, screws, and nuts, contains up to 0.35 percent sulfur and 0.35 percent lead; also, it sometimes has small additions of tellurium or selenium. These elements form many inclusions, which are normally avoided but are desired in this application because they break the long, hazardous strings of metal that are usually formed during machining into small chips. This keeps tools and workpieces clean, improves tool life, and permits machining at higher speeds.
Another group is the wear-resistant steels, made into wear plates for rock-processing machinery, crushers, and power shovels. These are austenitic steels that contain about 1.2 percent carbon and 12 percent manganese. The latter element is a strong austenizer; that is, it keeps steel austenitic at room temperature. Manganese steels are often called Hadfield steels, after their inventor, Robert Hadfield.
Wear resistance is brought about by the high work-hardening capabilities of these steels; this in turn is generated during the pounding (i.e., deforming) of the surface, when a large number of disturbances are created in the lattices of their crystals that effectively block the flow of dislocations. In other words, the more pounding the steel takes, the stronger it becomes. Such significant increases in strength by cold forming are also utilized in the production of high-strength, cold-drawn wire such as those used in prestressed concrete or automobile tires. A special case, piano wire drawn from 0.8-percent-carbon steel, can reach a tensile strength of 275 kilograms-force per square millimetre.
One important group that well demonstrates the enormous impact of material developments on engineering possibilities is the steels used for roller and ball bearings. These steels often contain 1 percent carbon, 1.2 percent chromium, 0.25 percent nickel, and 0.25 percent molybdenum and are very hard after heat treatment. Most important, however, they are extremely clean, having been purged of practically all inclusions by vacuum treatment of the liquid steel. Inclusions are very harmful in bearings because they create stress concentrations that result in low fatigue strength.
This outstanding group receives its stainless characteristics from an invisible, self-healing chromium oxide film that forms when chromium is added at concentrations greater than 10.5 percent. There are three major groups, the austenitic, the ferritic, and the martensitic.
The best corrosion resistance is obtained in austenitic stainless steels. Their microstructures consist of very clean fcc crystals in which all alloying elements are held in solid solution. These steels contain 16 to 26 percent chromium and up to 35 percent nickel, which, like manganese, is a strong austenizer. (Indeed, manganese is sometimes used instead of nickel.) Austenitic steels cannot be hardened by heat treatment; they are also nonmagnetic. The most common type is the 18/8 or 304 grade, which contains 18 percent chromium and 8 percent nickel.
The ferritic and martensitic groups both have a bcc microstructure. The latter has a higher carbon level (up to 1.2 percent); it can be hardened and is used for knives and tools. Ferritic stainless steels contain only up to 0.12 percent carbon. Both types have 11.5 to 29 percent chromium as their main alloy addition and practically no nickel. Their corrosion resistance is modest, and they are ferromagnetic.
A special group of stainless steels is employed at high temperatures—e.g., 800° C (1,450° F). Solution hardening is used in this group to keep the steels strong at such heat. They contain up to 25 percent chromium and 20 percent nickel, in addition to small amounts of strong carbide formers such as niobium or titanium to tie up the carbon and avoid a depletion of chromium at the grain boundaries. For even more severe service, as in aircraft jet engines or gas turbines, superalloys are used. These work on the same principle, but they are based on nickel or cobalt or both and contain either no iron at all or only up to 30 percent iron. Their maximum service temperature can reach 80 percent of their melting point.
An important group of steels, necessary for the generation and transmission of electrical power, is the high-silicon electrical steels. Electromagnets for alternating current are always made by laminating many thin sheets, which are insulated in order to minimize the flow of eddy currents and thereby reduce current losses and heat generation. A further improvement is achieved by adding up to 4.5 percent silicon, which imparts high electrical resistance. For electric transformers, grain-oriented sheets are often used; these contain about 3.5 percent silicon and are rolled and annealed in such a way that the edges of the unit cubes are oriented parallel to the direction of rolling. This improves the magnetic flux density by about 30 percent.
Tool steels are produced in small quantities, contain expensive alloys, and are often sold only by the kilogram and by their individual trade names. Generally they are very hard, wear-resistant, tough, inert to local overheating, and frequently engineered to particular service requirements. They also have to be dimensionally stable during hardening and tempering. They contain strong carbide formers such as tungsten, molybdenum, vanadium, and chromium in different combinations and often cobalt or nickel to improve high-temperature performance.
In principle, steel is formed into either flat products or long products, both of which have either a hot-rolled, cold-formed, or coated surface.
Flat products include plates, hot-rolled strip and sheets, and cold-rolled strip and sheets; all have a great variety of surface conditions. They are rolled from slabs, which are considered a semifinished product and are normally not sold. Provided by either a continuous caster or rolled from ingots by a slabbing mill, slabs are 50 to 250 millimetres thick, 0.6 to 2.6 metres wide, and up to 12 metres long (that is, 2 to 10 inches thick, 24 to 104 inches wide, and up to 40 feet long).
Plates are hot-rolled either from slabs or directly from ingots. Maximum dimensions vary with available slab sizes or ingot weights and with the sizes of installed rolling mills and auxiliary equipment. Thickness can be as low as 5 millimetres, but it is usually heavier (e.g., 25 millimetres) and can go as high as 200 millimetres. The width of plates is usually between 1.5 to 3.5 metres, but there are plants that can roll plates up to 5.5 metres wide. The maximum plate length that the largest mills can produce is 35 metres. Plates are usually made in small quantities and to a customer’s specification, with different dimensions and tolerances for flatness, profile, straightness, and other properties. The edges can be ordered in either as-rolled condition or sheared, machined, or gas-cut. Plates are also sometimes cladded with corrosion-resistant sheets.
Hot-rolled strip is often shipped directly from the hot-strip mill in a large coil weighing 10 to 35 tons. Its thickness is 1.5 to 12 millimetres, and its width, depending on the available mill, is 0.7 to 2 metres. Frequently, the large coils are slit into narrower coils or edge trimmed, or they are cut to length into sheets at the finishing section of a steel plant or at a service centre. Coils and sheets are shipped either with the hot-rolled surface or with the scale removed and the surface oiled.
Cold-rolled strip, produced from hot-rolled strip, is 0.1 to 2 millimetres thick and also up to 2 metres wide, depending on a shop’s facilities. Steel plants supply this product in coils or sheets, the latter being cut on special shear lines. Cold-rolled products are available in a great variety of surface conditions, often with a specific roughness, electrolytically cleaned, chemically treated, oiled, or coated with metals such as zinc, tin, chromium, and aluminum or with organic substances. They are usually produced to strict dimensional tolerances in order to assure efficient performance in the highly demanding operations of automated consumer-products industries.
Long products are made of either blooms or billets, which are, like slabs, considered a semifinished product and are cast by a continuous caster or rolled at a blooming mill. Billets have a cross section 50 to 125 millimetres square, and blooms are 125 to 400 millimetres square. In practice, they are not precisely distinguished by these dimensions, and there is considerable overlap in the use of the two terms.
Long products include bars, rods and wires, structural shapes and rails, and tubes. Bars are long products with square, rectangular, flat, round, hexagonal, or octagonal cross sections. The most important bar products are the rounds, which can reach a diameter of 250 millimetres. They are sometimes cold-drawn or even ground to very precise dimensions for use in machine parts. A special group of rounds are the reinforcing bars. Produced in diameters of 10 to 50 millimetres, they provide tensile strength to concrete sections subjected to a bending load. They normally have hot-rolled protrusions on their surface to improve bonding with concrete. Some bar mills also produce small channels, angles, tees, zees, and fence-post sections, with a maximum flange length of 75 millimetres, and call these products merchant bars.
Hot-rolled wire rods are produced in diameters between 5.5 and 12.5 millimetres and are shipped in coils weighing up to two tons. A great portion of these rods are cold-drawn into wire, which is often covered afterward by a metallic coating for corrosion protection. The use of wire is extremely wide, ranging from cords for belted tires to cables for suspension bridges.
The common structural shapes are wide flange I-beams, standard I-beams, channels, angles, tees, zees, H-pilings, and sheet pilings. All these shapes are standardized, and each company has price lists showing which sections are produced and in which quality and length they can be supplied. Railroad rails are always produced to national standards. In the United States, for example, there are rails weighing 115, 132, and 140 pounds per yard and cut to lengths of 39 or 78 feet. There are also a great number of special rails—e.g., for cranes and heavy transfer cars or for use in mines and construction.
Tubular steels are broadly grouped into welded and seamless products. Longitudinally welded tubes are normally produced up to 500 millimetres in diameter and 10 millimetres in wall thickness. Pipes produced from heavy plates are also longitudinally welded after being formed in a U-ing and O-ing operation; they can be 0.8 to 2 metres in diameter, with wall thicknesses up to 180 millimetres. Spiral-welded pipes are sometimes produced in diameters up to 1.5 metres. Seamless tubes are subjected to more demanding service; they are often rolled in diameters ranging from 120 to 400 millimetres and in wall thicknesses up to 15 millimetres, although special rolling mills can often increase the diameter to 650 millimetres. Smaller diameter tubes, both welded and seamless, can be produced by reduction mills or cold-drawing benches. Tubes are frequently machined on both ends for various coupling systems and coated with organic material.
Specifications for steel products as well as testing procedures are normally included in the general standard systems of most industrial countries. Institutions providing these standards are the American Society for Testing and Materials, Philadelphia; British Standards Institute, London; Deutsches Institut für Normung, Berlin; Japanese Industrial Standards Committee, Tokyo; Comité Européen de Normalisation, Brussels; and International Organization for Standardization, Geneva.
There are also product manuals published by a number of associations and societies, sometimes for special products only, that are often used as standards in technical specifications and commercial agreements. Organizations that issue these include the American Iron and Steel Institute, Washington, D.C.; Society of Automotive Engineers, Warrendale, Pennsylvania.; American Petroleum Institute, Washington, D.C.; and American Society of Mechanical Engineers, New York City.
Each steel producer publishes lists showing the steel grades and dimensions that it can deliver. Special alloys and coatings are often supplied under a company-owned trademark. There are also publications that provide cross-references for similar steel grades among the various standards and trademarks issued in different countries.
In principle, steelmaking is a melting, purifying, and alloying process carried out at approximately 1,600° C (2,900° F) in molten conditions. Various chemical reactions are initiated, either in sequence or simultaneously, in order to arrive at specified chemical compositions and temperatures. Indeed, many of the reactions interfere with one another, requiring the use of process models to help in analyzing options, optimizing competing reactions, and designing efficient commercial practices.
The major iron-bearing raw materials for steelmaking are blast-furnace iron, steel scrap, and direct-reduced iron (DRI). Liquid blast-furnace iron typically contains 3.8 to 4.5 percent carbon (C), 0.4 to 1.2 percent silicon (Si), 0.6 to 1.2 percent manganese (Mn), up to 0.2 percent phosphorus (P), and 0.04 percent sulfur (S). Its temperature is usually 1,400° to 1,500° C (2,550° to 2,700° F). The phosphorus content depends on the ore used, since phosphorus is not removed in the blast-furnace process, whereas sulfur is usually picked up during iron making from coke and other fuels. DRI is reduced from iron ore in the solid state by carbon monoxide (CO) and hydrogen (H2). It frequently contains about 3 percent unreduced iron ore and 4 percent gangue, depending on the ore used. It is normally shipped in briquettes and charged into the steelmaking furnace like scrap. Steel scrap is metallic iron containing residuals, such as copper, tin, and chromium, that vary with its origin. Of the three major steelmaking processes—basic oxygen, open hearth, and electric arc—the first two, with few exceptions, use liquid blast-furnace iron and scrap as raw material and the latter uses a solid charge of scrap and DRI.
The most important chemical reactions carried out on these materials (especially on blast-furnace iron) are the oxidation of carbon to carbon monoxide, silicon to silica, manganese to manganous oxide, and phosphorus to phosphate, as follows:
Unfortunately, iron is also lost in this series of reactions, as it is oxidized to ferrous oxide:
The FeO, absorbed into the liquid slag, then acts as an oxidizer itself, as in the following reactions:
In the open-hearth furnace, oxidation also takes place when gases containing carbon dioxide (CO2) contact the melt and react as follows:
The products of the above reactions, the oxides silica, manganese oxide, phosphate, and ferrous oxide, together with burnt lime (calcium oxide; CaO) added as flux, form the slag. Burnt lime has by itself a high melting point of 2,570° C (4,660° F) and is therefore solid at steelmaking temperatures, but when it is mixed with the other oxides, they all melt together at lower temperatures and thus form the slag. A basic slag contains approximately 55 percent CaO, 15 percent SiO2, 5 percent MnO, 18 percent FeO, and other oxides plus sulfides and phosphates. The basicity of a slag is often simply expressed by the ratio of CaO to SiO2, with CaO being the basic and SiO2 the acidic component. Usually, a basicity above 3.5 provides good absorption and holding capacity for calcium phosphates and calcium sulfides.
The majority of sulfur, present as ferrous sulfide (FeS), is removed from the melt not by oxidation but by the conversion of calcium oxide to calcium sulfide:
FeS + CaO → CaS + FeO.
According to this equation, desulfurization is successful only when using a slag with plenty of calcium oxide—in other words, with a high basicity. A low iron oxide content is also essential, since oxygen and sulfur compete to combine with the calcium. For this reason, many steel plants desulfurize blast-furnace iron before it is refined into steel, since at that stage it contains practically no dissolved oxygen, owing to its high silicon and carbon content. Nevertheless, sulfur is often introduced by scrap and flux during steelmaking, so that, in order to meet low sulfur specifications (for example, less than 0.008 percent), it is necessary to desulfurize the steel as well.
A very important chemical reaction during steelmaking is the oxidation of carbon. Its gaseous product, carbon monoxide, goes into the off-gas, but, before it does that, it generates the carbon monoxide boil, a phenomenon common to all steelmaking processes and very important for mixing. Mixing enhances chemical reactions, purges hydrogen and nitrogen, and improves heat transfer. Adjusting the carbon content is important, but it is often oxidized below specified levels, so that carbon powder must be injected to raise the carbon again.
As the carbon level is lowered in liquid steel, the level of dissolved oxygen theoretically increases according to the relationship %C × %O = 0.0025. This means that, for instance, a steel with 0.1 percent carbon, at equilibrium, contains about 0.025 percent, or 250 parts per million, dissolved oxygen. The level of dissolved oxygen in liquid steel must be lowered because oxygen reacts with carbon during solidification and forms carbon monoxide and blowholes in the cast. This reaction can start earlier, too, resulting in a dangerous carbon monoxide boil in the ladle. In addition, a high oxygen level creates many oxide inclusions that are harmful for most steel products. Therefore, usually at the end of steelmaking during the tapping stage, liquid steel is deoxidized by adding aluminum or silicon. Both elements are strong oxide formers and react with dissolved oxygen to form alumina (Al2O3) or silica. These float to the surface of the steel, where they are absorbed by the slag. The upward movement of these inclusions is often slow because they are small (e.g., 0.05 millimetre), and combinations of various deoxidizers are sometimes used to form larger inclusions that float more readily. In addition, stirring the melt with argon or an electromagnetic field often serves to give them a lift.
Deoxidation is also important before alloying steel with easy oxidizable metals such as chromium, titanium, and vanadium, in order to minimize losses and improve process control. Metals that do not oxidize readily, such as nickel, cobalt, molybdenum, and copper, can be added in the furnace to take advantage of high heating rates. In fact, alloying always has thermal effects on steelmaking—for example, the use of energy to heat and melt the alloying agents, or the heat of reaction or solution when they combine with other elements. Fortunately, there exists a large amount of empirical data, obtained from thousands of thermodynamic experiments, that, when supported by theoretical principles, allows steelmakers to predict such temperature changes.
Most alloys are added in the form of ferroalloys, which are iron-based alloys that are cheaper to produce than the pure metals. Many different grades are available. For example, ferrosilicon is supplied with levels of 50, 75, and 90 percent silicon and with varying levels of carbon and other additions.
Also important for steelmaking is the absorption and removal of the two gases hydrogen and nitrogen. Hydrogen can enter liquid steel from moist air, damp refractories, and wet flux and alloy additions. It causes brittleness of solidified steel—especially in large pieces, such as heavy forgings, that do not permit the gas to diffuse to the surface. Hydrogen can also form blowholes in castings. Nitrogen does not move into and out of liquid steel as easily as hydrogen, but it is well absorbed by liquid steel in the high-temperature zones of an electric arc or oxygen jet, where nitrogen molecules (N2) are broken up into atoms (N). Like hydrogen, nitrogen substantially decreases the ductility of steel.
Basic steelmaking takes place in containers lined with basic refractories. These may be bricks or ram material made of highly stable oxides, such as magnesite, alumina, or the double oxides chrome-magnesite and dolomite. It is desirable that the refractories not participate in the steelmaking reactions, but unfortunately they do erode and corrode. Refractory bricks are produced in all shapes and grades by a highly specialized industry.
Testing and sampling are an important part of liquid steelmaking. They are carried out by mechanized and often automated facilities, which immerse lances that are equipped with sensors for rapid computation of temperature and dissolved carbon, oxygen, and hydrogen. Test lances also take samples for analysis in laboratories. All results are usually fed automatically into a process-control computer.
More than half the world’s steel is produced in the basic oxygen process (BOP), which uses pure oxygen to convert a charge of liquid blast-furnace iron and scrap into steel. The basic oxygen furnace (BOF) is a refractory-lined, tiltable converter into which a vertically movable, water-cooled lance is inserted to blow oxygen through nozzles at supersonic velocity onto the charge (see figure). The use of pure oxygen at high flow rates results in such fast oxidation of the elements contained in blast-furnace iron that only about 20 minutes are required per heat—i.e., to refine one charge. Converters vary in size and are operated for heats ranging from 30 to 360 tons.
When oxygen contacts blast-furnace iron, a great amount of heat is released by the ensuing exothermic reactions, especially the oxidation of silicon to silica, so that using only blast-furnace iron would result in a liquid steel temperature too high for casting. Therefore, before the hot metal is added, a specific amount of scrap is charged into the furnace. Melting this scrap consumes about 340 kilocalories per kilogram, effectively cooling the process. A typical BOP charge, therefore, consists of about 75 percent liquid iron and 25 percent scrap. This requires a reliable supply of low-cost iron with a uniform chemical composition, which is attainable only by keeping the operating condition of a blast furnace as constant as possible; this in turn requires a consistent iron consumer. There are also certain iron properties—for example, the silicon and sulfur content—that are selected to optimize the blast furnace and BOF operations and to produce steel at minimal cost. Such interdependence requires that blast furnaces and BOFs work within a well-integrated operating system.
The basic oxygen converter is a cylindrical vessel with an open cone on top. For the largest converters, those that make 360-ton heats, the shell is about 8 metres in diameter and 11 metres high. The shells are built of heavy steel plates and sit in a trunnion ring so that the converter may be rotated for charging, testing, tapping, and slag-off. The lining, normally made of magnesite bricks, has different thickness and brick quality in certain zones, depending on the wear at each location. Total lining thickness of large converters exceeds one metre. The taphole is in the upper zone of the converter, right under the cone.
Oxygen lances are large, multiwall tubes that, on large converters, are about 300 millimetres in diameter and 21 metres long. Their tips have three to five nozzles, directed slightly outward, which produce the supersonic jets of oxygen. Proper water cooling of these lances is crucial. Special lance cranes (see figure) move the lance up and down and adjust its distance from the steel bath. The lances last for about 150 heats before their tips have to be replaced.
BOFs are equipped with huge off-gas systems in order to avoid gas leakage into the shop and to ensure proper cleaning of the gases before they are discharged into the atmosphere. Off-gas emerges from the converter mouth at about 1,650° C (3,000° F). It consists of about 90 percent carbon monoxide and 10 percent carbon dioxide, and it also contains ferrous oxide dust, which forms in the high-temperature zone of the oxygen jet. Two off-gas systems are in use: the full combustion and the suppressed combustion.
In the full-combustion system, off-gas is burned above the mouth of the converter with excess air, and both physical and chemical heat are utilized in a boiler or hot-water system incorporated in the hood and vertical offtakes. A large venturi scrubber or electrostatic precipitator then cleans the cooled off-gas. During the blow of a large converter, about 10,000 cubic metres (350,000 cubic feet) of off-gas is moved per minute through full-combustion apparatus by exhaust fans, and about 0.7 kilogram of iron oxide dust is collected per ton of steel.
In the other system, the suppressed-combustion system, a ring-shaped hood is lowered onto the converter mouth before the blow, keeping air away from the hot off-gases. This means that they are not burned and that their chemical heating value of about 3,000 kilocalories per cubic metre is preserved. The gas is cleaned, collected in gas holders, and used at other locations. Though this system is more complicated, it is much smaller, because off-gases are cooler and there is less to be handled and processed.
BOFs are housed in huge buildings sometimes 80 metres high to accommodate the long lance, the off-gas system, and gravity-type feeding equipment. Heavy cranes, long conveyor belts, and railroad tracks assure prompt supply of raw material to the converters and fast removal of liquid steel and slag from the BOF.
Making a heat begins with an inspection of the refractory lining, with the converter in a turned-down position. Sometimes a laser contour instrument is used to determine the remaining lining thickness. With the converter tilted at about 45°, scrap is then charged into the furnace by heavy cranes or special charging machines that drop one or two large boxes full of scrap through the converter mouth. Hot metal is poured into the converter by a special iron-charging ladle; this ladle receives the iron at a transfer station from transport ladles, which bring the iron from the blast furnace. Many plants lower the sulfur content of the iron just before it is charged into the converter by injecting a lime-magnesium mixture or calcium carbide or both into the charging ladle. Any blast-furnace slag and slag formed during desulfurization is skimmed off before the iron is charged.
Owing to predictable losses during the oxygen blow, there is always more iron and scrap charged than steel produced; for example, 1,080 kilograms of raw material may yield 1,000 kilograms of liquid steel, for a metallic yield of 92.6 percent. Chemical compositions, temperatures, and charging weights of the iron are often fed automatically into a control computer. For blowing, the converter is placed in an upright position, oxygen is turned on, and the lance is lowered. Oxygen flow rates, lance height, and lime additions are often controlled automatically. The flow rates of oxygen at large converters exceed 800 cubic metres per minute, and oxygen consumption is about 110 cubic metres per ton of steel. Usually, about 70 kilograms of pebble-sized burnt lime is added per ton of steel early in the blow; this combines with silica and other oxides to form about 150 kilograms of slag per ton of steel. Adding burnt dolomite (CaO·MgO) results in a magnesia (MgO) content in the slag of about 6 percent, thereby decreasing slag corrosion of the magnesite lining. Lime quality is of great importance in BOF operations, and special lime kilns are used to burn a high grade of limestone.
The oxidation reactions in the converter become violent at the highest rate of carbon removal—that is, when all the silicon is gone—about eight minutes into the blow. At this point oxygen reacts mainly with carbon to generate large amounts of carbon monoxide gas, which mixes with the slag. Keeping the foamy slag from overflowing the converter at the high blowing rates is an important control task. Often a small, water-cooled sensor lance, called the sublance, is immersed into the liquid steel during the end phase of the blow to check and sample the steel. Test results are automatically fed into a control computer, which predicts the end point and shuts off the oxygen when temperature and chemical composition have reached the specified level.
Well-controlled charging conditions make it possible to tap the heat based only on the sublance test. In other cases, the converter must be turned down and the temperature and chemical composition checked manually. Sometimes burnt lime is added and a short reblow is applied in order to increase the temperature or correct the chemical composition. For tapping, the converter is rotated, and steel is poured through the taphole into a ladle sitting on a transfer car beneath the converter. The temperature of the steel at tapping is specifically selected to fit within a temperature “window” for ingot pouring or continuous casting and after all temperature losses expected during treating and holding of the steel in the ladle have been predicted. For example, a 0.1-percent-carbon steel may tap at 1,596° C, 80° C above its theoretical solidification point. Higher carbon steels would be tapped at lower temperatures, following the A-B-C liquidus line of the equilibrium diagram in the figure.
Aluminum or ferrosilicon are added to the ladle before or during the tap in order to lower the level of dissolved oxygen in steel. Ferromanganese is also added, since most of the manganese content of the blast-furnace iron is oxidized during the blow, leaving only about 0.1 percent in the steel—usually not enough to meet specifications.
When slag appears, the converter is rotated all the way back, and the slag is poured over the converter mouth into a slag pot. For better separation of slag from liquid steel, special taphole-closing devices such as refractory balls or nitrogen jets, as well as slag-detection devices, are often used.
BOFs have a tap-to-tap time of 30 to 45 minutes and can blow more than 30 heats per day. Large BOF shops with three converters can produce up to five million tons of liquid steel per year. Repair and maintenance are extremely important, because steel is made around the clock and there is normally only one maintenance shift per week. A converter lining lasts 1,500 to 3,000 heats, after which it is broken out and a new one installed in a mechanized bricklaying operation. Converter relining takes less than one week.
There are a number of significant improvements, modifications, and process changes of the BOF steelmaking system. For example, when high-phosphorus ore is smelted in the blast furnace, and the BOF is consequently charged with a liquid iron containing more than 0.15 percent of that element, the LD-AC process can be followed, in which lime powder is injected through the lance along with oxygen for quick slag formation. A two-slag practice is then followed for sufficient phosphorus removal, with the first slag runoff being sold for fertilizer. Another variation that finds wide application is the injecting of argon (or sometimes nitrogen) into the molten charge through permeable refractory blocks in the bottom of the converter. Bottom stirring enhances chemical reactions and lowers the steel temperature at the oxygen impact area, resulting in less oxidation of iron and better yield. Another system, called the Q-BOP, uses no top lance at all, blowing oxygen, burnt-lime powder, and, when needed, argon upward through the liquid melt from several gas-cooled or oil-cooled bottom tuyeres. These tuyeres are two concentric steel tubes, with oxygen flowing from the inside annulus and gas or oil flowing through the outer annulus. Cooling of the tubes is accomplished by the endothermic heat required to break down the natural gas or oil into carbon monoxide and hydrogen.
The service life of the bottom of the Q-BOP converter is lower than that of the side wall, thus demanding additional maintenance time for bottom changing. On the other hand, bottom blowing has the advantage of generating a large contact surface among all reactants, thus improving metallurgical reactions and process control. Yield is also higher, since there is less local iron oxidation. However, less oxidation also means the release of less exothermic heat; this decreases the quantity of scrap that can be charged, which can be a cost disadvantage when the price of scrap is low. For this reason, some steel plants enhance bottom blowing with a postcombustion top lance. This is an oxygen lance with additional ports at the tip for burning carbon monoxide into carbon dioxide inside the converter. The additional heat generated by this combined blowing practice increases the potential scrap-charging rate.
Another technology for increasing scrap rates uses an oxy-fuel lance, which preheats the scrap in the converter for about 20 minutes before the liquid blast-furnace iron is added. Another scrap-increasing practice adds aluminum to the charge or melt; this releases heat as it is burned during the oxygen blow. Still another process injects coal powder through a modified oxygen lance or through special bottom tuyeres, simultaneously applying additional oxygen and using a postcombustion lance. In trial operations, this combination has resulted in scrap-charging capabilities all the way up to 100 percent; in other words, no hot metal has been charged, and the converter has become a scrap melter. Increasing scrap-charging rates helps to keep the plant operating when the supply of blast-furnace iron is limited, as, for example, during a blast-furnace reline.
About one-quarter of the world’s steel is produced by the electric-arc method, which uses high-current electric arcs to melt steel scrap and convert it into liquid steel of a specified chemical composition and temperature. External arc heating permits better thermal control than does the basic oxygen process, in which heating is accomplished by the exothermic oxidation of elements contained in the charge. This allows larger alloy additions to be made than are possible in basic oxygen steelmaking. However, electric-arc steelmaking is not as oxidizing, and slag-metal mixing is not as intense; therefore, electric-arc steels normally have carbon contents higher than 0.05 percent. In addition, they usually have a higher nitrogen content of 40 to 120 parts per million, compared with 30 to 50 parts per million in basic-oxygen steels. Nitrogen, which renders steel brittle, is absorbed by liquid steel from air in the high-temperature zone of the arc. The nitrogen content can be lowered by blowing other gases into the furnace, by heating with a short arc, and by applying a vigorous carbon monoxide boil or argon stir to the melt.
The major charge material of electric-arc steelmaking is scrap steel, and its availability at low cost and proper quality is essential. The importance of scrap quality becomes apparent when making steels of high ductility, which must have a total maximum content of residuals (i.e., copper, chromium, nickel, molybdenum, and tin) of 0.2 percent. Most of these residuals are present in scrap and, instead of oxidizing during steelmaking, they accumulate and increase in recycled scrap. In such cases some shops augment their scrap charges with direct-reduced iron or cold blast-furnace iron, which do not contain residuals. Generally, the higher contents of carbon, nitrogen, and residuals make the electric-arc process less attractive for producing low-carbon, ductile steels.
Most scrap yards keep various grades of scrap separated. High-alloy shops, such as stainless-steel producers, accumulate, purchase, and charge scrap of similar composition to the steel they make in order to minimize expensive alloying additions.
The electric-arc furnace (EAF) is a squat, cylindrical vessel made of heavy steel plates. It has a dish-shaped refractory hearth and three vertical electrodes that reach down through a dome-shaped, removable roof (see figure). The shell diameter of a 10-, 100-, and 300-ton EAF is approximately 2.5, 6, and 9 metres. The shell sits on a hydraulically operated rocker that tilts the furnace forward for tapping and backward for slag removal. The bottom—i.e., the hearth—is lined with tar-bonded magnesite bricks and has on one side a slightly inclined taphole and a spout or, as shown in the figure, an oval hearth and a vertical taphole. With this latter arrangement, a furnace needs be tilted only 10° for tapping, producing a tight and short tap stream that decreases heat loss and reoxidation of the liquid steel. Before charging, the vertical taphole is closed from the outside by a movable bottom plate and is filled with refractory sand.
Most furnace walls are made of replaceable, water-cooled panels; these are covered inside by sprayed-on refractories and slag for protection and to keep heat loss down. The roof is also made of water-cooled panels and has three circular openings, equally spaced, for insertion of the cylindrical electrodes. Another large roof opening, the so-called fourth hole, is used for off-gas removal. Additional openings in the furnace wall, with water-cooled doors, are used for lance injection, sampling, testing, inspection, and repair. The roof and electrodes can be lifted and moved away for charging scrap and for hearth maintenance.
The graphite electrodes, produced to high standards by a specialized industry, are actually strings of individual electrodes bolted end to end by short graphite nipples. This is done because shorter electrodes are easier to manufacture, transport, and handle. Electrode diameters depend on furnace size; a 100-ton EAF typically uses 600-millimetre electrodes. Three electrode strings are each clamped to arms that extend over the furnace roof and that are bolted to a vertically movable mast located beside the furnace. The mast controls the distance between each electrode tip and the scrap or melt, thereby regulating the arc length and current flow. Power-supply equipment—normally a step-down transformer, vacuum circuit breakers, a tap changer for electrode voltage control, and a furnace transformer—is installed in a concrete vault a short distance from the furnace. Heavy water-cooled cables and the power-carrying arms connect the furnace transformer with the electrodes.
EAF plants are smaller and less expensive to build than integrated steelmaking plants, which, in addition to basic oxygen furnaces, contain blast furnaces, sinter plants, and coke batteries for the making of iron. EAFs are also cost-efficient at low production rates—e.g., 150,000 tons per year—while basic oxygen furnaces and their associated blast furnaces can pay for themselves only if they produce more than 2,000,000 tons of liquid steel per year. Moreover, EAFs can be operated intermittently, while a blast furnace is best operated at very constant rates. The electric power used in EAF operation, however, is high, at 360 to 600 kilowatt-hours per ton of steel, and the installed power system is substantial. A 100-ton EAF often has a 70-megavolt-ampere transformer.
After tapping a heat, the roof is moved away, and the hearth is inspected and, when necessary, repaired. An overhead crane then charges the furnace with scrap from a cylindrical bucket that is open on the top for loading and fitted with a drop bottom for quick charging. Scrap buckets are loaded in such a manner as to assure a cushioning of heavy scrap when the load drops onto the hearth in order to obtain good electrical conductivity in the charge, low risk of electrode breakage, and good furnace wall protection during meltdown. Carbon and slag formers are sometimes added to the charge to prevent overoxidation of the steel and to quicken slag formation. After charging one bucket, the roof is moved back to the furnace, and the electrodes are lowered. Meltdown begins with a low power setting until the electrodes have burned themselves into the light scrap on top of the charge, protecting the sidewalls from overheating during higher-power meltdown. Leaving some scrap unmelted at the furnace wall for its protection, a second bucket is charged and the same meltdown procedure is followed. Melting very light scrap sometimes requires the charging of a third or even fourth bucket.
After meltdown, the carbon level in the steel is about 0.25 percent above the final tap level, which prevents overoxidation of the melt. By this time a basic slag has formed, typically consisting of 55 percent lime, 15 percent silica, and 15 to 20 percent iron oxide. Slag foaming is often generated by injecting carbon or a lime-carbon mixture, which reacts with the iron oxide in the slag to produce carbon monoxide gas. This foam shields the sidewall and permits a higher power setting. As required, the carbon content of the steel is either decreased by oxygen blowing or increased by carbon injection. Samples are taken, the temperature is checked, additions are made, and, when all conditions are right, the furnace is tapped by rotating it forward so that the steel flows over the spout or through the vertical taphole into a ladle. When slag appears, a quick back tilt is applied and the slag is poured through the rear door of the furnace into a slag pot. Some shops leave 15 percent of the liquid steel in the furnace. This “hot heel” practice permits complete slag separation.
Very clean steel—i.e., with low oxygen and sulfur content—can be produced in the EAF by a two-slag practice. After removal of slag from the first oxidizing meltdown, new slag formers are added that contain carbon or aluminum or both as reducing agents. The new reducing slag may consist of 65 percent lime, 20 percent silica, calcium carbide or alumina (or all three), and practically no iron oxide. Alloys, which oxidize easily, are added at this time to minimize losses and to improve metallurgical control. Refining continues under the reducing slag until the heat is ready for tapping. Total heat time is one to four hours, depending on the type of steel made—that is, on the amount of refining applied and auxiliary heating used. Many shops do not apply a two-slag practice but treat the steel, after scrap meltdown and tapping, in ladle treatment stations. These secondary metallurgical plants, discussed below, allow the EAF to run only as a highly efficient scrap melter.
From time to time, as the arc erodes their tips and the high-temperature furnace atmosphere oxidizes their bodies, new electrodes are added to the top of the electrode strings at the furnace. Electrodes are consumed at the rate of three to six kilograms per ton of steel, depending on the type of operation.
In order to lower power consumption, scrap can be preheated in both batch and continuous processes, often utilizing the heat of furnace off-gases. Scrap preheating to 500° C (930° F) cuts power consumption by 40 to 50 kilowatt-hours per ton, and decreases tap-to-tap time and electrode consumption. Sometimes scrap is preheated inside the EAF by oxyfuel burners, but this requires a large off-gas system for handling combustion gases. In addition, for better mixing and heat transfer, electromagnetic coils or permeable refractory blocks for gas stirring are often installed in furnace bottoms. Applying these methods and using the EAF as a scrap melter can reduce power and electrode consumption to a mere 360 kilowatt-hours per ton and three kilograms per ton. Heat times are reduced to about one hour. This means, by applying methods originally developed for the basic oxygen process, the EAF can approach the steelmaking rates of the BOF.
Several EAFs are operated by direct current (DC) instead of alternating current (AC). DC furnaces normally have only one very large electrode extending through the centre of the roof, with the counter electrode embedded in the furnace bottom and contacting the melt. A hot heel is kept in the furnace to ensure a good current flow through the charge. Power and electrode consumption is lower than in regular AC furnaces. The DC arc has a steadier and quieter burn, which results in less disturbance of the surrounding power system and less noise around the furnace. The electrical equipment is smaller but still expensive because of the required rectifiers. Critical in DC furnace operation are the short life of the bottom electrode, integrity of the hearth, and current limitations with a one-electrode system. Furnaces with capacities up to 130 tons are in operation.
Though it has been almost completely replaced by BOF and EAF steelmaking in many highly industrialized countries, the open hearth nevertheless accounts for about one-sixth of all steel produced worldwide.
The open-hearth furnace (OHF) uses the heat of combustion of gaseous or liquid fuels to convert a charge of scrap and liquid blast-furnace iron to liquid steel. The high flame temperature required for melting is obtained by preheating the combustion air and, sometimes, the fuel gas. Preheating is done in large, stovelike regenerators or checker chambers, located beneath the furnace (see figure). These contain checker bricks stacked in such a way that they absorb heat from furnace off-gases as they are directed through the chamber. After one chamber has been heated for about 20 minutes, a sliding valve is activated, directing the off-gases to the other chamber and simultaneously bringing air into the heated chamber. This combustion air, after picking up the heat from the checker brick, then enters the furnace through an end-wall above the checker chamber and burns the fuel, which also enters the furnace at the same wall. The combustion flames heat the charge, and the off-gases, after moving across the hearth to the other end wall, are directed downward to heat the other chamber. This cycle, with entry ports becoming exit ports, is reversed every 15 to 20 minutes. After heating the regenerator, the off-gases flow through a heat-recovery boiler and a gas-cleaning system before they are discharged into the atmosphere through a stack.
The OHF itself consists of a shallow, rectangular hearth that holds the charge, liquid steel, and slag (see figure). Depending on the furnace size, the long front wall on the charging side usually has three to seven rectangular openings fitted with water-cooled doors. These are used for charging scrap and iron, adding flux and alloying agents, running off slag, conducting tests, and repairing the hearth refractory. On the opposite side of the furnace, at the back wall, is the taphole and a spout for tapping into one or two ladles. The two end walls are used as inlets or outlets for gas and air, and they also hold the injection burners for heavy oil, tar, or natural gas, when used.
Above the hearth, an arched roof contains the flames and reflects the heat onto the melt. Since thermal exposure is intense here, the roof is made of high-grade chrome-magnesite refractory bricks suspended from a steel structure. Many furnaces have one to four retractable oxygen lances installed in the roof to increase the flame temperature and melting rate.
OHFs vary considerably in size, having been built for heats of 10 to 600 tons. The hearth of a 150-ton-capacity OHF is approximately 15 metres long and 5 metres wide. There are often up to a dozen furnaces in one shop, lined up end wall to end wall only a few metres apart with all front doors on one line and at the same level. This permits the charging of all furnaces by the same charging machine, crane, and rail system. Bulk materials, such as scrap, cold blast-furnace iron, ore, limestone, coke, and alloying agents, are charged through the furnace doors in small boxes of one- to two-cubic-metre capacity. The boxes are brought to the OHF on small railroad buggies, and a charging machine then moves one box after another through a door, turns them over, and dumps their contents onto the hearth.
When starting a heat, the hearth is first covered by limestone flux, and scrap is charged on top of that. Charging a large furnace may require two to three hours and as many as 150 full charging boxes. The burners and oxygen lances are on during charging, so that most of the scrap has been melted by the end of the scrap charge. Afterward a special pouring spout is placed into one of the doors, and blast-furnace iron is slowly poured from an iron ladle into the melt. Composition of the metallic charge varies from 20 percent scrap and 80 percent blast-furnace iron to 100 percent scrap; a common proportion is 60 percent iron and 40 percent scrap.
Carbon in the poured iron reacts with the oxidized molten scrap and generates the carbon monoxide boil. This stirs the shallow (about 300 millimetres deep) bath and accomplishes a high heat transfer and a good mixing of the slag and metal. The carbon monoxide boil may last two to three hours, during which time carbon is oxidized and lowered, slag is flushed off through the doors, and the temperature is raised. Increasing heat causes the limestone charged beneath the scrap to calcine and release carbon dioxide, according to the following reaction:
This begins the lime boil, which has a beneficial stirring effect similar to that of the carbon monoxide boil. After about one hour, the calcined lime rises through the melt and is dissolved in the slag.
During the subsequent refining period, flux and alloys are added, and oxygen or carbon is injected to lower or raise the carbon content. When temperature and chemical composition are in the specified range, the furnace is tapped by blowing the taphole open with a small explosive charge. Tap-to-tap time is six to nine hours, often including one hour for inspection, cleaning, and hearth repair. After 200 to 300 heats, there is usually a three-day process of checker cleaning and more extensive repair work. The roof is usually replaced after about 1,000 heats, which shuts the furnace down for one week. The hearth, being made up anew after every heat, lasts many years.
Used by many specialty steelmaking shops and foundries, induction furnaces are cylindrical, open-topped, tiltable refractory crucibles with a water-cooled induction coil installed on the outside, around the side wall. The coil is powered by alternating current, which induces eddy currents in the metallic charge that generate heat. The refractory wall of the crucible is usually thin enough to achieve good penetration of the electromagnetic field into the charge.
Induction furnaces are used mainly for remelting and alloying and have very limited refining capabilities; in other words, they are not used for carbon, phosphorus, or sulfur removal. The slag is cold and not very active, and often there is no slag at all. However, the electromagnetic field stirs the melt well, and this is beneficial for alloying. Most furnaces’ coils are powered by line frequency (i.e., 50 or 60 hertz), but there are also furnaces powered by medium frequency (e.g., up to 4,500 hertz), utilizing solid-state frequency converters. The electrical system always includes capacitor banks to compensate for the high inductance of the furnace coil. Efficiency of converting electric power into heat is about 75 percent, and power consumption is around 550 kilowatt-hours per ton of steel.
In commercial operation, a hot heel is often left in the furnace after tapping in order to decrease the thermal shock on the lining generated by the water-cooled coil. Smaller furnaces use prefabricated crucibles, but larger furnaces have a rammed—that is, compacted and dried—refractory mass as lining. Computer control is well utilized in this system, monitoring, for instance, the crucible lining thickness by the electrical performance of the furnace coil. The capacity of the furnace varies from a few kilograms to 50 tons.
Many induction furnaces are installed and operated in vacuum chambers. This is called vacuum induction melting, or VIM. When liquid steel is placed in a vacuum, removal of carbon, oxygen, and hydrogen takes place, generating a boil in the crucible. In many cases, the liquid steel is cast directly from the furnace into ingot molds that are placed inside the vacuum chamber.
An open-topped cylindrical container made of heavy steel plates and lined with refractory, the ladle is used for holding and transporting liquid steel. Here all secondary metallurgical work takes place, including deslagging and reslagging, electrical heating, chemical heating or cooling with scrap, powder injection or wire feeding, and stirring with gas or with electromagnetic fields. The ladle receives liquid steel during tapping while sitting on a stand beneath the primary steelmaking furnace. It is moved by cranes, ladle cars, turntables, or turrets. A ladle turret has two liftable forks, usually 180° apart, that revolve around a tower, each fork capable of holding a ladle. Ladles have two heavy trunnions on each side for crane pickup. Support plates under each trunnion are used for setting the ladles onto stands or ladle cars.
The side wall of a ladle is slightly cone-shaped, with the larger diameter on top for easy removal of a skull—i.e., solidified steel and slag. A ladle capable of holding 200 tons of steel has an outside diameter of approximately four metres and is about five metres high. Inside the ladle there is usually a 60-millimetre-thick refractory safety lining next to the shell. The working lining, that part contacting the steel and slag, is 180 to 300 millimetres thick, depending on ladle size and location in the ladle. The lining thickness and type of brick in one ladle are often different to counteract increased wear at certain locations—for example, at the impact area of the tapping stream or at the slag line. This results in more equal wear on the ladle lining and an extended ladle service life.
Sometimes, fired clay bricks are used because they bloat—that is, they expand during heating and seal the joints between them. Their thermal shock resistance is high, but their resistance to slag corrosion is low, so that the working lining has to be replaced every 6 to 12 heats. Because ladle rebricking takes about eight hours, up to 12 ladles are sometimes in use in large steelmaking shops in order to assure availability. For ladle operations requiring longer holding times, higher-grade refractory linings are made of high alumina or magnesia bricks. These give greater slag resistance, but they do not bloat and are less resistant to thermal shock. For these reasons, they are kept hot at special preheating stations. Ladles that use these bricks have service lives of up to 80 heats, so that fewer ladles are required. Preheating also decreases the heat loss of liquid steel during tapping and holding.
Except for very small ladles, which pour over the lip and a spout or through a teapot arrangement when tilted, most ladles have a funnel-shaped nozzle with a closing device installed in the bottom. Depending on ladle size, these nozzles have an orifice diameter of 15 to 100 millimetres and are made of high-grade refractory material. Often they are opened and closed by a vertical steel stopper rod, which is enclosed in refractory sleeves and partly immersed in the liquid steel. The head of the stopper rod closes the nozzle and is lifted a specific distance for controlling the flow rate; on top it is connected to a vertical slide that is either manually operated by a lever or remotely controlled from the crane pulpit.
Many shops use a slide-gate nozzle, which consists, in principle, of a fixed upper and a movable lower refractory plate. Both plates have holes that are adjusted relative to each other for closed, throttled, and full-open position. The lower plate is hydraulically shifted and is usually replaced after every heat. In a similar system, an old plate is pushed out by a new plate while pouring, and flow control is accomplished by using bottom plates with different orifice diameters. Having the entire flow-control system on the outside of the ladle and the inside of the ladle completely unrestricted is necessary for operating with long holding times and for certain steel treatments conducted in the ladle.
Ladles are often built with one or more permeable refractory bottom blocks and argon hookups for gas stirring. Ladles can also be placed against an electromagnetic stirring coil installed on a ladle car; in this case, their shells are made of a nonmagnetic alloy.
A number of shops use ladle lids to limit the liquid-steel heat loss. Lid-handling systems are normally mechanized, and removing, storing, and placing lids onto the ladles is done automatically.
The carrying out of metallurgical reactions in the ladle is a common practice in practically all steelmaking shops, because it is cost-efficient to operate the primary furnace as a high-speed melter and to adjust the final chemical composition and temperature of the steel after tapping. Also, certain metallurgical reactions, for reasons of equipment design and operation, are more efficiently performed in the ladle. The simplest form of steel treatment in the ladle takes place when the mixing effect of the tapping stream is used to add deoxidizers, slag formers, and small amounts of alloying agents. These materials are either placed into the ladle before tapping or are injected into the tapping stream.
Deoxidation reactions carried out in the ladle are exothermic and thus raise the temperature of the liquid steel, but the steel also loses heat by radiation from the top surface, by heating of the ladle lining, and by heat flux through the lining and shell. Temperature drops that take place when just holding the steel can range from 0.3° to 2° C per minute. (Small ladles, owing to their high surface-to-volume ratio, have a greater temperature loss than large ladles.) The rate of temperature drop then slows as the refractories become heated and a steady flow of heat prevails through the lining and slag layer.
Tapping at the right temperature is necessary in order to meet critical temperature windows for teeming or casting operations. Heat losses during and after tap can usually be predicted by computer, using a process model that considers the temperature and configuration of the tap stream, the thermal condition of the ladle before tap, the thicknesses of the ladle lining and slag layer, the expected holding times and stirring conditions, and the thermal effects of alloying additions. Actual control over steel temperature can be achieved in a ladle furnace (LF). This is a small electric-arc furnace with an 8- to 25-megavolt-ampere transformer, three electrodes for arc heating, and the ladle acting as the furnace shell—as shown in A in the figure. Argon or electromagnetic stirring is applied for better heat transfer. Most LFs can raise the temperature of the steel by 4° C per minute, and several shops accomplish an increase of 4° to 6° C by inducing a strong exothermic chemical reaction (for instance, by feeding aluminum and injecting oxygen) at the stirring station. Subsequent argon stirring removes most of the alumina inclusions formed by this process. Both heating technologies permit long holding times of full ladles and improve the continuous caster operation.
Keeping furnace slags on the molten steel too long can result in a reversion of elements such as phosphorus back into the steel. To avoid this, slag can be removed at slag-skimming stations, where the ladle is tilted forward and a rake scrapes the slag into a slag pot parked beneath the ladle. Some shops use a vacuum system, which sucks the slag off the liquid steel and granulates it instantaneously. In either case, after slag removal the steel is covered with slag formers or an insulating layer to minimize heat loss and reoxidation. Special equipment is used to quickly place a blanket of material on the steel surface.
In most continuous casting operations, it is necessary to maintain minimal fluctuation in steel temperature, and this requires the use of a ladle stirring station to establish a uniform temperature and chemical composition throughout the ladle. The steel can be stirred by argon injected through a refractory-lined lance or through a permeable refractory block in the bottom of the ladle, or it can be stirred by an electromagnetic coil.
Additions are usually made at the stirring station by a wire feeder, which runs a heavy wire at controlled speed through a refractory-covered lance and into the steel. Aluminum wire is often used for trimming; other materials, such as calcium-silicon, zirconium, and rare-earth metals, are often enclosed in thin steel tubes and are fed by the same machines. The wires and filled tubes are normally shipped to steel plants in large coils, but there are also machines that fill the tubes with the appropriate materials on-site.
Another widely used treatment is powder injection. Powdered metal is fluidized by argon in a pressure vessel and injected by a refractory-lined lance deep into the liquid steel. Because powder has a large contact surface area, it reacts quickly with the steel. Deep injection is beneficial when adding materials such as calcium or magnesium, which evaporate at steelmaking temperature, because ferrostatic pressure suppresses the evaporation of these metals for some time. Powders are shipped to the shop in sealed containers or in special tank cars topped with inert gas.
Many powder-injection stations are used for desulfurization. One effective desulfurizer is a calcium-silicon alloy containing 30 percent calcium. Metallic calcium desulfurizes by forming the very stable compound calcium sulfide (CaS), and it is alloyed with silicon because pure calcium reacts instantaneously with water and is therefore difficult to handle. Injecting four kilograms of calcium-silicon per ton of steel can remove approximately three-quarters of the sulfur, so that the sulfur content will drop, for example, from 0.016 to 0.004 percent. For steel grades that do not permit silicon additions, a magnesium-lime mixture is used. Magnesium is a good desulfurizer, and it also acts as a deoxidizer by combining locally with dissolved oxygen. This makes it possible for the lime to desulfurize the steel according to the following reaction:
Like magnesium, lime has a double function, because it helps to prevent the very low-melting magnesium powder from melting inside the lance.
Adding calcium accomplishes another important function. Sulfur is normally present in solidified steel in the form of manganese sulfide inclusions, which are soft at hot-rolling temperatures and are rolled into long strings or platelets. This results in poor physical properties of the steel in directions perpendicular to that of the rolling. The addition of calcium improves these properties by forming strong inclusions, containing mainly calcium sulfide, that are not plastic at hot-rolling temperatures. This phenomenon, called inclusion shape control, can also be achieved by small additions of zirconium or rare earth.
Exposing steel to vacuum conditions has a profound effect on all metallurgical reactions involving gases. First, it lowers the level of gases dissolved in liquid steel. Hydrogen, for example, is readily removed in a vacuum to less than two parts per million. Nitrogen is not as mobile in liquid steel as hydrogen, so that only 15 to 30 percent is typically removed during a 20-minute vacuum treatment.
Another important process is vacuum decarburization and deoxidation. In theory, oxygen and carbon, when dissolved in steel, react to form carbon monoxide until they reach equilibrium at the following relationship:
This means that, under vacuum conditions (when there are only small amounts of carbon monoxide in the surrounding gas and therefore little carbon monoxide pressure), carbon and oxygen will react vigorously until they reach equilibrium at very low levels. For instance, liquid steel at 1 atmosphere pressure may contain 0.043 percent carbon and 0.058 percent oxygen, but, if the pressure is lowered to 0.1 atmosphere, the two elements will react until they reach equilibrium at 0.014 percent carbon and 0.018 percent oxygen. Under a pressure as low as 0.01 atmosphere, equilibrium will be reached at 0.004 percent carbon and 0.006 percent oxygen. In practical operation, the obtainable levels of carbon and oxygen are far above equilibrium conditions, because the movement of carbon and oxygen atoms in liquid steel is time-consuming and treatment time is limited. In addition, the steel is continuously reoxidized by multiple sources of oxygen. Nevertheless, it is common practice to produce ultralow-carbon steel, containing less than 0.003 percent carbon, in 20 minutes at a vacuum treatment station under pressure of one torr. (In vacuum technology, pressures are often expressed in torr, which is equivalent to the pressure of a column of one millimetre of mercury. One atmosphere equals 760 torr.)
There are several types of vacuum treatment, their use depending on steel grade and required production rates. In the tank degasser (shown in B in the figure), the ladle is placed in an open-top vacuum tank, which is connected to vacuum pumps. The vacuum pumping system often consists of two or three mechanical pumps, which lower the pressure to about 0.1 atmosphere, and four or five stage steam ejectors, which bring the pressure to under 1 torr, or 0.0013 atmosphere. Practical treatment time is 20 to 30 minutes. The ladles used in tank degassing stations are large and, when filled with steel, retain about one metre of freeboard in order to contain the melt during a vigorous boil.
A modification of the tank degassers is the vacuum oxygen decarburizer (VOD), which has an oxygen lance in the centre of the tank lid to enhance carbon removal under vacuum. The VOD is often used to lower the carbon content of high-alloy steels without also overoxidizing such oxidizable alloying elements as chromium. This is possible because, in the pressure-dependent carbon-oxygen reaction outlined above, oxygen reacts with carbon before it combines with chromium. The VOD is often used in the production of stainless steels.
There are also tank degassers that have electrodes installed like a ladle furnace, thus permitting arc heating under vacuum. This process is called vacuum arc degassing, or VAD.
For higher production rates (e.g., 25 ladles treated per day) and large ladles (e.g., 200 tons), a recirculation degasser is used, as shown in C in the figure. This has two refractory-lined snorkels that are part of a high, cylindrical, refractory-lined vacuum vessel and are immersed in the steel. As the system is evacuated, atmospheric pressure pushes the liquid steel through the snorkels and up into the vessel. One atmosphere lifts liquid steel about 1.3 metres. Injecting argon into one of the snorkels then circulates the steel through the vessel, continuously exposing a portion of the steel to the vacuum. Recirculation facilities are often very elaborate, using fast vessel-exchange systems or even two operating vessels at one station to achieve high production rates. Some units also inject oxygen during vacuum treatment, through either the side or the top of the vessel. This is done to speed up decarburization or, by simultaneously adding aluminum, to increase the steel temperature. Some shops apply a similar system but use a vacuum vessel with only one snorkel. Here, a portion of the steel in the ladle flows in and out of the vacuum vessel and is exposed to the vacuum by a continuous raising and lowering of either the vessel or the ladle.
In the production of stainless steel and other high-alloy grades that contain highly oxidizable elements such as chromium, lowering the levels of carbon by regular oxygen injection has the undesirable consequence of oxidizing the alloying elements as well. The argon-oxygen decarburization (AOD) process alleviates this problem by diluting the injected oxygen with argon. This lowers the partial pressure of oxygen and carbon monoxide, so that, based on the pressure-dependent equilibrium relationship %C × %O = 0.0025 × CO pressure, the oxygen prefers to combine with carbon and oxidizes only a small amount of alloy.
The AOD process is carried out in a refractory-lined converter similar to the BOF but with two to six argon-oxygen tuyeres installed in the lower side wall. The tuyeres consist of two concentric steel tubes, with the inert gas flowing in the outer annulus and oxygen in the inner tube. The converter has tilting and emission-control equipment similar to that of the BOF; the lining is also basic, but it lasts only 50 to 100 heats because of the long refining time and the high temperature of more than 1,700° C (3,100° F) that is necessary for improving the chromium yield. Most shops have three converter shells and one trunnion ring at a blowing station, rotating them between operation, relining, and preheating.
When making austenitic stainless steel, the AOD converter is charged with liquid high-carbon chromium-nickel steel that has been melted in a regular EAF and may contain 1.5 percent carbon, 19 percent chromium, and 10 percent nickel. The blow starts with a high-oxygen gas mixture of, for instance, 80 percent oxygen and 20 percent argon, because there is still plenty of carbon in the steel with which oxygen prefers to combine. As the carbon level drops, the gas mixture is gradually changed into one rich in argon; this may end with a blowing gas of 20 percent oxygen and 80 percent argon. After a blowing time of about one hour, the final carbon content is on the order of 0.015 percent, and only about 2 percent chromium has been lost. The steel is then deoxidized by ferrochrome silicon and desulfurized with burnt lime. Argon is also blown during this end phase for better mixing and removal of hydrogen and nitrogen.
The tap-to-tap time is about two hours, and consumption of oxygen and argon is about 25 and 20 cubic metres, respectively, per ton of steel. To minimize cost, argon is sometimes replaced by nitrogen or compressed air at the beginning of the blow. AOD converters with capacities up to 160 tons are in operation.
The simplest way to solidify liquid steel is to pour it into heavy, thick-walled iron ingot molds, which stand on stout iron plates called stools.
During and after pouring, the walls and bottom of the mold extract heat from the melt, and a solid shell forms, growing approximately with the square root of time multiplied by a constant. The value of the constant depends on the heat flux between the already solidified shell and the cooling media surrounding it and is actually equivalent to the solidified shell’s thickness after one minute—namely, about 20 millimetres when solidifying steel. Accordingly, the ingot shell is about 40 millimetres thick after four minutes and 60 millimetres after nine minutes. As the shell thickens, the level of the liquid melt in the centre of the mold drops, because solidified steel has a higher density than liquid steel—i.e., 7.86 versus 7.06 grams per cubic centimetre (4.5 versus 4.1 ounces per cubic inch). This creates a cavity on top of the ingot, as shown in A in the figure by a schematic presentation of solidifying layers. Since an open cavity oxidizes, it does not weld during hot rolling and must be cut off, resulting in a loss of steel. The cavity can be made shallower by keeping the top of the ingot hot and liquid longer. This is done by inserting insulating refractory heads (as shown in C in the figure) and by adding exothermic powders; more liquid steel can also be added after a good-sized shell has formed.
The solidification pattern described above can be observed in well-deoxidized steel, which shows no evolution of gas as it solidifies. For this reason, it is called a killed steel. A different solidification pattern is applied to certain other steels to which fewer deoxidizers have been added. These contain a controlled amount of dissolved oxygen, which, during solidification, reacts with carbon and generates a mild carbon monoxide boil. The rising carbon monoxide bubbles stir the melt, lift inclusions, and cause the formation of a very clean shell about 50 millimetres thick, called the rim. After the rim has formed, a cooling plate is placed on top of the ingot, freezing a layer of liquid steel and trapping the gas bubbles inside the solidifying ingot, as shown in B in the figure. This ingot has no open cavity, but there are many blowholes in the centre that normally weld together during hot-rolling. Low-carbon steel, because of its higher dissolved oxygen content, is often cast this way and is called rimmed steel. Normally, rimmed steel is cast into a big-end-down mold, as shown in B in the figure, for easier mold stripping and ingot handling.
An important characteristic of all solidification processes is segregation. This takes place when crystals grow in a multicomponent melt, because crystals are always purer than the liquid melt from which they solidify. Therefore, as steel solidifies, the levels of carbon, phosphorus, and sulfur grow in the remaining liquid, resulting in an enrichment of these elements in the centre of the ingot. Segregation can be minimized by keeping segregating elements at low levels or by solidifying at a fast rate—i.e., by not providing the time for separation. It is also impaired by stirring the melt.
The layouts of pouring pits differ greatly, depending on the type of steel produced and the rate of production. In top pouring conducted in high-tonnage shops, a row of perhaps 20 molds is lined up in buggies on a railroad track in front of a pouring platform (see figure). A crane brings the ladle to the platform and holds it while the operator fills one mold after another. After standing for a specified time, the molds are pulled out of the teeming aisle and into a stripper building, where they are lifted from the ingots. In a different procedure, called bottom pouring, as many as six ingot molds stand on a single large and thick bottom plate with several pipelike refractory runners installed on its top surface. These runners connect the molds to a refractory-lined, funnel-shaped feeder tube, which receives liquid steel from the ladle and directs it to the molds, filling them simultaneously from the bottom. Bottom pouring avoids the splashing from the ladle stream that is experienced during top pouring. The system is often completely mechanized, with the bottom plates movable on wide transfer tracks and prepared for the next use away from the pouring aisle.
Iron molds are cleaned and repaired in a mold yard. Depending on practice, they are replaced after 40 to 70 pours. Most specialty steel shops pour their alloy grades in big-end-up molds and use hot tops, as shown in C in the figure, in order to minimize the size of the cavity and consequent steel loss. All large ingots—for instance, 200-ton ingots intended for forgings—are also poured this way.
About 55 percent of the world’s liquid steel production is solidified in continuous casting processes, the most widely used of which feeds liquid steel continuously into a short, water-cooled vertical copper mold and, at the same time, continuously withdraws the frozen shell, including the liquid steel it contains.
The key control parameter of continuous casting is matching the flow of liquid steel into the mold with the withdrawal speed of the strand out of the mold. The control of flow rates is accomplished by the tundish, a small, refractory-lined distributer that is placed over the mold and that receives steel from the furnace ladle (see figure). Withdrawal speed is controlled by driven rolls, which contact the strand at a point where it has already developed a thick, solidified shell.
Feeding of the caster mold from the tundish is controlled by a stopper rod or a sliding gate similar to the equipment used in ladles (see above Secondary steelmaking: The ladle: Tapping). The liquid steel in the tundish must be within a specific temperature “window”—a range just above its liquidus that is determined by the steel’s grade; in addition, measures are always taken to keep air away from the steel in order to minimize reoxidation. Shielding can be accomplished by pouring steel through refractory tubes that are immersed in the steel or through wide sleeves that are pressurized with argon. The tundish itself is covered with a lid and is often also topped with argon. Both ladle and tundish sit on a turret or transfer car to permit a quick exchange.
The mold is made of copper because of the high heat conductivity of that metal. It is heavily water-cooled and oscillates up and down to avoid sticking of the solidified shell to its walls. In addition, the mold wall is lubricated by oil or slag, which is maintained on the steel meniscus and flows down into the gap between mold and strand. The slag layer, when used, is formed by the continuous addition of casting powder. Besides providing lubrication, it keeps air away from the liquid steel, acts as a heat barrier, and absorbs inclusions.
Many continuous casters contain sensors in the mold for automatically synchronizing the flow of liquid steel into the mold with the strand withdrawal speed. As it exits the mold, the strand has a shell thickness of only about 10 millimetres and is immediately water-cooled by spray nozzles. The strength and soundness of the shell at this location determine the maximum casting speed, because rupturing it would result in a breakout of liquid steel and damage to the caster. On its way down, the strand is supported by many rolls to avoid a bulging of the shell by the ferrostatic pressure of the liquid steel it contains. As the shell thickness increases toward the end of this so-called secondary cooling zone, the supporting rolls grow larger and are spaced farther apart. The secondary zone is often also called the metallurgical length, because this is where the strand solidifies and the cast structure develops. Depending on the strand’s cross section and the casting speed, it can be 10 to 40 metres long. The flow of water to the many nozzles in the various sections is often computer-controlled and automatically adjusted as casting conditions change.
After the strand passes through the last pair of support rolls, it enters the run-out table and is cut, while moving, by one or two oxyacetylene torches.
Continuous casters in commercial operation are built according to different design principles. For some steels and solidification patterns, all components are arranged in a vertical line—a straight mold, a straight secondary cooling zone, and vertical strand cutting. Other casters also have a straight mold and a vertical secondary cooling zone, but they bend the strand on its way down, after it has solidified, into a horizontal direction and cut it on a run-out table. (In spite of the horizontal turn, even this design requires a high building and a long ladle lift.)
The majority of continuous casters have a curved mold, a curved secondary cooling zone, and a series of straightening rolls before the horizontal run-out table. Everything down to the straightener is on one radius or on several matching radii. This design results in a low casting machine, as shown in the figure.
Different design principles are used for casting strands of different cross sections. Billet casters solidify 80- to 175-millimetre squares or rounds, bloom casters solidify sections of 300 by 400 millimetres, and beam blank casters produce large, dog-bone-like sections that are directly fed into an I-beam or H-beam rolling mill. Huge slab casters solidify sections up to 250 millimetres thick and 2,600 millimetres wide at production rates of up to three million tons per year.
In order to match the quantity of steel produced in a heat with the solidification capacity of a mold for a certain strand section, it is often necessary to use a multistrand caster. Some billet casters have six molds in one line next to one another, and all are fed from the same tundish.
To begin casting, a starter head matching the inside dimension of the mold and connected to a starter chain is moved up into the mold. The starter chain has dimensions similar to the strand to be cast and is long enough to be moved up and down by the driven rolls. When liquid steel fills the mold, it freezes to the caster head, which is immediately withdrawn. The chain in front of the solidifying strand moves through the secondary cooling zone, and, after the head has cleared the last support roll, it is disconnected from the strand by an upward-moving push-out roll. The chain is then pulled by a winch onto a support cradle, lifted from the table, and stored for reuse. At the end of casting, when the tundish is almost empty, the flow of steel to the mold is discontinued, and the strand is stopped and, after solidifying, completely withdrawn. For the next cast, the starter chain, with the head in front, is moved again by the driven rolls into the secondary cooling zone and mold.
Casting of one ladle takes 45 to 90 minutes, depending on heat size, steel grade, caster layout, and casting conditions. Turning the caster around—that is, preparing it for the next cast—is usually accomplished in a half hour, but it takes longer when the mold is changed for casting a different section. Slab casters often use molds with movable side plates, thus permitting a fast change of width during caster turnaround or even during casting. Such devices, together with fast exchange systems for casting tubes, tundishes, and ladles, permit sequential heats to be cast without stopping the caster—sometimes for several days. Starting and stopping a caster causes a few metres of steel on both ends of the strand to fall below the specified properties, thereby lowering the steel-to-strand yield. In sequential casting, on the other hand, the yield from liquid steel to acceptable strand approaches 100 percent, compared with perhaps 93 percent when turning the caster around after each ladle or to 86 percent in an ingot-casting operation that uses a blooming or slabbing mill to roll a slab or bloom of the same size. The benefits are substantial because much less raw material, liquid steel, and energy are needed to make the same tonnage of cast product.
Metallurgical quality is often enhanced by computer control over some or all systems of the caster. Casting conditions are often further improved by electrical tundish heating to adjust steel temperature, by electromagnetic stirring coils around the strand to decrease segregation, by in-line rolling to compact the centre just before it solidifies, and, most important, by well-designed inspection systems to check the liquid steel and the hot strand during casting. Such systems provide a high level of quality assurance, making it possible to charge the cut strand hot into a reheat furnace or, with only a little reheating of the edges, directly into a hot-rolling mill. This not only minimizes reheating but eliminates cooling, cold inspection, scarfing or grinding, and storage. Plants that integrate a continuous caster with a hot-rolling mill often need only 90 minutes to convert liquid steel into a hot-rolled product.
Some plants have been built specifically for direct rolling. One example is a thin-slab caster that casts strands 50 millimetres thick and 1,250 millimetres wide at speeds of about five metres per minute. After the strand is cut on the run-out table, the slabs are directly heated in-line in a long tunnel furnace or by induction coils and then fed, also in-line, directly into the finishing train of a hot-strip mill. With everything in one continuous line, operating and maintenance systems must be kept at the highest level.
Another special continuous process is the rotary casting of rounds, mainly for seamless tubes. A rotary caster is similar to a straight-mold vertical caster, except that the round mold, the strand, and the withdrawal system revolve at about 75 rotations per minute. This creates a centrifugal force within the strand and results in a cleaner cast and better contact between strand and mold. Still another variation is the casting of rounds in a horizontal caster. This entirely different system employs a large tundish with a horizontal nozzle in its side wall that extends directly into a water-cooled horizontal mold. The strand oscillates and is pulled out of the mold in small increments each time a new shell has formed at the mold entrance. Everything is located on one level, so that there are no high ladle lifts. Ferrostatic pressure in the strand is also very low, but segregation tendencies caused by gravitational forces require more careful preparation of the liquid steel.
There have been, and still are, many continuous-casting concepts tested in laboratories, pilot plants, and trial operations. Examples include single- or dual-roll strip casters, which cast strip directly from liquid steel, and belt casters for thin-slab production. There have also been hundreds of patents issued on continuous casting, all with the goal of making the process more cost-efficient, improving metallurgical control, and casting as close to the final product shape as possible.
For the manufacture of special products, refining and solidification processes are often combined.
Vacuum ingot pouring is often employed to produce very large ingots that are subsequently processed, in expensive forging and machining operations, into such products as rotors for power generators. In this process, an ingot mold is placed inside a cylindrical tank that is connected to vacuum pumps. The tank is closed by a lid, and a small, stopper-operated ladle having a capacity of about 25 tons of liquid steel is set on top of the lid. The nozzle of this so-called pony ladle is sealed by an aluminum disk, the tank is evacuated, and the furnace ladle starts pouring steel into the pony ladle. When the ferrostatic pressure reaches a certain point, the stopper is opened, the aluminum plate burns through, and the stream of liquid steel is degassed before it fills the mold for solidification. Pouring under vacuum lowers the hydrogen content, an important matter for large ingots.
In this process, employed for casting steels that contain easily oxidized alloying elements, a consumable electrode made of forged steel or of compacted powder or sponge is continuously melted by an arc under vacuum. At the same time, the shallow molten pool underneath the electrode is continuously solidified in a water-cooled, normally round copper mold. As the mold is filled, the electrode moves up. The melting current, in flowing between the electrode and the mold, passes through the arc, liquid pool, and solidified strand. Melting under high vacuum lowers the levels of dissolved oxygen, oxide inclusions, hydrogen, nitrogen, and elements having a high vapour pressure, such as lead, manganese, and tin. In addition, the shallow pool results in a directional solidification, with the crystals growing parallel to the axes of the ingot; this greatly improves the subsequent hot-forming operation. There is no segregation and no cavity. Ingots weighing up to 50 tons and measuring 1.5 metres in diameter have been cast with this method.
In this process, there is a slowly melting consumable electrode and a water-cooled mold for solidification, as in vacuum arc remelting, but the melting is conducted under normal atmosphere and is accomplished by a thick, superheated layer of slag on top of the shallow metal pool. This slag is resistance-heated by the high electrical current passing from the electrode to the mold, and it also desulfurizes the molten steel drops as they pass through on their way from the electrode to the liquid pool. Solidification patterns are similar to those in vacuum arc remelting. The ingot surface is very clean, owing to the presence of a slag layer between the ingot and mold, and does not need surface conditioning. Some electroslag installations cast ingots heavier than 200 tons.
Foundries that cast steel into commercial products mainly employ coreless induction furnaces or electric-arc furnaces for melting scrap. Scrap quality is normally high because a large portion of return scrap is used in the form of gates and risers left over from previous casting operations. Since it is often not necessary to refine scrap—that is, to lower the sulfur and phosphorus content—an acid process can be applied using a high-silica slag that may contain 60 percent silica, 10 percent lime, 10 percent manganese oxide, and 15 percent iron oxide. This permits the furnaces to run with a cheaper acid lining.
Tapping temperatures are usually higher than for ingot pouring or continuous casting in order to have a liquid steel with good fluidity that fills the thin parts of a casting. Molding is similar to that in gray-iron foundries, but a more heat-resistant mold material is necessary because of the higher temperatures. Solidifying steel castings normally show a higher linear shrinkage (1.5 percent) than gray iron castings, which shrink about 1 percent. Small parts are cast in greensand molds, but larger parts are made in stronger dry-sand molds.
Forming processes convert solidified steel into products useful for the fabricating and construction industries. The objectives are to obtain a desired shape, to improve cast steel’s physical properties (which are not suitable for most applications), and to produce a surface suitable for a specific use. During plastic forming, the large crystals in cast steel are converted into many small, long crystals, transforming the usually brittle cast into a ductile and tough steel. In order to accomplish this, it is often necessary to reduce the cross section of a cast structure to one-eighth or even less of its original.
The major forming processes are carried out hot, at about 1,200° C (2,200° F), because of steel’s low resistance to plastic deformation at this temperature. This requires the use of reheating furnaces of different designs. Cold forming is often applied as a secondary process for making special steel products such as sheet or wire.
There are a number of steel-forming processes—including forging, pressing, piercing, drawing, and extruding—but by far the most important one is rolling. In this process, the rolls, working always in pairs, are driven in opposite directions with the same peripheral velocity and are held at a specific distance from each other by heavy bearings and mill housings. The steel workpiece is pulled by friction into the roll gap, which is smaller than the cross section of the workpiece, so that both rolls exert a pressure and continuously form the piece until it leaves the roll gap with a smaller section and increased length. As shown in the figure, the reduction in cross section is calculated by subtracting the out-section (S2) from the in-section (S1) and then dividing by S1. Assuming the workpiece maintains its original volume as it is formed, the elongation (L2) divided by the original length (L1) equals S1 divided by S2. When rolling flat products, there is not much change in width, so that the thickness alone can be used to calculate reduction.
The basic principles of a rolling-mill design are shown in B in the figure. Two heavy bearings mounted on each side of a roll sit in chocks, which slide in a mill housing for adjusting the roll gap with a screw. The two housings are connected to each other and to the foundation, and the complete assembly is called a roll stand. There are also compact rolling units (C in the figure), which do not have housings; often used in the tandem rolling of long products, they can be exchanged quickly for repair or for a change in the rolling program. Rolls are driven through spindles and couplings, either directly or via a gear, by one or several electric motors. Depending on the product rolled, there are stands that have two, three, four, and more rolls; accordingly, they are given the names two-high, three-high, four-high, six-high, cluster mill, and planetary mill (schematically shown in the figure). For rolling strip, heavy backup rolls support the smaller work rolls, because thin rolls form flat material better than do large-diameter rolls.
In a rolling shop, stands are arranged according to three layout principles. One is called the open train (G in the figure), in which the stands are arranged side by side, often driven by the same motor and linked by spindles. This arrangement is applied only to the rolling of long products, with guides or cross-transfers being used to move the workpiece from stand to stand. A tandem mill arrangement (H in the figure) has one stand behind the other and is used for high-production rolling of almost all products. This continuous arrangement requires the construction of long rolling trains and buildings, but layouts can be shortened by a so-called semicontinuous mill, in which the workpiece is passed back and forth through a reversing mill before being sent through the rest of the line. When open-train and tandem arrangements are combined for rolling long products in more compact layouts, it is called a cross-country mill.
Cast ingots, sometimes still hot, arrive at slabbing and blooming mills on railroad cars and are charged upright by a special crane into under-floor soaking pits. These are gas-fired rectangular chambers, about 5 metres deep, in which four to eight ingots are simultaneously heated to about 1,250° C (2,300° F). An ingot used for conversion into a slab can be 1.5 metres wide, 0.8 metre thick, and 2.5 metres high and can weigh 23 tons. The soaking pits are highly computerized for scheduling, firing rates, heating times (which can last 8 to 18 hours), and rolling programs.
After heating, a tiltable transfer buggy brings a hot ingot to a two-high reversing mill, which takes one pass after another, reversing the rolls and roller table each time the ingot has passed through. Because each pass reduces the slab by only about 50 millimetres, it may take 21 passes, including several edge passes with the slab standing upright on its edges, to obtain a slab measuring 0.2 metre thick, 1.5 metres wide, and 10 metres long.
The rolls usually have a diameter of about 1.2 metres; each is driven by one or two electric motors totaling 7,000 to 12,000 horsepower. The two roller tables, situated in front and in back of the stand, have movable manipulators that guide the slab into the rolls and turn it onto its edges when required. High-pressure water nozzles remove surface scale, and a crop-shear discards the ends and cuts the slab into proper length. Some slabbing mills place a pair of heavy vertical rolls next to the horizontal rolls for edge rolling; this avoids the time-consuming turning of the slab into an upright position. Such an arrangement is called a universal mill.
For making long products, blooms some 250 millimetres square are rolled from ingots in a similar fashion on the same type of mill.
Rolled from heavy slabs supplied by a slabbing mill or continuous caster or sometimes rolled directly from an ingot, plates vary greatly in dimensions. The largest mills can roll plates 200 millimetres thick, 5 metres wide, and 35 metres long. These three dimensions are determined by the slab or ingot weight as well as the rolling-mill size. Sometimes only a few plates of the same dimensions and quality specifications are ordered.
Most mills have two continuous, broadside push-through or walk-through furnaces, which heat the slabs to about 1,250° C. Sometimes two batch-type furnaces are also used for heating odd-sized or extra-heavy slabs and ingots. Before rolling, high-pressure water jets descale the slabs. Most plate mills are four-high mills, as shown in C in the figure, and are supplemented by vertical edge rolls. The work rolls and backup rolls of large mills have diameters of 1.2 and 2.4 metres, respectively, and a roll face length up to 6 metres. Their maximum total rolling force is often 10,000 tons, and their rolls are driven by an 8,000-kilowatt motor. Most mills have hydraulic roll adjustment, which transmits the roll pressure to a computer; the computer uses this and other rolling parameters, such as temperature and thickness of the plate at all locations, to control the rolling process by a mathematical model. This technology—actually a computerized art—permits not only the rolling of huge workpieces with high accuracy (e.g., to a thickness tolerance of 0.2 millimetre) but also the control of rectangularity, flatness, plan-view shape, yield, physical properties, and profile. Several plants are even capable of rolling plates with a tapered or stepped thickness. Sometimes plants use two rolling mills, a roughing stand and a finishing stand, to improve surface quality and increase production. Most plate mills also have elaborate equipment for leveling, cooling, shearing or milling of edges, heat-treating, and marking.
The rolling of hot strip begins with a slab, which is inspected and, if necessary, surface cleaned either manually or by scarfing machines with oxyacetylene torches. The slabs are then pushed, or walked on their broadside, through gas-fired furnaces that have a hearth dimension of about 13 metres by 30 metres. In a pusher-type furnace, the slabs slide on water-cooled skids, and, each time a new slab is charged, a heated slab drops through a discharge door onto a roller table. In walking-beam furnaces, several walking beams lift the workpieces from the hearth, move them forward, and set them back down in a series of rectangular movements. These furnaces have the advantage of producing no cold stripes and skid marks across the slabs. Preheating temperature, as with slabs and plates, is about 1,250° C.
A heated slab moves first through a scale breaker, which is a two-high rolling mill with vertical rolls that loosens the furnace scale and removes it with high-pressure water jets. Then the slab passes through four-high roughing stands, typically four arranged in tandem, which roll it to a thickness of about 30 millimetres. The stands are spaced about 30 to 70 metres apart, so that the slab is only in one roll gap at a time. After roughing, it proceeds to a long (about 140 metres) roller table in front of the finishing train for cooling, when required for metallurgical reasons. As the slab enters the finishing train (at about 20 metres per minute), a crop-shear cuts the head and tail, and high-pressure steam jets remove the secondary scale formed during rolling. Six or seven four-high finishing stands then roll the strip to its final thickness of 1.5 to 10 millimetres.
Finishing stands are arranged in tandem, only five to six metres apart and close-coupled, so that the strip is in all rolls at the same time. For process control, a computer receives continuous information from on-line sensors, measuring such parameters as thickness, temperature, tension, width, speed, and shape of the strip, as well as roll pressure, torque, and electrical load. Reduction is high in the first stands (e.g., 45 percent) and low in the last stand (e.g., 10 percent) to ensure good surface and flatness of the strip, which leaves the last finishing stand at 600 to 1,200 metres per minute and 820° to 950° C (1,510° to 1,750° F). The strip is water-cooled on a 150-metre-long run-out table and coiled at high speed at 520° to 720° C (970° to 1,325° F). Mills have at least two coilers to ensure 100 percent availability.
All the equipment in a hot-strip mill is arranged in a straight line of about 600 metres from furnace to coiler, with the slab or strip passing only once through each stand. Total installed power of only the heavy rolling-mill motors can exceed 125,000 horsepower.
Controlling rolling and coiling temperatures is essential for metallurgical reasons, because it greatly influences the physical properties of both hot-rolled and cold-rolled strip. Also, a number of systems are in use to improve dimensional control of the strip. In order to guide the strip through the flat rolls of a tandem mill, it is made thicker in the centre (by about 0.1 millimetre) than at the edges. This so-called crown, as well as the strip’s entire profile, is often controlled by roll bending, accomplished by hydraulic cylinders and extra-long bearings on each side of the extended roll neck. Another system, which improves the wear pattern and service time of the work rolls, is roll shifting—i.e., a sideward adjustment of the rolls along their axes. Normally, the rolling program of a hot-strip mill is influenced by roll wear. Since the heaviest roll wear takes place at the colder edges of the strip, it is common to roll wide strips first and narrow strips later. Roll shifting permits so-called schedule-free rolling—i.e., strip of any width can be rolled at any time. It also is used for controlling the strip profile.
Many highly mechanized hot-strip mills have a capacity of three million to five million tons per year, and as much as 60 percent of the raw steel produced in industrial countries is rolled on these mills. There are, however, hot-strip mills designed for smaller production. For example, a semi-continuous hot-strip mill has only one reversing rougher in front of the finishing train. Another rolling system goes even farther and uses one four-high reversing rougher and one four-high reversing finishing mill, with hot-coiling boxes in front and in back of the finishing mill. (Hot coilers operate in a furnace to keep the strip hot.) In addition, there are planetary-type hot-strip mills, which have a cage of approximately 20 small rolls around each of two backup rolls (see F in the figure). The small rolls, in turning around the big roll, make a small reduction every time they pass over the wedge-shaped portion of the workpiece in the roll gap. Planetary mills can reduce a slab from 25 to 2.5 millimetres in one pass—although at a slow rate.
The rolling of cold strip begins with the retrieval of hot-rolled strip from a coil storage yard, which often uses fully automated cranes for setting and retrieving coils according to rolling schedules. The coils are first descaled in continuous pickle lines, which are discussed below (see Treating of steel: Surface treating: Pickling). The cleaned and oiled coils are fed into a cold-reduction mill, which is usually a tandem mill of four to six four-high stands with an uncoiling reel at the entry and a recoiling reel at the exit. When rolling from, for example, 2 millimetres to 0.3 millimetre, the cold reduction is usually 35 percent on the first stands and 15 percent on the final stand. The exit speed is normally high, often 100 kilometres (60 miles) per hour, in order to achieve proper production rates with such small cross sections. Since the strip temperature may go as high as 200° C (390° F), proper cooling of strip and rolls is essential. Heavy-duty lubricants are also used to minimize friction in the roll gap.
Typically, the work rolls have a diameter of a half-metre, and the backup rolls of 1.2 metres. For wide strip, the roll face can be 2.4 metres long. The work rolls are precision ground with a specific crown to compensate for roll bending. The last stand usually takes only a small reduction to improve control over the final thickness, profile, and flatness of the strip. To improve control further, many shops use hydraulic roll bending, or they use a differential cooling of the rolls to change their shape by thermal expansion. For additional shape control, a number of shops employ a six-high mill (D in the figure) as the last stand, shifting the work rolls and intermediate rolls along their axes during rolling. This provides continuous shape control, because the rolls are ground to a specific profile. All these systems, together with the high speed of rolling, make cold-reduction mills highly complex to operate and controllable only by computer.
Usually, cold-rolled strip cannot be used as rolled, because it is too hard and has low ductility. Therefore, it is annealed in batch or continuous annealing plants (see below Treating of steel: Heat-tresting: Annealing). After annealing, the strip is cold-rolled to about a 3-percent reduction on a temper mill to improve its physical properties. (Temper mills are dry, four-high reversing mills that are similar to cold-reduction mills but less powerful.) This rolling operation also gives the strips their final surface finish, an important characteristic and often specified by the customer. If required, shearing lines cut the coils into sheets.
Several plants integrate some or all of the operating steps of a cold-rolling shop into a continuous operation, moving an endless strip (welded together at the pickler or cold mill) through the processes without coiling and coil storage. Indeed, some plants move one continuous strip from the pickle line to the temper-mill exit, with cold-rolling and annealing in between. One of these continuous lines can take less than two hours to convert a hot-rolled coil into a shippable cold-rolled product—a great operating advantage that requires, however, excellent computer control at all levels and perfect maintenance to provide the needed reliability for the completely linked-up equipment. With direct charging of a hot-strip mill from a continuous caster, it is possible to have liquid steel in shippable form five hours after it has been tapped at the furnace.
Billets are the feedstock for long products of small cross section. In cases when they are not directly cast by a continuous caster, they are rolled from blooms by billet mills. One method of rolling billets, which are usually 75 to 125 millimetres square, is to use a three-high mill with box passes, as shown in A in the figure. After a rectangular bloom is rolled into a square cross section at the lower rolls, it is lifted to the next pass on the upper rolls and rolled back into a rectangular one; this is turned 90° while being lowered on a roller table for another square rolling in the lower pass, and so on. In another method, alternating horizontal and vertical stands are arranged in tandem, using diamond and square passes without turning or twisting the billet (as shown in B in the figure).
Bars are long products, usually of round, square, rectangular, or hexagonal cross section and of 12- to 50-millimetre diameter or equivalent. (Since bar mills are also capable of rolling small shaped products such as angles, flats, channels, fence posts, and tees, these products are sometimes called merchant bars.) In rolling bars, a billet measuring, for instance, 120 millimetres square and five metres long is heated in pusher or walking-beam furnaces to 1,200° C. Τhere is a great variety of layouts used in bar-rolling mills. In principle, after removal of the furnace scale by water jets, a primary reduction takes place in several passes through roll stands in open, semicontinuous, or fully continuous arrangement. These can use an alternating square-diamond rolling principle on horizontal and vertical rolls, as shown in B in the figure, or a series of oval-to-round passes, as illustrated in C in the figure.
Guiding the strands properly from roll gap to roll gap is an important part of this rolling technology. When using only horizontal rolls, the guides also twist the bar 90° between diamond and square passes. In a continuous arrangement of close-coupled mills—in which several roll pairs or roll sets are installed a short distance from one another and all are driven through gears by one or two motors—bars are allowed to buckle in a controlled vertical loop in order to maintain a low tension in a bar between the stands. When using an open-train arrangement, a U-shaped trough called the repeater guides and threads the strand, as indicated in G in the figure. This generates a horizontal loop, caused by the entry speed of each receiving stand being slower than the exit speed of the delivering stands.
The finishing stand of a bar mill gives the bar its final shape and often a specific surface pattern, such as the protrusions on concrete-reinforcing bars. The rolling speed increases as the cross section at each successive stand decreases, and the exit speed can be as high as 15 metres per second. The hot bar is then cut by a flying shear into cooling-bed length (e.g., 50 metres), after which it is cooled, inspected, and cold-cut to shipping length.
Rod mills are similar to bar mills at the front end, but the finishing end is different. Rods have a smaller section (5.5 to 15 millimetres in diameter) and are always coiled, while bars are normally shipped in cut length. The final rolling in rod mills often takes place in a close-coupled set of 10 pairs of small rolls (200 and 150 millimetres in diameter); these are all installed in a block, with their axes at a 45° angle and arranged in an alternating fashion like the vertical and horizontal rolls in a continuous bar mill. Exit speed of small-diameter rods can go up to 100 metres per second. The rod is immediately coiled by quickly rotating laying heads and cooled before bundling. For enhanced production, two strands are often rolled simultaneously. Such high-speed operation requires cooling of the rod and almost every rolling-mill component. The cooling condition of the bars and rods is also carefully controlled to meet metallurgical specifications.
Computers are used for designing roll passes and for scheduling and controlling the complex operations. Bar and rod mills produce 150,000 to 750,000 tons per year. The largest mills are housed in buildings up to 600 metres long. The most space-consuming part of these manufacturing facilities (and the source of most bottlenecks) is the finishing and shipping area, which handles the many different lightweight shapes that are produced in various steel grades, heat treatments, and surface conditions and are made to many specific customer orders.
These are long products with irregular cross sections, such as beams, channels, angles, and rails. Rolling starts with blooms that may be 150 millimetres by 200 millimetres by 5 metres long. The blooms are received, either cold or hot, directly from the blooming mill or continuous caster. They are charged into a pusher or walking-beam continuous furnace and heated for up to three hours to 1,200° C. (Sometimes, three batch-type furnaces are used instead.)
Most shapes are formed by grooved rolls with mating projections that form together a window in their gap. This window becomes progressively smaller and more like the desired shape, pass after pass, until at the end, in the final pass, the specified cross section is obtained. D in the figure shows only 5 progressive passes out of about 11 in the rolling of a rail. Rolling shapes usually takes a total of 9 to 15 passes, with an area reduction of about 25 percent at the initial passes and only 7 percent at the last pass.
Roll and pass design is critical for this rolling technology. There are usually three to five stands arranged in various ways, each taking one to five passes. Only one pass is made through the finishing stand, which controls the final dimension and surface. Sometimes two-high reversing mills are used at the beginning in a fashion similar to blooming mills, with manipulators on run-out roller tables. In other cases, two or three three-high, nonreversing stands are arranged as an open train; in this arrangement, lifting roller tables move the workpiece between the upper and lower pass lines, and the workpiece is in only one roll gap at a time. Mills that produce medium and small shapes often have stands in tandem arrangement, rolling one workpiece simultaneously in several stands and using a controlled loop between stands. Wide-flange I-beams and H-pilings are usually rolled on universal mills using vertical edgers, as indicated in E in the figure. Blooms with a dog-bone cross section are often supplied to these structural-shape mills by beam-blank continuous casters.
Rolling temperatures are carefully controlled for metallurgical reasons. Heavy-walled, wide-flange I-beams are sometimes heat-treated in-line by computer-controlled water quenching and by tempering with their own retained heat. The heads of rails are often heat-treated in-line to improve wear and impact resistance. Rails are also slow-cooled under an insulated cover, directly after rolling, for at least 10 hours to diffuse hydrogen out of the steel.
After rolling, a hot saw cuts the shapes into lengths that can be handled by the cooling bed. Each shop conducts large-size finishing operations such as straightening, cold-cutting to ordered length, marking, and inspection.
Tubular products are manufactured according to two basic technologies. One is the welding of tubes from strip, and the other is the production of seamless tube from rounds or blooms.
The most widely used welding system, the electric-resistance welding (ERW) line, starts with a descaled hot-rolled strip that is first slit into coils of a specific width to fit a desired tube diameter. In the entry section is an uncoiler, a welder that joins the ends of coils for continuous operation, and a looping pit, which permits constant welding rates of, typically, three metres per minute. Several consecutive forming rolls then shape the strip into a tube with a longitudinal seam on top, as shown schematically in A in the figure. Two squeeze rolls press the seam together, while two electrode rolls or sliding contacts feed the electric power to the seam for resistance heating and welding. A cutting tool removes the flash created during welding, and, after a preliminary inspection, the tube is cut into cooling-bed length by a saw that moves with the tube.
Tubes up to 500 millimetres in diameter with walls 10 millimetres thick are produced on ERW lines. Larger-diameter pipes are often produced by forming the strip into an endless spiral, as shown schematically in B in the figure. Forming is followed by continuous welding of the seam, often by automatic arc welding. Pipes up to 1.5 metres in diameter and with a 12-millimetre wall thickness are sometimes produced by this spiral welding process. Still larger pipes are produced from plates by a U-ing and O-ing process, which applies heavy presses to form plates into a U and then an O. The longitudinal seam (or seams) are then welded by automatic arc-welding equipment.
Seamless tube rolling always begins by piercing a round or bloom to generate a hollow. In roll piercing, an oval round is preheated to about 1,200° C and is cross-rolled slowly between two short, large-diameter rolls that rotate in the same direction (shown schematically in C in the figure). The round also revolves and is pulled into the roll gap in a spiraling motion, because the rolls have a converging-diverging shape and are installed relative to each other at an angle of about 20°. This revolving, continuous plastic working of an oval cross section between the two rolls creates tensile stresses in the long axes of the oval, which rupture the centre and create a cavity. At this point the cavity meets the piercer, which is a projectile-shaped rotating cone held in place by a bar and a thrust bearing. The piercer acts like a third roll in the centre and produces the inside of the tube.
The cross or helical rolling action of roll piercing demands excellent hot formability of the prerolled round. Another process, push piercing, does not have such exacting requirements. This usually takes continuously cast square blooms and forms them into hollow rounds by the action of a heavy hydraulic pusher, which pushes them into the gap of two large-diameter contoured rolls that form together a circular pass line. In the roll gap the bloom is met by a heavy piercer, which forms the hollow, as shown in D in the figure. This mill can form a 250-millimetre-square, 3-metre-long bloom into a tube with an outside diameter of 300 millimetres and an inside diameter of 150 millimetres. Since there are only compression forces acting on the steel in this process, the workpiece is practically not elongated at all.
A number of rolling technologies are used to form the pierced hollows into tubes with specific dimensions and tolerances. Often, the hollow is reheated and then sent through another cross-roll piercer mill, called the elongator; this reduces the wall thickness by 30 to 60 percent. In a subsequent step, a long, preheated, lubricated cylinder called a mandrel may be inserted into the tube. The tube would then be rolled, with the mandrel inside, in a continuous close-coupled, seven-stand, two-high mill, usually with the rolls arranged at a 45° angle and in an alternating pattern like the horizontal and vertical rolls. A very uniform wall thickness can be formed by this process. Smaller diameter tubes are often formed from larger tubes in a continuous three-roll, close-coupled stretch-reduction mill (E in the figure). These mills sometimes have 20 sets of rolls arranged in tandem.
Heavy ingots, some weighing up to 300 tons, are sometimes formed at steel plants by huge hydraulic presses with a forging force of up to 10,000 tons. These make such large products as rotors for power-generating units or large sleeves for rolls or pressure vessels. Careful, uniform heating of the ingots to forging temperature may take 60 hours, and, before completion of the forging process, the workpiece may be reheated six times. The forging is accomplished by flat-, vee-, or swage-shaped dies, depending on the shape of the final product. Saddles and mandrels are used for forging rings and sleeves. The workpiece is connected to a long bar, which helps to move and turn it by a crane or manipulator. Large heat-treating furnaces are available in these forging shops to improve microstructure and to release internal stresses caused by the forging operation.
The cold drawing of wire is an important and special sector of steelmaking. It produces wire in hundreds of sizes and shapes and within a spectrum of physical properties unmatched by other steel products. Wire is also produced with many types of surface finish.
In principle, heat-treating already takes place when steel is hot-rolled at a particular temperature and cooled afterward at a certain rate, but there are also many heat-treating process facilities specifically designed to produce particular microstructures and properties. The simplest heat-treating process is normalizing. This consists of holding steel for a short time at a temperature 20° to 40° C above the G-S-K line (shown in the iron-carbon diagram in the figure) and then cooling it afterward in still air. Holding the steel in the gamma zone transforms the as-rolled or as-cast microstructure into austenite, which dissolves carbides. Then, during cooling, a very uniform grain is formed, consisting of either pearlite and ferrite or pearlite and cementite, depending on carbon content.
In all heat-treatment operations, the temperatures, holding times, and heating and cooling rates are varied according to the chemical composition, size, and shape of the steel. In general, alloy steels, which have a lower heat conductivity than carbon steels, are heated more slowly to avoid internal stresses.
To make steel ductile for subsequent forming operations, an annealing treatment is applied. In annealing, the steel is usually held for several hours at several degrees below Ar1 (shown by the P-S-K line in the figure) and then slowly cooled. This precipitates and coagulates the carbides and results in large ferrite crystals. Cold-formed steel is usually annealed and recrystallized in this manner, holding it for several hours at about 680° C (1,260° F).
Annealing is performed in an inert or reducing atmosphere to prevent any oxidation of the steel surface. In batch annealing of cold-rolled strip, for example, several coils are set on a base and on top of one another. Then they are covered with a shell made of heat-resistant steel, which is sealed on the bottom and holds the inert gas during annealing. A gas-fired bell furnace is then lowered by a crane over this cover for heating. The total processing time, including cooling, may be 50 to 120 hours, depending on furnace load and steel grade.
In a different system, the cold-rolled strip is pulled through an 80-metre-high furnace with the strip moving up and down between many top and bottom rolls. These continuous-annealing furnaces are usually heated by gas-fired radiation tubes in order to separate combustion gases from the inert atmosphere surrounding the strip. In this dynamic annealing process, the strip is heated to higher temperatures (for example, 780° C, or 1,440° F), held for only a few seconds, and immediately cooled by fast-circulating inert gas. The entry and exit sections of continuous-annealing lines are built, as on other strip-processing lines, to allow an uninterrupted and constant travel (at, say, 500 metres per minute) of the strip through the process section—in this case, the heating and cooling zones. The entry group has two uncoiling reels, a cross-shear, welding equipment for joining two strips, and a strip accumulator. The latter is often a looping tower, which supplies the process section above with strip at constant speed while welding is done at the entry section. The exit group works in a similar fashion, with a looping tower and two reels; it also cuts samples and substandard portions out of the strip.
Continuous-annealing lines are often 200 metres long, and the strip between uncoiler and recoiler is more than one kilometre in length. Strip annealed this way is not as soft as batch-annealed steel—a disadvantage compensated for by using ultralow-carbon steels—but it does have operating advantages in that annealing of one coil may take only one hour and the mechanical and surface properties of the strip are very uniform.
The most common heat treatment for plates, tubular products, and rails is the quench-and-temper process. Large plates are heated in roller-type or walking-beam furnaces, quenched in special chambers, and then tempered in a separate low-temperature furnace. Uniform heating and quenching is crucial; otherwise, residual stresses will distort and warp the plate. Tubes made for very demanding services, such as oil drilling, are usually heat-treated in walking-beam furnaces and special quench-and-temper systems.
The heads of rails are sometimes heat-treated in-line by induction heating coils, air quenching, and tempering by a controlled use of the heat retained in the rail after quenching. Heavy-walled structural shapes are sometimes water-quenched directly after the last pass at the rolling mill and also tempered by the heat retained in the steel. In-line heat-treating results in cost savings because it eliminates extra heat-treating processes and facilities.
The quenching media and the type of agitation during quenching are carefully selected to obtain specified physical properties with minimum internal stresses and distortions. Oil is the mildest medium, and salt brine has the strongest quenching effect; water is between the two. In special cases, steel is cooled and held for some time in a molten salt bath, which is kept at a temperature either just above or just below the temperature where martensite begins to form. These two heat treatments are called martempering and austempering, and both result in even less distortion of the metal.
The surface treatment of steel also begins during hot-rolling, because reheating conditions, in-line scale removal, rolling temperature, and cooling rate all determine the type and thickness of scale formed on the product, and this affects atmospheric corrosion, paintability, and subsequent scale-removal operations. Sometimes the final pass in hot-rolling generates specific surface patterns—for example, the protrusions on reinforcing bars or floor plates—and in cold-rolling a specific surface roughness is rolled into the strip at the temper mill to improve the deep-drawing operation and to assure a good surface finish on the final product—for instance, on the roof of an automobile.
Before cold forming, hot-rolled steel is always descaled, most commonly in an operation known as pickling. Scale consists of thin layers of iron oxide crystals, of which the chemical compositions, structures, and densities vary according to the temperature, oxidizing conditions, and steel properties that are present during their formation. These crystals can be dissolved by acids; normally, hot hydrochloric or sulfuric acid is used, but for some alloy steels a different acid, such as nitric acid, is needed. In addition, inhibitors are added to the acid to protect the steel from being dissolved as well.
The pickling of hot-rolled strip is carried out in continuous pickle lines, which are sometimes 300 metres long. The strip is pulled through three to five consecutive pickling tanks, each one 25 to 30 metres long, at a constant speed of about 300 metres per minute. Like other continuous strip-processing lines, pickle lines also have an entry and exit group to establish constant pickling conditions. After the last acid tank, there are sections that rinse, neutralize, dry, inspect, and oil the strip.
Long products, such as bars and wire rods, are normally pickled in batch operations by placing them on racks and immersing them in long, acid-containing vats. Sometimes shotblasting is used instead of pickling; this removes scale from heavy hot-rolled products by directing high-velocity abrasives onto the surface of the steel.
The removal of organic substances and other residues from the surface of steel, in particular after cold forming with lubricants, is carried out either in special cleaning lines or in the cleaning sections of another processing line. Hot solutions of caustic soda, phosphates, or alkaline silicates are used. The strip is often moved through several sets of electrodes, which, submerged in the cleaning liquid, electrolytically generate hydrogen gas at the steel surface for lifting residues off the strip.
Approximately one-third of the steel shipped by the industry is coated on its surface by a metallic, inorganic, or organic coating. By far the largest installations are operated for coating cold-rolled strip. In this group the most widely used are those which coat the steel with zinc, zinc alloys, or aluminum.
In hot-dip galvanizing lines, which also have the usual entry and exit groups, the strip moves first at constant speed—say, 150 metres per minute—through a cleaning section and a long, horizontal, nonoxidizing preheating furnace. (When hard strips are coated directly after cold reduction, this furnace is also used for annealing.) The hot strip, still protected by the inert furnace atmosphere in a long steel channel, enters the zinc bath at a temperature of approximately 480° C (900° F), supplying heat to the zinc bath, which is at about 440° C (825° F). The liquid zinc is contained in a refractory-lined, induction-heated vessel called the zinc pot (shown schematically in A in the figure). When it contacts the strip surface, the liquid zinc alloys with the iron and forms a strong metallurgical bond. However, the iron-zinc alloy is brittle, so that the coating, if too thick, will crack during forming of the sheet. For this reason, about 0.1 to 0.25 percent aluminum is added to the zinc, inhibiting iron-zinc formation and keeping the alloy layer to less than 15 percent of the total coating thickness. Excess liquid zinc is wiped off each side of the strip by two gas-knives, which have long, slotlike orifices through which high-pressure gas is blown. Coating thickness is controlled by adjusting the gas pressure and the location of the knives. Common coating weights are 180 or 275 grams of zinc per square metre of sheet, counting both surfaces. Sometimes, a heavy coating is produced on one side and a lighter coating on the other; this is called a differential coating. The total length of hot-dip galvanizing lines, including furnaces and cooling zones, sometimes reaches 400 metres. The entire system is computer-controlled, based on the continuous, in-line measuring of the coating weight.
There are several variations of the basic galvanizing process. The galvanneal process heats the strip above the zinc pot right after coating, using induction coils or gas-fired burners to create a controlled, heavy iron-zinc layer for improved weldability, abrasion resistance, and paintability of the product. Several processes use a zinc-aluminum alloy, and some lines have a second pot filled with liquid aluminum for aluminum coating. The pots are often quickly exchangeable.
Electrolytic galvanizing lines have similar entry and exit sections, but they deposit zinc in as many as 20 consecutive electrolytic coating cells. Of the several successful cell designs, the simple vertical cell (B in the figure) is discussed here to explain the principle. The strip, connected to the negative side of a direct current through large-diameter conductor rolls located above and between two cells, is dipped into a tank of electrolyte by a submerged sink roll. Partially submerged anodes, opposing the strip, are connected to the positive side of the electric current by heavy bus bars. Zinc cations (i.e., positively charged zinc atoms) present in the electrolyte are converted by the current into regular zinc atoms, which deposit on the strip. The bath is supplied with zinc cations either by zinc anodes, which are continuously dissolved by the direct current, or by zinc compounds continuously added to the electrolyte. In the latter case the anodes are made of insoluble materials, such as titanium coated with iridium oxide. The electrolyte is an acidic solution of zinc sulfide or zinc chloride with other bath additions to improve the quality of the coating and the current efficiency. Coating thickness is easier to control here than in the hot-dip process because of the good relationship between electrical current and deposited zinc. Theoretically, 1.22 kilograms of zinc are formed when applying a current of 1,000 amperes over one hour; this means that a line with an installed electrical capacity of one million amperes can deposit 1.22 tons of zinc per hour. The control parameters of such a line are mainly the current density between anodes and strip, the line voltage, the chemical composition and temperature of the electrolyte, and the line speed.
Electrolytic lines normally produce lower coating weights (15 to 60 grams per square metre) than do hot-dip lines, and they can also easily supply differential coatings and one-sided coatings for specific applications. Many lines can deposit zinc-alloy coatings, such as zinc-nickel or zinc-iron, and some lines are capable of producing multilayered coatings of different alloys, the goal being to optimize a combination of specific requirements such as corrosion resistance, weldability, abrasion resistance, drawability, and paintability. The processing speed of electrolytic galvanizing lines can often reach 180 metres per minute.
Electrolytic tinning lines for the production of tinplate are, in principle, of similar design, except that all rolls are smaller (because the strip is thinner and narrower), the line speed is faster (e.g., 700 metres per minute), and different electrolytes and anodes are used. Electrolytic coating lines also coat strips with chromium and other metals and alloys. Most of these lines have a shear line installed at the end to produce cut-to-length sheets upon request.
Many long products are also surface coated. Wires, for example, are often hot-dip galvanized in continuous multistrand lines. In addition, electrolytic coating of wire with all types of metal is often done by hanging coils from current-carrying C-hooks or bars into long vats, which have anodes installed and are filled with electrolyte. Many tubular products and reinforcing bars are coated with organic material to inhibit corrosion.
The steel industry has grown from ancient times, when a few men may have operated, periodically, a small furnace producing 10 kilograms, to the modern integrated iron- and steelworks, with annual steel production of about 1 million tons. The largest commercial steelmaking enterprise, Nippon Steel in Japan, was responsible for producing 26 million tons in 1987, and 11 other companies generally distributed throughout the world each had outputs of more than 10 million tons. Excluding the Eastern-bloc countries, for which employment data are not available, some 1.7 million people were employed in 1987 in producing 430 million tons of steel. That is equivalent to about 250 tons of steel per person employed per year—a remarkably efficient use of human endeavour.
Iron production began in Anatolia about 2000 BC, and the Iron Age was well established by 1000 BC. The technology of iron making then spread widely; by 500 BC it had reached the western limits of Europe, and by 400 BC it had reached China. Iron ores are widely distributed, and the other raw material, charcoal, was readily available. The iron was produced in small shaft furnaces as solid lumps, called blooms, and these were then hot forged into bars of wrought iron, a malleable material containing bits of slag and charcoal.
The carbon contents of the early irons ranged from very low (0.07 percent) to high (0.8 percent), the latter constituting a genuine steel. When the carbon content of steel is above 0.3 percent, the material will become very hard and brittle if it is quenched in water from a temperature of about 850° to 900° C (1,550° to 1,650° F). The brittleness can be decreased by reheating the steel within the range of 350° to 500° C (660° to 930° F), in a process known as tempering. This type of heat treatment was known to the Egyptians by 900 BC, as can be judged by the microstructure of remaining artifacts, and formed the basis of a steel industry for producing a material that was ideally suited to the fabrication of swords and knives.
The Chinese made a rapid transition from the production of low-carbon iron to high-carbon cast iron, and there is evidence that they could produce heat-treated steel during the early Han dynasty (206 BC–AD 25). The Japanese acquired the art of metalworking from the Chinese, but there is little evidence of a specifically Japanese steel industry until a much later date.
The Romans, who have never been looked upon as innovators but more as organizers, helped to spread the knowledge of iron making, so that the output of wrought iron in the Roman world greatly increased. With the decline of Roman influence, iron making continued much as before in Europe, and there is little evidence of any change for many centuries in the rest of the world. However, by the beginning of the 15th century, waterpower was used to blow air into bloomery furnaces; as a consequence, the temperature in the furnace increased to above 1,200° C (2,200° F), so that, instead of forming a solid bloom of iron, a liquid was produced rich in carbon—i.e., cast iron. In order to make this into wrought iron by reducing the carbon content, solidified cast iron was passed through a finery, where it was melted in an oxidizing atmosphere with charcoal as the fuel. This removed the carbon to give a semisolid bloom, which, after cooling, was hammered into shape.
In order to convert wrought iron into steel—that is, increase the carbon content—a carburization process was used. Iron billets were heated with charcoal in sealed clay pots that were placed in large bottle-shaped kilns holding about 10 to 14 tons of metal and about 2 tons of charcoal. When the kiln was heated, carbon from the charcoal diffused into the iron. In an attempt to achieve homogeneity, the initial product was removed from the kiln, forged, and again reheated with charcoal in the kiln. During the reheating process, carbon monoxide gas was formed internally at the nonmetallic inclusions; as a result, blisters formed on the steel surface—hence the term blister steel to describe the product. This process spread widely throughout Europe, where the best blister steel was made with Swedish wrought iron. One common steel product was weapons. To make a good sword, the carburizing, hammering, and carburizing processes had to be repeated about 20 times before the steel was finally quenched and tempered and made ready for service. Thus, the material was not cheap.
About the beginning of the 18th century, coke produced from coal began to replace charcoal as the fuel for the blast furnace; as a result, cast iron became cheaper and even more widely used as an engineering material. The Industrial Revolution then led to an increased demand for wrought iron, which was the only material available in sufficient quantity that could be used for carrying loads in tension. One major problem was the fact that wrought iron was produced in small batches. This was solved about the end of the 18th century by the puddling process, which converted the readily available blast-furnace iron into wrought iron. In Britain by 1860 there were 3,400 puddling furnaces producing a total of 1.6 million tons per year—about half the world’s production of wrought iron. Only about 60,000 tons were converted into blister steel in Britain; annual world production of blister steel at this time was about 95,000 tons. Blister steel continued to be made on a small scale into the 20th century, the last heat taking place at Newcastle, Eng., in 1951.
A major development occurred in 1751, when Benjamin Huntsman established a steelworks at Sheffield, Eng., where the steel was made by melting blister steel in clay crucibles at a temperature of 1,500° to 1,600° C (2,700° to 2,900° F), using coke as a fuel. Originally, the charge in the crucible weighed about 6 kilograms, but by 1870 it had increased to 30 kilograms, which, with a crucible weight of 10 kilograms, was the maximum a man could be expected to lift from a hot furnace. The liquid metal was cast to give an ingot about 75 millimetres in square section and 500 millimetres long, but multiple casts were also made. Sheffield became the centre of crucible steel production; in 1873, the peak year, output was 110,000 tons—about half the world’s production. The crucible process spread to Sweden and France following the end of the Napoleonic Wars and then to Germany, where it was associated with Alfred Krupp’s works in Essen. A small crucible steelworks was started in Tokyo in 1895, and crucible steel was produced in Pittsburgh, Pa., U.S., from 1860, using a charge of wrought iron and pig iron.
The crucible process allowed alloy steels to be produced for the first time, since alloying elements could be added to the molten metal in the crucible, but it went into decline from the early 20th century, as electric-arc furnaces became more widely used. It is believed that the last crucible furnace in Sheffield was operated until 1968.
Bulk steel production was made possible by Henry Bessemer in 1855, when he obtained British patents for a pneumatic steelmaking process. (A similar process is said to have been used in the United States by William Kelly in 1851, but it was not patented until 1857.) Bessemer used a pear-shaped vessel lined with ganister, a refractory material containing silica, into which air was blown from the bottom through a charge of molten pig iron. Bessemer realized that the subsequent oxidation of the silicon and carbon in the iron would release heat and that, if a large enough vessel were used, the heat generated would more than offset the heat lost. A temperature of 1,650° C (3,000° F) could thus be obtained in a blowing time of 15 minutes with a charge weight of about half a ton.
One difficulty with Bessemer’s process was that it could convert only a pig iron low in phosphorus and sulfur. (These elements could have been removed by adding a basic flux such as lime, but the basic slag produced would have degraded the acidic refractory lining of Bessemer’s converter.) While there were good supplies of low-phosphorus iron ores (mostly hematite) in Britain and the United States, they were more expensive than phosphorus-rich ores. In 1878 Sidney Gilchrist Thomas and Percy Gilchrist developed a basic-lined converter in which calcined dolomite was the refractory material. This enabled a lime-rich slag to be used that would hold phosphorus and sulfur in solution. This “basic Bessemer” process was little used in Britain and the United States, but it enabled the phosphoric ores of Alsace and Lorraine to be used, and this provided the basis for the development of the Belgian, French, and German steel industries. World production of steel rose to about 50 million tons by 1900.
An alternative steelmaking process was developed in the 1860s by William and Friedrich Siemens in Britain and Pierre and Émile Martin in France. The open-hearth furnace was fired with air and fuel gas that were preheated by combustion gases to 800° C (1,450° F). A flame temperature of about 2,000° C (3,600° F) could be obtained, and this was sufficient to melt the charge. Refining—that is, removal of carbon, manganese, and silicon from the metal—was achieved by a reaction between the slag (to which iron ore was added) and the liquid metal in the hearth of the furnace. Initially, charges of 10 tons were made, but furnace capacity gradually increased to 100 tons and eventually to 300 tons. Initially an acid-lined furnace was used, but later a basic process was developed that enabled phosphorus and sulfur to be removed from the charge. A heat could be produced in 12 to 18 hours, sufficient time to analyze the material and adjust its composition before it was tapped from the furnace.
The great advantage of the open hearth was its flexibility: the charge could be all molten pig iron, all cold scrap, or any combination of the two. Thus, steel could be made away from a source of liquid iron. Up to 1950, 90 percent of steel in Britain and the United States was produced in the open-hearth process, and as recently as 1988 more than 96 million tons per year were produced in this way by Eastern-bloc countries.
The refining of steel in the conventional open-hearth furnace required time-consuming reactions between slag and metal. After World War II, tonnage oxygen became available, and many attempts were made to speed up the steelmaking process by blowing oxygen directly into the charge. The Linz-Donawitz (LD) process, developed in Austria in 1949, blew oxygen through a lance into the top of a pear-shaped vessel similar to a Bessemer converter. Since there was no cooling effect from inert nitrogen gas present in air, any heat not lost to the off-gas could be used to melt scrap added to the pig-iron charge. In addition, by adding lime to the charge, it was possible to produce a basic slag that would remove phosphorus and sulfur. With this process, which became known as the basic oxygen process (BOP), it was possible to produce 200 tons of steel from a charge consisting of up to 35 percent scrap in a tap-to-tap time of 60 minutes. The charges of a basic oxygen furnace have grown to 400 tons and, with a low-silicon charge, blowing times can be reduced to 15 to 20 minutes.
Shortly after the introduction of the LD process, a modification was developed that involved blowing burnt lime through the lance along with the oxygen. Known as the LD-AC (after the ARBED steel company of Luxembourg and the Centre National of Belgium) or the OLP (oxygen-lime powder) process, this led to the more effective refining of pig iron smelted from high-phosphorus European ores. A return to the original bottom-blown Bessemer concept was developed in Canada and Germany in the mid-1960s; this process used two concentric tuyeres with a hydrocarbon gas in the outer annulus and oxygen in the centre. Known originally by the German abbreviation OBM (for Oxygen bodenblasen Maxhuette, “oxygen bottom-blowing Maxhuette”), it became known in North America as the Q-BOP. Beginning about 1960, all oxygen steelmaking processes replaced the open-hearth and Bessemer processes on both sides of the Atlantic.
With the increasing sophistication of the electric power industry toward the end of the 19th century, it became possible to contemplate the use of electricity as an energy source in steelmaking. By 1900, small electric-arc furnaces capable of melting about one ton of steel were introduced. These were used primarily to make tool steels, thereby replacing crucible steelmaking. By 1920 furnace size had increased to a capacity of 30 tons. The electricity supply was three-phase 7.5 megavolt-amperes, with three graphite electrodes being fed through the roof and the arcs forming between the electrodes and the charge in the hearth. By 1950 furnace capacity had increased to 50 tons and electric power to 20 megavolt-amperes.
Although small arc furnaces were lined with acidic refractories, these were little more than melting units, since hardly any refining occurred. The larger furnaces were basic-lined, and a lime-rich slag was formed under which silicon, sulfur, and phosphorus could be removed from the melt. The furnace could be operated with a charge that was entirely scrap or a mixture of scrap and pig iron, and steel of excellent quality with sulfur and phosphorus contents as low as 0.01 percent could be produced. The basic electric-arc process was therefore ideally suited for producing low-alloy steels and by 1950 had almost completely replaced the basic open-hearth process in this capacity. At that time, electric-arc furnaces produced about 10 percent of all the steel manufactured (about 200 million tons worldwide), but, with the subsequent use of oxygen to speed up the basic arc process, basic electric-arc furnaces accounted for almost 30 percent of steel production by 1989. In that year, world steel production was 770 million tons.
With the need for improved properties in steels, an important development after World War II was the continuation of refining in the ladle after the steel had been tapped from the furnace. The initial developments, made during the period 1950–60, were to stir the liquid in the ladle by blowing a stream of argon through it. This had the effect of reducing variations in the temperature and composition of the metal, allowing solid oxide inclusions to rise to the surface and become incorporated in the slag, and removing dissolved gases such as hydrogen, oxygen, and nitrogen. Gas stirring alone, however, could not remove hydrogen to an acceptable level when casting large ingots. With the commercial availability after 1950 of large vacuum pumps, it became possible to place ladles in large evacuated chambers and then, by blowing argon as before, remove hydrogen to less than two parts per million. Between 1955 and 1965 a variety of improved degassing systems of this type were developed in Germany.
The oldest ladle addition treatment was the Perrin process developed in 1933 for removing sulfur. The steel was poured into a ladle already containing a liquid reducing slag, so that violent mixing occurred and sulfur was transferred from the metal to the slag. The process was expensive and not very efficient. In the postwar period, desulfurizing powders based on calcium, silicon, and magnesium were injected into the liquid steel in the ladle through a lance using an inert carrier gas. This method was pioneered in Japan to produce steels for gas and oil pipelines.
Alloying elements are added to steels in order to improve specific properties such as strength, wear, and corrosion resistance. Although theories of alloying have been developed, most commercial alloy steels have been developed by an experimental approach with occasional inspired guesses. The first experimental study of alloy additions to steel was made in 1820 by the Britons James Stodart and Michael Faraday, who added gold and silver to steel in an attempt to improve its corrosion resistance. The mixtures were not commercially feasible, but they initiated the idea of adding chromium to steel (see below Stainless steel).
The first commercial alloy steel is usually attributed to the Briton Robert F. Mushet, who in 1868 discovered that adding tungsten to steel greatly increased its hardness even after air cooling. This material formed the basis of the subsequent development of tool steels for the machining of metals.
About 1865 Mushet also discovered that the addition of manganese to Bessemer steel enabled the casting of ingots free of blowholes. He was also aware that manganese alleviated the brittleness induced by the presence of sulfur, but it was Robert Hadfield who developed (in 1882) a steel containing 12 to 14 percent manganese and 1 percent carbon that greatly improved wear resistance and was used for jaw crushers and railway crossover points.
The real driving force for alloy steel development was armaments. About 1889 a steel was produced with 0.3 percent carbon and 4 percent nickel; shortly thereafter it was further improved by the addition of chromium and became widely used for armour plate on battleships. In 1918 it was found that this steel could be made less brittle by the addition of molybdenum.
The general understanding of why or how alloying elements influenced the depth of hardening—the hardenability—came out of research conducted chiefly in the United States during the 1930s. An understanding of why properties changed on tempering came about in the period 1955–1965, following the use of the transmission electron microscope.
An important development immediately after World War II was the improvement of steel compositions for plates and sections that could readily be welded. The driving force for this work was the failure of plates on the Liberty ships mass-produced during the war by welding, a faster fabricating process than riveting. The improvements were effected by increasing the manganese content to 1.5 percent and keeping the carbon content below 0.25 percent.
A group of steels given the generic title high-strength low-alloy (HSLA) steels had the similar aim of improving the general properties of mild steels with small additions of alloying elements that would not greatly increase the cost. By 1962 the term microalloyed steel was introduced for mild-steel compositions to which 0.01 to 0.05 percent niobium had been added. Similar steels were also produced containing vanadium.
The period 1960–80 was one of considerable development of microalloyed steels. By linking alloying with control over temperature during rolling, yield strengths were raised to almost twice that of conventional mild steel.
It is not surprising that attempts should be made to improve the corrosion resistance of steel by the addition of alloying elements, but it is surprising that a commercially successful material was not produced until 1914. This was a composition of 0.4 percent carbon and 13 percent chromium, developed by Harry Brearley in Sheffield for producing cutlery.
Chromium was first identified as a chemical element about 1798 and was extracted as an iron-chromium-carbon alloy. This was the material used initially by Stodart and Faraday in 1820 in their experiments on alloying. The same material was used by John Woods and John Clark in 1872 to make an alloy containing 30 to 35 percent chromium; although it was noted as having improved corrosion resistance, the steel was never exploited. Success became possible when Hans Goldschmidt, working in Germany, discovered in 1895 how to make low-carbon ferrochromium.
The link between the carbon content of chromium steels and their corrosion resistance was established in Germany by Philip Monnartz in 1911. During the interwar period, it became clearly established that there had to be at least 8 percent chromium dissolved in the iron matrix (and not bound up with carbon in the form of carbides), so that on exposure to air a protective film of chromic oxide would form on the steel surface. In Brearley’s steel, 3.5 percent of the chromium was tied up with the carbon, but there was still sufficient remaining chromium to confer corrosion resistance.
The addition of nickel to stainless steel was patented in Germany in 1912, but the materials were not exploited until 1925, when a steel containing 18 percent chromium, 8 percent nickel, and 0.2 percent carbon came into use. This material was exploited by the chemical industry from 1929 onward and became known as the 18/8 austenitic grade.
By the late 1930s there was a growing awareness that the austenitic stainless steels were useful for service at elevated temperatures, and modified compositions were used for the early jet aircraft engines produced during World War II. The basic compositions from that period are still in use for high-temperature service. Duplex stainless steel was developed during the 1950s to meet the needs of the chemical industry for high strength linked to corrosion resistance and wear resistance. These alloys have a microstructure consisting of about half ferrite and half austenite and a composition of 25 percent chromium, 5 percent nickel, 3 percent copper, and 2 percent molybdenum.
The early metals shapers, the smiths, used hand tools to form iron into finished shapes. Essentially, these consisted of tongs for holding the metal on an anvil and a hammer for beating it. Converting an iron bloom into a wrought-iron bar required considerable hammering. Water-driven hammers were in use by the 15th century in Germany, but heavy hammers capable of dealing with 100-kilogram blooms came into use only in the 18th century. Slitting mills for making thin strips that were then fabricated into nails were introduced about that time, as were rolling mills for converting bars into flat plates. Grooved rolls for producing rods from puddled iron were patented by John Purnell in 1766; these were powered by a 35-horsepower waterwheel.
Steel-forming operations were on a relatively small scale until the introduction of the Bessemer process, in which large volumes of liquid steel were produced for the first time. The liquid metal was poured from ladles into large cast-iron ingot molds with an average size of 700 millimetres in square section and 1.5 to 2 metres in length. Such an ingot would weigh about seven tons. After solidifying, the ingot was stripped from the mold, reheated, and then reduced in size by hot-rolling in a primary (blooming) mill to give billets about 100 millimetres in section. The billets were sheared into 3- to 4-metre lengths, and these formed the starting material for rolling into bars, beams, rods, and strip.
This type of billet production persisted until the 1960s, when a profound change occurred with the development of continuous-casting machines. With liquid steel going directly from the furnace into the casting machine, there was no need to pour large ingots or to reheat them with heavy energy requirements. Nor were the very expensive blooming mills required for reducing the ingots to forms that were now produced directly by casting. Continuous casting was first used for nonferrous metals in the 1930s, and in the early 1950s experiments were undertaken with it at steel plants in Britain, the United States, and the U.S.S.R. The first production plant using continuous casting was operated at Barrow, Eng., by the United Steels Company. In 1965, 2 percent of total steel production was continuously cast; by 1970 this had risen to 5 percent, and, by 1990, 64 percent of all the steel produced in the world was continuously cast (in Japan it was more than 90 percent).
Continuous casting was partly responsible for a new type of steel plant that developed after 1970—the so-called mini-mill. There steel was made in an electric-arc furnace using an all-scrap charge and was then continuously cast into small-diameter billets for rolling into rods or drawing into wire. Mini-mills were built in industrial regions, where scrap arises, whereas the location of conventional steel plants remained linked to the availability of iron ore and low-cost energy.
With the development of the gas industry at the beginning of the 19th century, an increased demand developed for tubes to transmit gas. In 1824 a method for pressure butt-welding of heated, curved strip was developed in Britain, and in 1832 a plant for producing tubes was established in the United States. Similar processes are still being used to produce seamed tubing. An improvement on the hot-pressure butt-weld was developed in the United States about 1921, when the seam was joined by electric-resistance welding. Most seamed tubes are still produced this way, including the large-diameter tubes formed by spirally coiling a continuous strip and then arc welding the spiral seam.
Seamless tubing involved the piercing of a round billet; this process was developed in Britain in 1841. A greatly improved process was developed by the Mannesmann company in Germany in 1886; this involved rolling the billet longitudinally and at the same time forcing it onto a piercing bar called a mandrel. The method is widely used for both ferrous and nonferrous metals.
As the size of ingots increased in the late 19th century, large hammer forges were developed that simulated the early blacksmiths’ hammering action. For really large components, the first press forge was built in Britain in 1861 and introduced into the United States by 1877. In these forges, the upper forging die is pressed against the workpiece on the lower anvil by a hydraulically operated piston.
The introduction of the crucible process enabled steel castings to be produced for the first time. Steel products were being cast in Germany and Switzerland from 1824, and, by 1855, steel gear wheels were cast in Sheffield. In the United States, steel castings were first produced in Pittsburgh in 1871.
The crucible process continued to be the chief melting method until 1893, when the Tropenas converter, a side-blown, Bessemer-type vessel, was developed in Sheffield. Electric melting in acid-lined furnaces was pioneered in Switzerland in 1907, and electric furnace melting is now predominantly used for making steel castings.
Research on molding sands (which have a great influence on the quality of steel castings) started in the United States in 1919, and this led to the publishing of international standards for molding materials during the period 1924–28. X-ray methods for assessing the soundness of steel castings were introduced in the U.S.S.R. in the 1920s, and magnetic crack-detection methods followed in the 1930s.
Plates are produced by hot-rolling, the technology for which developed in the early 19th century. In order to produce sheet from plate, the steel is cold-rolled, and, as there is a limit to the reduction in thickness that can be achieved by one pass through a rolling stand, a series of stands are arranged in tandem. The first mill of this type was installed in 1904 in the United States.
In making wide, thin sheets, difficulties arise because the small-diameter rolls necessary for producing thin material have a tendency to bend in service, giving a sheet that is thicker in the middle than at the edges. The problems were overcome after World War II by the introduction of larger-diameter backup rolls. In an extreme case, the cluster mill, each small work roll was backed by nine larger-diameter supporting rolls.
The table provides a list of raw steel production by country.
extreme sports, sporting events or pursuits characterized by high speeds and high risk. The sports most commonly placed in this group are skateboarding, snowboarding, freestyle skiing, in-line roller-skating, street lugeing, and BMX and mountain biking. Typically, extreme sports operate outside traditional mainstream sports and are celebrated for their adrenaline-pumping thrills. Racing and acrobatic competitions for motorcycles and snowmobiles are also often classified as “extreme,” and the term can be stretched to include such daring pursuits as rock climbing and skydiving.
(Read Tony Hawk’s Britannica entry on skateboarding.)
The primary extreme sports—skateboarding, in-line roller-skating, and BMX, for example—often make use of half-pipes (U-shaped structures) and urban landscapes for performing a wide range of tricks. The sports also share a unique subculture that separates them from traditional team sports. It is a youth-oriented culture that has embraced punk music and fashion and emphasizes individual creativity.
The term extreme sports is generally attributed to the X Games, a made-for-television sports festival created by the cable network ESPN in 1995. The success of the X Games raised the profile and economic viability of these sports. The extreme sports of mountain biking and snowboarding debuted at the Summer and Winter Olympic Games in 1996 and 1998, respectively.
culture, behaviour peculiar to Homo sapiens, together with material objects used as an integral part of this behaviour. Thus, culture includes language, ideas, beliefs, customs, codes, institutions, tools, techniques, works of art, rituals, and ceremonies, among other elements.
The existence and use of culture depends upon an ability possessed by humans alone. This ability has been called variously the capacity for rational or abstract thought, but a good case has been made for rational behaviour among subhuman animals, and the meaning of abstract is not sufficiently explicit or precise. The term symboling has been proposed as a more suitable name for the unique mental ability of humans, consisting of assigning to things and events certain meanings that cannot be grasped with the senses alone. Articulate speech—language—is a good example. The meaning of the word dog is not inherent in the sounds themselves; it is assigned, freely and arbitrarily, to the sounds by human beings. Holy water, “biting one’s thumb” at someone (Romeo and Juliet, Act I, scene 1), or fetishes are other examples. Symboling is a kind of behaviour objectively definable and should not be confused with symbolizing, which has an entirely different meaning.
What has been termed the classic definition of culture was provided by the 19th-century English anthropologist Edward Burnett Tylor in the first paragraph of his Primitive Culture (1871):
Culture . . . is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society.
In Anthropology (1881) Tylor made it clear that culture, so defined, is possessed by man alone. This conception of culture served anthropologists well for some 50 years. With the increasing maturity of anthropological science, further reflections upon the nature of their subject matter and concepts led to a multiplication and diversification of definitions of culture. In Culture: A Critical Review of Concepts and Definitions (1952), U.S. anthropologists A.L. Kroeber and Clyde Kluckhohn cited 164 definitions of culture, ranging from “learned behaviour” to “ideas in the mind,” “a logical construct,” “a statistical fiction,” “a psychic defense mechanism,” and so on. The definition—or the conception—of culture that is preferred by Kroeber and Kluckhohn and also by a great many other anthropologists is that culture is an abstraction or, more specifically, “an abstraction from behaviour.”
These conceptions have defects or shortcomings. The existence of behavioral traditions—that is, patterns of behaviour transmitted by social rather than by biologic hereditary means—has definitely been established for nonhuman animals. “Ideas in the mind” become significant in society only as expressed in language, acts, and objects. “A logical construct” or “a statistical fiction” is not specific enough to be useful. The conception of culture as an abstraction led, first, to a questioning of the reality of culture (inasmuch as abstractions were regarded as imperceptible) and, second, to a denial of its existence; thus, the subject matter of nonbiological anthropology, “culture,” was defined out of existence, and without real, objective things and events in the external world there can be no science.
Kroeber and Kluckhohn were led to their conclusion that culture is an abstraction by reasoning that if culture is behaviour it, ipso facto, becomes the subject matter of psychology; therefore, they concluded that culture “is an abstraction from concrete behavior but is not itself behavior.” But what, one might ask, is an abstraction of a marriage ceremony or a pottery bowl, to use Kroeber and Kluckhohn’s examples? This question poses difficulties that were not adequately met by these authors. A solution was perhaps provided by Leslie A. White in the essay “The Concept of Culture” (1959). The issue is not really whether culture is real or an abstraction, he reasoned; the issue is the context of the scientific interpretation.
When things and events are considered in the context of their relation to the human organism, they constitute behaviour; when they are considered not in terms of their relation to the human organism but in their relationship to one another, they become culture by definition. The mother-in-law taboo is a complex of concepts, attitudes, and acts. When one considers them in their relationship to the human organism—that is, as things that the organism does—they become behaviour by definition. When, however, one considers the mother-in-law taboo in its relationship to the place of residence of a newly married couple, to the customary division of labour between the sexes, to their respective roles in the society’s mode of subsistence and offense and defense, and these in turn to the technology of the society, the mother-in-law taboo becomes, again by definition, culture. This distinction is precisely the one that students of words have made for many years. When words are considered in their relationship to the human organism—that is, as acts—they become behaviour. But when they are considered in terms of their relationship to one another—producing lexicon, grammar, syntax, and so forth—they become language, the subject matter not of psychology but of the science of linguistics. Culture, therefore, is the name given to a class of things and events dependent upon symboling (i.e., articulate speech) that are considered in a kind of extra-human context.
Culture, as noted above, is due to an ability possessed by man alone. The question of whether the difference between the mind of man and that of the lower animals is one of kind or of degree has been debated for many years, and even today reputable scientists can be found on both sides of this issue. But no one who holds the view that the difference is one of degree has adduced any evidence to show that nonhuman animals are capable, to any degree whatever, of a kind of behaviour that all human beings exhibit. This kind of behaviour may be illustrated by the following examples: remembering the sabbath to keep it holy, classifying one’s relatives and distinguishing one class from another (such as uncles from cousins), defining and prohibiting incest, and so on. There is no reason or evidence that leads one to believe that any animal other than man can have or be brought to any appreciation or comprehension whatever of such meanings and acts. There is, as Tylor argued long ago, a “mental gulf that divides the lowest savage from the highest ape” (Anthropology).
In line with the foregoing distinction, human behaviour is to be defined as behaviour consisting of, or dependent upon, symboling rather than upon anything else that Homo sapiens does; coughing, yawning, stretching, and the like are not human.
Next to nothing is yet known about the neuroanatomy of symboling. Man is characterized by a very large brain, considered both absolutely and relatively, and it is reasonable—and even obligatory—to believe that the central nervous system, especially the forebrain, is the locus of the ability to symbol. But how it does this and with what specific mechanisms remain to be discovered. One is thus led to the conclusion that at some point in the evolution of primates a threshold was reached in some line, or lines, when the ability to symbol was realized and made explicit in overt behaviour. There is no intermediate stage, logical or neurological, between symboling and nonsymboling; an individual or a species is capable of symboling, or he or it is not. The life of Helen Keller makes this clear: when, through the aid of her teacher, Anne Sullivan, Keller was enabled to escape from the isolation to which her blindness and deafness had consigned her and to effect contact with the world of human meanings and values, the transformation was instantaneous.
But even if almost nothing is known about the neuroanatomy of symboling, a great deal is known about the evolution of mind (or “minding,” if mind is considered as a process rather than a thing), in which one finds symboling as the characteristic of a particular stage of development. The evolution of minding can be traced in the following sequence of stages. First is the simple reflexive stage, in which behaviour is determined by the intrinsic properties of both the organism and the thing reacted to—for example, the contraction of the pupil of the eye under increased stimulation by light. Second is the conditioned reflex stage, in which the response is elicited not by properties intrinsic in the stimulus but by meanings that the stimulus has acquired for the responding organism through experience—for example, Pavlov’s dog’s salivary glands responding to the sound of a bell. Third is the instrumental stage, as exemplified by a chimpanzee knocking down a banana with a stick. Here the response is determined by the intrinsic properties of the things involved (banana, stick, chimpanzee’s neurosensory-muscular system); but a new element has been introduced into behaviour, namely, the exercise of control by the reacting organism over things in the external world. And, finally, there is the symbol stage, in which the configuration of behaviour involves nonintrinsic meanings, as has already been suggested.
These four stages exhibit a characteristic of the evolution of all living things: a movement in the direction of making life more secure and enduring. In the first stage the organism distinguishes between the beneficial, the injurious, and the neutral, but it must come into direct contact with the object or event in question to do so. In the second stage the organism may react at a distance, as it were—that is, through an intermediate stimulus. The conditioned reflex brings signs into the life process; one thing or event may serve as an indication of something else—food, danger, and so forth. And, since anything can serve as a sign of anything else (a green triangle can mean food, sex, or an electric shock to the laboratory rat), the reactions of the organism are emancipated from the limitations that stage one imposes upon living things, namely, the intrinsic properties of things. The possibility of obtaining life-sustaining things and of avoiding life-destroying things is thus much enhanced, and the security and continuity of life are correspondingly increased. But in stage two the organism still plays a subordinate role to the external world; it does not and cannot determine the significance of the intermediary stimulus: the bark of a distant dog to the rabbit or the sound of the bell to Pavlov’s dog. This meaning is determined by things and events in the external world (or in the laboratory by the experimenter). In stages one and two, therefore, the organism is at the mercy of the external world in this respect.
In the third stage the element of control over environment is introduced. The ape who obtains food by means of a stick (tool) is not subordinate to his situation. He does not merely undergo a situation; he dominates it. His behaviour is not determined by the juxtaposition of things and events; on the contrary, the juxtaposition is determined by the ape. He is confronted with alternatives, and he makes choices. The configuration of behaviour in stage three is constructed within the dynamic organism of the ape and then imposed upon the external world.
The evolution of minding is a cumulative process; the achievements of each stage are carried on into the succeeding one or ones. The fourth stage reintroduces the factor of nonintrinsic meanings to the advances made in stages two and three. Stage four is the stage of symboling, of articulate speech. Thus, one observes two aspects of the evolution of minding, both of which contribute to the security and survivability of life: the emancipation of behaviour from limitations imposed upon it by the external world and increased control over the environment. To be sure, neither emancipation nor control becomes complete, but quantitative increase is significant.
The direction of biologic evolution toward greater expansion and security of life can be seen from another point of view: the advance from instinctive behaviour (i.e., responses determined by intrinsic properties of the organism) to learned and freely variable behaviour, patterns of which may be acquired and transmitted from one individual and generation to another, and finally to a system of things and events, the essence of which is meanings that cannot be comprehended by the senses alone. This system is, of course, culture, and the species is the human species. Culture is a man-made environment, brought into existence by the ability to symbol.
Once established, culture has a life of its own, so to speak; that is, it is a continuum of things and events in a cause and effect relationship; it flows down through time from one generation to another. Since its inception 1,000,000 or more years ago, this culture—with its language, beliefs, tools, codes, and so on—has had an existence external to each individual born into it. The function of this external, man-made environment is to make life secure and enduring for the society of human beings living within the cultural system. Thus, culture may be seen as the most recent, the most highly developed means of promoting the security and continuity of life, in a series that began with the simple reflex.
Society preceded culture; society, conceived as the interaction of living beings, is coextensive with life itself. Man’s immediate prehuman ancestors had societies, but they did not have culture. Studies of monkeys and apes have greatly enlarged scientific knowledge of their social life—and, by inference, the scientific conception of the earliest human societies. Data derived from paleontological sources and from accumulating studies of living, nonhuman primates are now fairly abundant, and hypotheses derived from these are numerous and varied in detail. A fair summary of them may be made as follows: The growth of the primate brain was stimulated by life in the trees, specifically, by eye-hand coordinations involved in swinging from limb to limb and by manipulating food with the hands (as among the insectivorous lemurs). Descent to the ground, as a consequence of deforestation or increase in body size (which would tend to restrict arboreal locomotion and increase the difficulty of obtaining enough food to supply increased need), and the assumption of erect posture were other significant steps in biologic evolution and the eventual emergence of culture. Some theories reject the arboreal stage in man’s evolutionary past, but this does not seriously affect the overall conception of his development.
The Australopithecines of Africa, extinct manlike higher primates about which reliable knowledge is very considerable today, exemplify the stage of erect posture in primate evolution. Erect posture freed the arms and hands from their earlier function of locomotion and made possible an extensive and versatile use of tools. Again, the eye-hand-object coordinations involved in tool using stimulated the growth of the brain, especially the forebrain. It is not possible to determine on the basis of paleontological evidence the precise point at which the ability to symbol (specifically, articulate speech) was realized, as expressed in overt behaviour. It is believed by some that man’s prehuman ancestors used tools habitually and that habit became custom through the transmission of tool using from one generation to another long before articulate speech came into being. In fact, some theorists hold, the customary use of tools became a powerful stimulus in the development of a brain that was capable of symboling or articulate speech.
The introjection of symboling into primate social life was revolutionary. Everything was transformed, everything acquired new meaning; the symbol added a new dimension to primate—now human—existence. An ax was no longer merely a tool with which to chop; it could become a symbol of authority. Mating became marriage, and all social relationships between parents and children and brothers and sisters became moral obligations, duties, rights, and privileges. The world of nature, from the stones beside the path to the stars in their courses, became alive and conscious spirits. “And all that I beheld respired with inward meaning” (Wordsworth). The anthropoid had at last become a man.
Thus far in this article, culture has been considered in general, as the possession of all mankind. Now it is appropriate to turn to particular cultures, or sociocultural systems. Human beings, like other animal species, live in societies, and each society possesses culture. It has long been customary for ethnologists to speak of Seneca culture, Eskimo culture, North American Plains culture, and so on—that is, the culture of a particular society (Seneca) or an indefinite number of societies (Eskimo) or the cultures found in or characteristic of a topographic area (the North American Plains). There is no objection to this usage as a convenient means of reference: “Seneca culture” is the culture that the Seneca tribe possesses at a particular time. Similarly, Eskimo culture refers to a class of cultures, and Plains culture refers to a type of culture. What is needed is a term that defines culture precisely in its particular manifestations for the purpose of scientific study, and for this the term sociocultural system has been proposed. It is defined as the culture possessed by a distinguishable and autonomous group (society) of human beings, such as a tribe or a modern nation. Cultural elements may pass freely from one system to another (cultural diffusion), but the boundary provided by the distinction between one system and another (Seneca, Cayuga; United States, Japan) makes it possible to study the system at any given time or over a period of time.
Every human society, therefore, has its own sociocultural system: a particular and unique expression of human culture as a whole. Every sociocultural system possesses the components of human culture as a whole—namely, technological, sociological, and ideological elements. But sociocultural systems vary widely in their structure and organization. These variations are attributable to differences among physical habitats and the resources that they offer or withhold for human use; to the range of possibilities inherent in various areas of activity, such as language or the manufacture and use of tools; and to the degree of development. The biologic factor of man may, for purposes of analysis and comparison of sociocultural systems, be considered as a constant. Although the equality or inequality of races, or physical types, of mankind has not been established by science, all evidence and reason lead to the conclusion that, whatever differences of native endowment may exist, they are insignificant as compared with the overriding influence of the external tradition that is culture.
Since the infant of the human species enters the world cultureless, his behaviour—his attitudes, values, ideals, and beliefs, as well as his overt motor activity—is powerfully influenced by the culture that surrounds him on all sides. It is almost impossible to exaggerate the power and influence of culture upon the human animal. It is powerful enough to hold the sex urge in check and achieve premarital chastity and even voluntary vows of celibacy for life. It can cause a person to die of hunger, though nourishment is available, because some foods are branded unclean by the culture. And it can cause a person to disembowel or shoot himself to wipe out a stain of dishonour. Culture is stronger than life and stronger than death. Among subhuman animals, death is merely the cessation of the vital processes of metabolism, respiration, and so on. In the human species, however, death is also a concept; only man knows death. But culture triumphs over death and offers man eternal life. Thus, culture may deny satisfactions on the one hand while it fulfills desires on the other.
The predominant emphasis, perhaps, in studies of culture and personality has been the inquiry into the process by which the individual personality is formed as it develops under the influence of its cultural milieu. But the individual biologic organism is itself a significant determinant in the development of personality. The mature personality is, therefore, a function of both biologic and cultural factors, and it is virtually impossible to distinguish these factors from each other and to evaluate the magnitude of each in particular cases. If the cultural factor were a constant, personality would vary with the variations of the neurosensory-glandular-muscular structure of the individual. But there are no tests that can indicate, for example, precisely how much of the taxicab driver’s ability to make change is due to innate endowment and how much to cultural experience. Therefore, the student of culture and personality is driven to work with “modal personalities,” that is, the personality of the typical Crow Indian or the typical Frenchman insofar as this can be determined. But it is of interest, theoretically at least, to note that even if both factors, the biologic and the cultural, were constant—which they never are in actuality—variations of personality would still be possible. Within the confines of these two constants, individuals might undergo a number of profound experiences in different chronological permutations. For example, two young women might have the same experiences of (1) having a baby, (2) graduating from college, and (3) getting married. But the effect of sequence (1), (2), (3) upon personality development would be quite different than that of sequence (2), (3), (1).
Ethnocentrism is the name given to a tendency to interpret or evaluate other cultures in terms of one’s own. This tendency has been, perhaps, more prevalent in modern nations than among preliterate tribes. The citizens of a large nation, especially in the past, have been less likely to observe people in another nation or culture than have been members of small tribes who are well acquainted with the ways of their culturally diverse neighbours. Thus, the American tourist could report that Londoners drive “on the wrong side of the street” or an Englishman might find some customs on the Continent “queer” or “boorish,” merely because they are different. Members of a Pueblo tribe in the American Southwest, on the other hand, might be well acquainted with cultural differences not only among other Pueblos but also in non-Pueblo tribes such as the Navajo and Apache.
Ethnocentrism became prominent among many Europeans after the discovery of the Americas, the islands of the Pacific, and the Far East. Even anthropologists might characterize all preliterate peoples as being without religion (as did Sir John Lubbock) or as having a “prelogical mentality” (as did Lucien Lévy-Bruhl) merely because their ways of thinking did not correspond with those of the culture of western Europe. Thus, inhabitants of non-Western cultures, particularly those lacking the art of writing, were widely described as being immoral, illogical, queer, or just perverse (“Ye Beastly Devices of ye Heathen”).
Increased knowledge led to or facilitated a deeper understanding and, with it, a finer appreciation of cultures quite different from one’s own. When it was understood that universal needs could be served with culturally diverse means, that worship might assume a variety of forms, that morality consists in conforming to ethical rules of conduct but does not inhere in the rules themselves, a new view emerged that each culture should be understood and appreciated in terms of itself. What is moral in one culture might be immoral or ethically neutral in another. For example, it was not immoral to kill a baby girl at birth or an aged grandparent who was nonproductive when it was impossible to obtain enough food for all; or wife lending among the Eskimo might be practiced as a gesture of hospitality, a way of cementing a friendship and promoting mutual aid in a harsh and dangerous environment, and thus may acquire the status of a high moral value.
The view that elements of a culture are to be understood and judged in terms of their relationship to the culture as a whole—a doctrine known as cultural relativism—led to the conclusion that the cultures themselves could not be evaluated or graded as higher and lower, superior or inferior. If it was unwarranted to say that patriliny (descent through the male line) was superior or inferior to matriliny (descent through the female line), if it was unjustified or meaningless to say that monogamy was better or worse than polygamy, then it was equally unsound or meaningless to say that one culture was higher or superior to another. A large number of anthropologists subscribed to this view; they argued that such judgments were subjective and therefore unscientific.
It is, of course, true that some values are imponderable and some criteria are subjective. Are people in modern Western culture happier than the Aborigines of Australia? Is it better to be a child than an adult, alive than dead? These certainly are not questions for science. But to say that the culture of the ancient Mayas was not superior to or more highly developed than the crude and simple culture of the Tasmanians or to say that the culture of England in 1966 was not higher than England’s culture in 1066 is to fly in the face of science as well as of common sense.
Cultures have ponderable values as well as imponderable, and the imponderable ones can be measured with objective, meaningful yardsticks. A culture is a means to an end: the security and continuity of life. Some kinds of culture are better means of making life secure than others. Agriculture is a better means of providing food than hunting and gathering. The productivity of human labour has been increased by machinery and by the utilization of the energy of nonhuman animals, water and wind power, and fossil fuels. Some cultures have more effective means of coping with disease than others, and this superiority is expressed mathematically in death rates. And there are many other ways in which meaningful differences can be measured and evaluations made. Thus, the proposition that cultures have ponderable values that can be measured meaningfully by objective yardsticks and arranged in a series of stages, higher and lower, is substantiated. But, it should be noted, this is not equivalent to saying that man is happier or that the dignity of the individual (an imponderable) is greater in an industrialized or agricultural sociocultural system than in one supported by human labour alone and sustained wholly by wild foods.
Actually, however, there is no necessary conflict between the doctrine of cultural relativism and the thesis that cultures can be objectively graded in a scientific manner. It is one thing to reject the statement that monogamy is better than polygamy and quite another to deny that one kind of sociocultural system contains a better means of providing food or combating disease than another.
Every sociocultural system exists in a natural habitat, and, of course, this environment exerts an influence upon the cultural system. The cultures of some Eskimo groups present remarkable instances of adaptation to environmental conditions: tailored fur clothing, snow goggles, boats and harpoons for hunting sea mammals, and, in some instances, hemispherical snow houses, or igloos. Some sedentary, horticultural tribes of the upper Missouri River went out into the Great Plains and became nomadic hunters after the introduction of the horse. The culture of the Navajos underwent profound change after they acquired herds of sheep and a market for their rugs was developed. The older theories of simple environmentalism, some of which maintained that even styles of myths and tales were determined by topography, climate, flora, and other factors, are no longer in vogue. The present view is that the environment permits, at times encourages, and also prohibits the acquisition or use of certain cultural traits but otherwise does not determine culture change. The Fuegians living at the southern tip of South America, as viewed by Charles Darwin on his voyage on the Beagle, lived in a very cold, harsh environment but were virtually without both clothing and dwellings.
“Culture is contagious,” as a prominent anthropologist once remarked, meaning that customs, beliefs, tools, techniques, folktales, ornaments, and so on may diffuse from one people or region to another. To be sure, a culture trait must offer some advantage, some utility or pleasure, to be sought and accepted by a people. (Some anthropologists have assumed that basic features of social structure, such as clan organization, may diffuse, but a sounder view holds that these features involving the organic structure of the society must be developed within societies themselves.) The degree of isolation of a sociocultural system—brought about by physical barriers such as deserts, mountain ranges, and bodies of water—has, of course, an important bearing upon the ease or difficulty of diffusion. Within the limits of desirability on the one hand and the possibility of communication on the other, diffusion of culture has taken place everywhere and in all times. Archaeological evidence shows that amber from the Baltic region diffused to the Mediterranean coast; and, conversely, early coins from the Middle East found their way to northern Europe. In aboriginal North America, copper objects from northern Michigan have been found in mounds in Georgia; macaw feathers from Central America turn up in archaeological sites in northern Arizona. Some Indian tribes in northwestern regions of the United States had possessed horses, originally brought into the Southwest by Spanish explorers, years before they had ever even seen white men. The wide dispersion of tobacco, corn (maize), coffee, the sweet potato, and many other traits are conspicuous examples of cultural diffusion.
Diffusion may take place between tribes or nations that are approximately equal in political and military power and of equivalent stages of cultural development, such as the spread of the sun dance among the Plains tribes of North America. But in other instances, it takes place between sociocultural systems differing widely in this respect. Conspicuous examples of this have been instances of conquest and colonization of various regions by the nations of modern Europe. In these cases it is often said that the culture of the more highly developed nation is “imposed” upon the less developed peoples and cultures, and there is, of course, much truth in this; the acquisition of foreign culture by the subject people is called acculturation and is manifested by the indigenous populations of Latin America as well as of other regions. But even in cases of conquest, traits from the conquered peoples may diffuse to those of the more advanced cultures; examples might include, in addition to the cultivated plants cited above, individual words (coyote), musical themes, games, and art motifs.
One of the major problems of ethnology during the latter half of the 19th and the early decades of the 20th centuries was the question “How are cultural similarities in noncontiguous regions to be explained?” Did the concepts of pyramid building, mummification, and sun worship originate independently in ancient Egypt and in the Andean highlands and in Yucatán or did these traits originate in Egypt and diffuse from there to the Americas, as some anthropologists have believed? Some schools of ethnological theory have held to one view, some, to another. The 19th-century classical evolutionists (which included Edward Burnett Tylor and Lewis H. Morgan, among others) held that the mind of man is so constituted or endowed that he will develop cultures everywhere along the same lines. “Diffusionists”—those, such as Fritz Graebner and Elliot Smith, who offered grand theories about the diffusion of traits all over the world—maintained that man was inherently uninventive and that culture, once created, tended to spread everywhere. Each school tended to insist that its view was the correct one, and it would continue to hold that view unless definite proof of the contrary could be adduced.
The tendency nowadays is not to side categorically with one school as against another but to decide each case on its own merits. The consensus with regard to pyramids is that they were developed independently in Egypt and the Americas because they differ markedly in structure and function: the Egyptian pyramids were built of stone blocks and contained tombs within their interiors. The American pyramids were constructed of earth, then faced with stone, and they served as the bases of temples. The verdict with regard to the bow and arrow is that it was invented only once and subsequently diffused to all regions where it has been found. The probable antiquity of the origin of fire making, however, and the various ways of generating it—by percussion, friction, compression (fire pistons)—indicate multiple origins.
Evolution of culture—that is, the development of forms through time—has taken place. No amount of diffusion of picture writing could of itself, for instance, produce the alphabetic system of writing; as Tylor demonstrated so well, the art of writing has developed through a series of stages, which began with picture writing, progressed to hieroglyphic writing, and culminated in alphabetic writing. In the realm of social organization there was a development from territorial groups composed of families to segmented societies (clans and larger groupings). Sociocultural evolution, like biologic evolution, exhibits a progressive differentiation of structure and specialization of function.
A misunderstanding has arisen with regard to the relationship between evolution and diffusion. It has been argued, for example, that the theory of cultural evolution was unsound because some peoples skipped a stage in a supposedly determined sequence; for example, some African tribes, as a consequence of diffusion, went from the Stone Age to the Iron Age without an intermediate age of copper and bronze. But the classical evolutionists did not maintain that peoples, or societies, had to pass through a fixed series of stages in the course of development, but that tools, techniques, institutions—in short, culture—had to pass through the stages. The sequence of stages of writing did not mean that a society could not acquire the alphabet without working its way through hieroglyphic writing; it was obvious that many peoples did skip directly to the alphabet.
The concept of culture embraces the culture of mankind as a whole. An understanding of human culture is facilitated, however, by analyzing “the complex whole” into component parts or categories. In somewhat the same sense that the atom has been regarded as the unit of matter, the cell as the unit of life, so the culture trait is generally regarded as the unit of culture. A trait may be an object (knife), a way of doing something (weaving), a belief (in spirits), or an attitude (the so-called horror of incest). But, within the category of culture, each trait is related to other traits. A distinguishable and relatively self-contained cluster of traits is conventionally called a culture complex. The association of traits in a complex may be of a functional and mechanical nature, such as horse, saddle, bridle, quirt, and the like, or it may lie in conceptional or emotional associations, such as the acts and attitudes involved in seclusion in a menstrual hut or retrieving a heart that has been stolen by witches.
The relationship between an actual culture and its habitat is always an intimate one, and therefore one finds a more or less close correlation between kind of habitat and type of culture. This results in the concept of culture area. This conception goes back at least as far as the early 19th century, but it was first brought into prominence by the U.S. anthropologist Clark Wissler in The American Indian (1917) and Man and Culture (1923). He divided the Indian cultures (as they were in the latter half of the 19th century) into geographic cultural regions: the Caribou area of northern Canada; the Northwest coast, characterized by the use of salmon and cedar; the Great Plains, where tribes hunted bison with the horse; the Pueblo area of the Southwest; and so on. Others later distinguished culture areas in other continents.
Appreciation of the relationship between culture and topographic area suggests the concept of culture type, such as hunting and gathering or a special way of hunting—for example, the use of the horse in bison hunting in the Plains or the method of hunting of sea mammals among the Eskimo; pastoral cultures centred upon sheep, cattle, reindeer, and so on; and horticulture (with digging stick and hoe) and agriculture (with ox-drawn plow). Less common are trading cultures such as are found in Melanesia or specialized production of some object for trade, such as pottery, bronze axes, or salt, as was the case in Luzon.
Configuration and pattern, especially the latter, are concepts closely related to culture area and culture type. All of them have one thing in common; they view culture not in terms of its individual components, or traits, but as meaningful organizations of traits: areas, occupations, configurations (art, mathematics, physics), or patterns (in which psychological factors are the bases of organization). Clark Wissler’s “universal culture pattern” was a recognition of the fact that all particular and actual cultures possess the same general categories: language, art, social organization, religion, technology, and so on.
A sociocultural system presents itself under two aspects: structure and function. As culture evolves, sociocultural systems (like biologic systems) become more differentiated structurally and more specialized functionally, proceeding from the simple to the complex. Systems on the lowest stage of development have only two significant kinds of parts: the local territorial group and the family. There is a corresponding minimum of specialization, limited, with but few exceptions, to division of function, or labour, along sex lines and to division between children and adults. The exceptions are headmen and shamans; they are special organs, so to speak, in the body politic. The headman is a mechanism of social integration, direction, and control, expressing, however, the consensus of the band. The shaman, though a self-appointed priest or magician, is also an instrument of society; he may be regarded as the first specialist in the history of human society.
All human societies are divided into classes and segments. Class is defined as one of an indefinite number of groupings each of which differs in composition from the other or others, such as men and women; married, widowed, and divorced; children and adults. Segment is defined as one of an indefinite number of groupings all of which are alike in structure and function: families, lineages, clans, and so on. On more advanced levels of development there are occupational classes, such as farmers, pastoralists, artisans, metalworkers, and scribes, and territorial segments, such as wards, barrios, counties, and states.
Segmentation is a cultural process essential to the evolution of culture; it is a means of increasing the size of a society or a grouping within a sociocultural system (such as an army) and therefore of increasing its power to make life secure, without suffering a corresponding loss of effectiveness through diminished solidarity; segmentation is a means of maintaining solidarity at the same time that it enlarges the social grouping. A tribe could not increase in size beyond a certain point without resorting to segmentation: the formation of lineages, clans, and the like. The word clannish points to one of the functions of segments in general: the fostering of solidarity. Tribes become segments in confederacies; and above the tribal level, the evolution of civil society employs barrios, demes, counties, and states in its process of segmentation. In present-day society, the army and the church offer illuminating examples of increased size and sustained solidarity proceeding hand in hand.
Division of labour along occupational lines is rare, although not wholly lacking, in preliterate societies—despite a widespread notion that one member of a tribe specializes in making arrows, which he exchanges for moccasins made by another specialist. Occupational groupings were virtually lacking in all cultural systems of aboriginal North America, for example. Guilds of metalworkers are found in some African tribes and specialists in canoe making and tattooing existed in Polynesia. But it is not until the transition from preliterate society, based upon ties of kinship, to civil society, based upon property relations and territorial distinctions (the state), that division of labour along occupational lines becomes extensive. On this level there are found many kinds of specialists: metalworkers, scribes, astrologers, soldiers, dancers, musicians, alchemists, prostitutes, eunuchs, and so forth.
Production of goods is everywhere followed by distribution and exchange. Among the Kurnai of Australia, for example, game was divided and distributed as follows: the hunter who killed a wallaby, for example, kept the head; his father received the ribs on the right side, his mother the ribs on the left side, plus the backbone, and so on; the various parts of the animal went to various classes of relatives in accordance with fixed, traditional rules.
Distribution along kinship lines constitutes a system of circulation and exchange within the tribe as a whole, for everyone is a relative of everyone else. It takes the form of bestowing gifts to relatives on all sorts of occasions—such as birth, initiation, marriage, death. In some cases there is an exchange of goods on the spot, but more often A gives something to B who gives A a gift at a later date. All this takes place in the network of rights and obligations among kindred; one has both an obligation to give and a right to receive on certain occasions and in certain contexts. The whole process is one of mutual aid and cooperation.
The consequence of this form of distribution and exchange is that the recipient receives kinds of things that he already has; each household has the same kinds of foods, utensils, ornaments, and other things that every other household has. Why, then, it might be asked, does this form of exchange take place? Two reasons may be distinguished. First, this kind of exchange fortifies ties of kinship and mutual aid—as neighbourhood exchange among households in modern American culture initiates friendships that in times of need constitute mutual aid. Second, this system of circulation of goods is in effect a system of social security: a household in need, due to illness or accident, receives help from the community (“No household can starve as long as others have corn,” as the Iroquois put it). Here we have an economic system subordinated to the welfare of the society as a whole.
Exchange or circulation of goods and services (a basket is the material form of “a service,” that is, human labour) must, of course, take place in sociocultural systems where division of labour finds expression in specialization: the ironworker must obtain food; the horticulturalist needs an iron hoe.
Exchange of goods between sociocultural systems is universal and takes place on the lowest levels of cultural development. In some instances it is the only form of nonhostile communication: in the so-called silent trade the actual exchange takes place in a neutral zone without the presence of the participating parties. Archaeological evidence shows that intergroup exchange occurred in remote times and over great distances, as already noted above in the discussion of diffusion.
An interesting form of the circulation of goods—usually referred to as redistribution—occurs among more highly developed tribes. The head of the sociopolitical system, that is, the chief or priest-chief, imposes levies upon all households, thus acquiring a large amount of goods—food, utensils, art objects, and so on—which he then redistributes to the households of the tribe. This may take the form and occasion of ceremonies and feasts or distribution may be made in cases of need. This widespread and interesting form of redistribution serves the same ends as those served by distribution as a function of the kinship system, namely, fostering solidarity and social security—an equitable distribution that tends to iron out inequalities among households.
Some economic concepts in modern Western culture do not correspond closely with conceptions and customs in many preliterate societies. Ownership is a case in point. Complete possession of and exclusive right to use something in an economic context, such as land, a dwelling, or a boat, is rare, if not wholly lacking, in preliterate societies (although one might have exclusive rights to a dream, spell, or charm). In general, one has merely the right to use or occupy a tract of land or a house; when its use has terminated, anyone can take it over. In some societies it might be said that a boat “belonged” to the men who made it or even to the individual who initiated its construction. But anyone else in the community would have the right to use it when the “owners” (the men who made it) were not using it. It is the right to use, rather than exclusive and absolute possession, that is significant; there is no such thing as absentee ownership in primitive society.
A band or tribe “holds” the land it occupies; here again, it is tenure rather than ownership that is significant; the land “belongs” to Nature, or Mother Earth; people merely hold and use it. There is usually an intimate relationship between the people and “their” land. Navajo Indians fell on their knees and kissed the earth when they were returned to their former territory after forcible detention in an alien land. Land is defended against outsiders, except when they are accepted as guests, but the significant thing is not that the outsiders do not own the land but that they pose a threat to those who occupy it.
In some tribes there is a distinct conception that the land held “belongs” to the tribe, the chief of which allots plots or tracts to individuals or households for their use. But when use terminates, the land reverts to the tribal domain.
During the latter part of the 19th century there was considerable discussion of “primitive communism.” This doctrine came to be interpreted as meaning that private property, the private right to hold or use, was nonexistent in primitive society. It was extended also to communism in wives and children in some tribes; this was interpreted to be a vestige of a former stage of “primordial promiscuity.” Many ethnologists, however, launched a vigorous attack upon “the doctrine of primitive communism.” Some of the conceptions of earlier anthropologists—such as group marriage—were shown to be unwarranted in the light of later research.
Today, with these polemics well in the past, the situation with regard to property rights in tribal societies may be summarized as follows. Tenure and use, rather than ownership in fee simple, were the significant concepts and practices. Private, or personal, possession of goods and use of land were recognized. But possession and right were qualified by the rights and obligations of kinship: one had an obligation both to give and to receive within the body of kindred, according to specific rules. In a de facto sense, things belonged to the body of kindred; rights of possession and use were regulated by customs of kinship. In some cultures a borrower was not obliged to return an object borrowed, on the theory that if a person could afford to lend something, he relinquished his right to its possession. The mode of life in preliterate society, based upon kinship and functioning in accordance with the principles of cooperation and mutual aid, did indeed justify the adjective communal; it was the noun communism that was resented—if not feared—because of its Marxist connotation.
One of the most important, as well as characteristic, features of the economic life of preliterate societies, as contrasted with modern civilizations, is this: no individual and no class or group in tribal society was denied access to the resources of nature; all were free to exploit them. This is, of course, in sharp contrast to civil society in which private ownership by some, or a class, is the means of excluding others—slaves, serfs, a proletariat—from the exploitation and enjoyment of the resources of nature. It is this freedom of access, the freedom to exploit and to enjoy the resources of nature, that has given primitive society its characteristics of freedom and equality. And, being based upon kinship ties, it had fraternity as well.
In the human species individuals are equipped with fewer instincts than is the case in many nonhuman species. And, as already noted, they are born cultureless. Therefore an infant Homo sapiens must learn a very great deal and acquire a vast number of conditioned reflexes and habit patterns in order to live effectively, not only in society but in a particular kind of sociocultural system, be it Tibetan, Eskimo, or French. This process, taken as a whole, is called socialization (occasionally, enculturation)—the making of a social being out of one that was at birth wholly individualistic and egoistic.
Education in its broadest sense may properly be regarded as the process by which the culture of a sociocultural system is impressed or imposed upon the plastic, receptive infant. It is this process that makes continuity of culture possible. Education, formal and informal, is the specific means of socialization. By informal education is meant the way a child learns to adapt his behaviour to that of others, to be like others, to become a member of a group. By formal education is meant the intentional and more or less systematic effort to affect the behaviour of others by transmitting elements of culture to them, be it knowledge or belief, patterns of behaviour, or ideals and values. These attempts may be overt or covert. The teacher may make his purpose apparent, even emphatic, to the learner. But much education is effected in an unobtrusive way, without teacher or learner being aware that culture is being transmitted. Thus, in myths and tales, certain characters are presented as heroes or villains; certain traits are extolled, others are deplored or denounced. The impressionable child acquires ideals and values, an image of the good or the bad.
The growing child is immersed in the fountain of informal education constantly; the formal education tends to be periodic. Many sociocultural systems distinguish rather sharply a series of stages in the education and development of full-fledged men and women. First there is infancy, during which perhaps the most profound and enduring influences of a person’s life are brought to bear. Weaning ushers in a new stage, that of childhood, during which boys and girls become distinguished from each other. Puberty rites transform children into men and women. These rites vary enormously in emphasis and content. Sometimes they include whipping, isolation, scarification, or circumcision. Very often the ritual is accompanied by explicit instruction in the mythology and lore of the tribe and in ethical codes. Such rituals as confirmation and Bar Mitzvah in modern Western culture belong to the category of puberty rites.
With marriage come instruction and admonition, appropriate to the occasion, from elder relatives and, in more advanced cultures, from priests. In some sociocultural systems men may become members of associations or sodalities: men’s clubs, warrior societies, secret societies of magic or medicine. In some cases it is said that in passing through initiation rites a person is “born again.” Women also may belong to sodalities, and in some instances they may become members of secret, magical societies along with men.
Man’s oldest philosophy is animism, the doctrine that everything is alive and possesses mental faculties like those possessed by man: desire, will, purpose, anger, love, and the like. This philosophy results from man’s projection of his own self, his psyche, into other things and beings, inanimate and living, without being aware of this projection. “To the Omaha,” wrote anthropologist Alice Fletcher,
nothing is without life: . . . He projects his own consciousness upon all things and ascribes to them experiences and characteristics with which he is familiar; . . . akin to his own conscious being.
(“Wakonda,” in F.W. Hodge [ed.], Handbook of American Indians North of Mexico)
“A belief in spirits is,” according to Edward Burnett Tylor, “the minimum definition of religion.” Some later students, however, made the same claim for a belief in impersonal, supernatural power, or mana (manitou, orenda, and so on). In any case, these two elements of religion are virtually worldwide and undoubtedly represent a very early stage in the development of religion. In some cultures spirits are virtually innumerable, but, in the course of time, the more important spirits become gods. Thus, there has been a tendency toward monotheism in the history of religion. The German Roman Catholic priest and anthropologist Father Wilhelm Schmidt argued not only that some primitive peoples believe in a Supreme Being but that monotheism was characteristic of the earliest and simplest cultures. Schmidt’s thesis, however, has been severely criticized by other ethnologists. Also, as Tylor pointed out many years ago, the Supreme Being of some very primitive peoples is an originator god, or a philosophical explanatory device, accountable only for the existence and structure of the world; after his work was completed, he had no further significance; he was not worshiped and played no part in the daily lives of the people.
In the past there was much discussion—and debate—about the difference between magic and religion. Both were deemed expressions of a belief in the supernatural. Some argued that religion was social (moral) whereas magic was antisocial (immoral). Another distinction was that magic was the use of supernatural power divorced from a spiritual being. The distinction between religion and magic was so beset with exceptions as to render most definitions of these terms logically imperfect. Another difficulty was the tacit assumption that different entities, religion and magic, exist per se, and therefore that “correct” definitions of them must exist also (Adam called the animal a horse because it was a horse). Much confusion and debate would have been obviated if it had been recognized (as it generally is now) that there is no such thing as a “correct” definition—all definitions are man-made and arbitrary—and that the problem is not what religion or magic are but what beliefs, events, and experiences one wishes to designate with the words religion and magic (see also magic).
Sociocultural systems, like other kinds of systems, must have means of self-regulation and control in order to persist and function. In human society these means are numerous and varied. The kinship organization specifies reciprocal and correlative rights, duties, and obligations of one class of relatives to another. Codes of ethics govern the relationship of the individual to the well-being of society as a whole. Codes of etiquette regulate class structure by requiring individuals to conform to their respective classes. Custom is a general term that embraces all these mechanisms of regulation and control and even more. Custom is the name given to uniformities in sociocultural systems. Uniformities are important because they make anticipation and prediction possible; without them, orderly conduct of social life would not be possible. Custom, therefore, is a means of social regulation and control, of effecting compliance with itself in order to render effective conduct of social life possible.
As in the case of religion and magic, much effort and debate have been spent in attempts to achieve a clean-cut distinction between custom and law. There is little or no difficulty when one is concerned with the extremes of the spectrum of social control. The way that a Hopi Indian holds his corn-husk cigarette in his hand is a matter of custom rather than law, as most ethnologists would probably agree. At the other extreme, a state edict prohibiting the manufacture and sale of alcoholic beverages is a law, not a custom. But in other situations the distinction is far from clear, and disagreement with regard to definitions arises. For example, in marriage the obligation to wed someone within a specified group or class (endogamy) or outside a group or class (exogamy) has been called both law and custom. Probably the most useful distinction between custom and law is the following. If an infraction of a social rule or deviation from a norm is punished merely by expressions of social disapproval, gossip, ridicule, or ostracism, the rule is called custom. If, however, infractions or violations are punished by an agency, designated by society and empowered to act on its behalf, then the rule is called a law. But even here there is difficulty. The same kind of offense may be punished by custom in one society, by law in another—as in, for example, adultery, incest, miscegenation, and black magic.
It is the ethnologist, rather than the historian, who is disturbed by instances of ambiguity with regard to custom and law; in preliterate societies the distinction between the two is not always clear. But in civil societies—that is, states brought into being by the agricultural revolution and their more recent successors—the distinction is usually sharper and more apparent, though instances of sumptuary laws that prohibit the wearing of silk or that limit the length of a garment merge law and custom or reinforce the latter by the former.
One need not be unduly disturbed by the difficulty of making sharp distinctions among sociocultural phenomena and of formulating definitions. The phenomena of culture, like those of the external world in general, are what they are, and if man-made concepts and words do not correspond closely with them, one may regret the lack of fit. But it is better to do this than to distort real phenomena by trying to force them into artificial concepts and definitions.
volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across the net. A team is allowed only three touches of the ball before it must be returned over the net.
Volleyball was invented in 1895 by William G. Morgan, physical director of the Young Men’s Christian Association (YMCA) in Holyoke, Massachusetts. It was designed as an indoor sport for businessmen who found the new game of basketball too vigorous. Morgan called the sport “mintonette,” until a professor from Springfield College in Massachusetts noted the volleying nature of play and proposed the name of “volleyball.” The original rules were written by Morgan and printed in the first edition of the Official Handbook of the Athletic League of the Young Men’s Christian Associations of North America (1897). The game soon proved to have wide appeal for both sexes in schools, playgrounds, the armed forces, and other organizations in the United States, and it was subsequently introduced to other countries.
In 1916 rules were issued jointly by the YMCA and the National Collegiate Athletic Association (NCAA). The first nationwide tournament in the United States was conducted by the National YMCA Physical Education Committee in New York City in 1922. The United States Volleyball Association (USVBA) was formed in 1928 and recognized as the rules-making, governing body in the United States. From 1928 the USVBA—now known as USA Volleyball (USAV)—has conducted annual national men’s and senior men’s (age 35 and older) volleyball championships, except during 1944 and 1945. Its women’s division was started in 1949, and a senior women’s division (age 30 and older) was added in 1977. Other national events in the United States are conducted by member groups of the USAV such as the YMCA and the NCAA.
Volleyball was introduced into Europe by American troops during World War I, when national organizations were formed. The Fédération Internationale de Volley Ball (FIVB) was organized in Paris in 1947 and moved to Lausanne, Switzerland, in 1984. The USVBA was one of the 13 charter members of the FIVB, whose membership grew to more than 210 member countries by the late 20th century.
International volleyball competition began in 1913 with the first Far East Games, in Manila. During the early 1900s and continuing until after World War II, volleyball in Asia was played on a larger court, with a lower net, and nine players on a team.
The FIVB-sponsored world volleyball championships (for men only in 1949; for both men and women in 1952 and succeeding years) led to acceptance of standardized playing rules and officiating. Volleyball became an Olympic sport for both men and women at the 1964 Olympic Games in Tokyo.
European championships were long dominated by Czechoslovakian, Hungarian, Polish, Bulgarian, Romanian, and Soviet (later, Russian) teams. At the world and Olympic level, Soviet teams have won more titles, both men’s and women’s, than those of any other nation. Their success was attributed to widespread grassroots interest and well-organized play and instruction at all levels of skill. A highly publicized Japanese women’s team, Olympic champions in 1964, reflected the interest of private industry in sport. Young women working for the sponsoring company devoted their free time to conditioning, team practice, and competition under expert and demanding coaching. Encouraged by the Japanese Volleyball Association, this women’s team made its mark in international competition, winning the World Championship in 1962, 1966, and 1967, in addition to the 1964 Olympics. At the end of the 20th century, however, the Cuban women’s team dominated both the World Championships and the Olympics.
The Pan American Games (involving South, Central, and North America) added volleyball in 1955, and Brazil, Mexico, Canada, Cuba, and the United States are frequent contenders for top honors. In Asia, China, Japan, and Korea dominate competition. Volleyball, especially beach volleyball, is played in Australia, New Zealand, and throughout the South Pacific.
A four-year cycle of international volleyball events, recommended by the FIVB, began in 1969 with World Cup championships, to be held in the year following the Olympic Games; the second year is the World Championships; in the third the regional events are held (e.g., European championships, Asian Games, African Games, Pan American Games); and in the fourth year the Olympic Games.
Beach volleyball—usually played, as its name implies, on a sand court with two players per team—was introduced in California in 1930. The first official beach volleyball tournament was held in 1948 at Will Rogers State Beach, in Santa Monica, California, and the first FIVB-sanctioned world championship was held in 1986 at Rio de Janeiro. Beach volleyball was added to the roster of the 1996 Olympic Games in Atlanta, Georgia. American athletes have been especially successful in Olympic beach volleyball competition. Notable U.S. players include Karch Kiraly, Misty May-Traenor, and Kerri Walsh Jennings.
Volleyball requires a minimum of equipment and space and can be played indoors or outdoors. The game is played on a smooth-surfaced court 9 meters (30 feet) wide by 18 meters (60 feet) long, divided by a center line into two equal areas, one of which is selected by or assigned to each of the two competing teams. Players may not step completely beyond the center line while the ball is in play. A line 3 meters (10 feet) from and parallel to the center line of each half of the court indicates the point in front of which a back court player may not drive the ball over the net from a position above the top of the net. (This offensive action, called a spike, or kill, is usually performed most effectively and with greatest power near the net by the forward line of players.)
A tightly stretched net is placed across the court exactly above the middle of the center line; official net heights (measured from the top edge of the net to the playing surface—in the middle of the court) are 2.4 meters (8 feet) for men and 2.2 meters (7.4 feet) for women. Further adjustments in net height can be made for young people and others who need a lower net. A vertical tape marker is attached to the net directly above each side boundary line of the court, and, to help game officials judge whether served or volleyed balls are in or out of bounds, a flexible antenna extends 1 meter (3 feet) above the net along the outer edge of each vertical tape marker.
The ball used is around 260 to 280 grams (9 to 10 ounces) and is inflated to about 65 cm (25.6 inches) in circumference. A ball must pass over the net entirely between the antennae. A service area, traditionally 3 meters (10 feet) long, is marked outside and behind the right one-third of each court end line. At the 1996 Olympic Games the service area was extended to 9 meters (30 feet). The service must be made from within or behind this area. A space at least 2 meters (6 feet) wide around the entire court is needed to permit freedom of action, eliminate hazards from obstructions, and allow space for net support posts and the officials’ stands. A clear area above the court at least 8 meters (26 feet) high is required to permit the ball to be served or received and played without interference.
Informally, any number can play volleyball. In competition each team consists of six players, three of whom take the forward positions in a row close to and facing the net, the other three playing the back court. (An exception to this rotation is the libero, a position introduced at the 2000 Olympics; see below.) Play is started when the right back (the person on the right of the second row) of the serving team steps outside the end line into the serving area and bats the ball with a hand, fist, or arm over the net into the opponents’ half of the court. The opponents receive the ball and return it across the net in a series of not more than three contacts with the ball. This must be done without any player catching or holding the ball while it is in play and without any player touching the net or entering the opponents’ court area. The ball must not touch the floor, and a player may not touch the ball twice in succession. A player continues to serve until that player’s team makes an error, commits a foul, or completes the game. When the service changes, the receiving team becomes the serving team and its players rotate clockwise one position, the right forward shifting to the right back position and then serving from the service area. Either team can score, with points being awarded for successfully hitting the ball onto the opposing side’s half of the court, as well as when the opposing side commits errors or fouls, such as hitting the ball out of bounds, failing to return the ball, contacting the ball more than three times before returning it, etc. Only one point at a time is scored for a successful play. A game is won by the team that first scores 25 points, provided the winning team is ahead by 2 or more points, except in the fifth set, when a team needs to score only 15 points and win by 2 points.
The 2000 Olympics introduced significant rule changes to international competition. One change created the libero, a player on each team who serves as a defensive specialist. The libero wears a different color from the rest of the team and is not allowed to serve or rotate to the front line. Another important rule change allowed the defensive side to score, whereas formerly only the serving team was awarded points.
Pittsburgh recognized as a city: The War of 1812 cut off the supply of British goods, stimulating American industry. By 1815, Pittsburgh was producing significant quantities of iron, brass, tin, and glass. On March 18, 1816, the 46-year-old local government became a city. It was served by numerous river steamboats that increased trading traffic on the rivers.

Roberto Clemente led the Pirates to two World Series titles, being named World Series MVP in 1971. Clemente had two three-home run games in his career, as well as eight five-hit games in MLB. Re

Pittsburgh Downtown Partnership contact information: Bank Tower, The, 307 Fourth Ave Floor 2, Pittsburgh, PA 15222

Trouble in Mind is a play by Alice Childress, which debuted Off-Broadway at the Greenwich Mews Theatre in 1955. It premiered on Broadway at Roundabout Theatre Company's American Airlines Theatre on November 18, 2021. The play focuses on racism and sexism in American theatre. It was published in the anthologies Black Theater: a 20th Century Collection of the Work of its Best Playwrights (Dodd, Mead & Co. 1971), the second edition of Black Drama in America: an Anthology (Howard University Press, 1994), Plays by American Women: 1930-1960 (Applause Books, 2001), and Alice Childress: Selected Plays (Northwestern University Press, 2011). It was first published on its own by Theatre Communications Group in 2022.

PNC Park, the home of the Pittsburgh Pirates, is located at 115 Federal Street, Pittsburgh, PA 15212

Pittsburgh's population grew from 149 people in 1760 to 332 people in 1761, and the number of women grew from 29 in 1760 to 75 in 1761.

In 1861, a young German immigrant, Edward Frauenheim, started the Iron City Brewery, one of the first American breweries to produce a lager, in the bustling river port known at the time as the "Smoky City."[2] This founder of Frauenheim, Miller & Company started brewing Iron City Beer, now the flagship of the Iron City Brewing Company (PBC), in a city thriving on heavy industry and commerce.

In 1975, The Terrible Towel was created by the late Steelers broadcaster Myron Cope to inspire fan involvement in a playoff game against the then-Baltimore Colts.